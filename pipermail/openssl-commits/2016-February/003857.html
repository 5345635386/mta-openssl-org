<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [openssl-commits] [openssl]  master update
   </TITLE>
   <LINK REL="Index" HREF="https://mta.openssl.org/pipermail/openssl-commits/2016-February/index.html" >
   <LINK REL="made" HREF="mailto:openssl-commits%40openssl.org?Subject=Re%3A%20%5Bopenssl-commits%5D%20%5Bopenssl%5D%20%20master%20update&In-Reply-To=%3C1455096738.114233.1172.nullmailer%40dev.openssl.org%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="003852.html">
   <LINK REL="Next"  HREF="003860.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[openssl-commits] [openssl]  master update</H1>
    <B>Andy Polyakov</B> 
    <A HREF="mailto:openssl-commits%40openssl.org?Subject=Re%3A%20%5Bopenssl-commits%5D%20%5Bopenssl%5D%20%20master%20update&In-Reply-To=%3C1455096738.114233.1172.nullmailer%40dev.openssl.org%3E"
       TITLE="[openssl-commits] [openssl]  master update">appro at openssl.org
       </A><BR>
    <I>Wed Feb 10 09:32:18 UTC 2016</I>
    <P><UL>
        <LI>Previous message: <A HREF="003852.html">[openssl-commits] [openssl]  master update
</A></li>
        <LI>Next message: <A HREF="003860.html">[openssl-commits] [openssl]  master update
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3857">[ date ]</a>
              <a href="thread.html#3857">[ thread ]</a>
              <a href="subject.html#3857">[ subject ]</a>
              <a href="author.html#3857">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>The branch master has been updated
       via  5d1f03f29e2794f6d1642dfedf10fc3e334937d0 (commit)
       via  e87c056745845ecaa6a884fa9cf0dc0c404f0c46 (commit)
       via  a98c648e40ea5158c8ba29b5a70ccc239d426a20 (commit)
      from  d40cf9bc9c0913310ef2232b13d8e15e73e9d2b4 (commit)


- Log -----------------------------------------------------------------
commit 5d1f03f29e2794f6d1642dfedf10fc3e334937d0
Author: Andy Polyakov &lt;<A HREF="../../../mailman/listinfo/openssl-commits.html">appro at openssl.org</A>&gt;
Date:   Tue Feb 9 23:08:30 2016 +0100

    Configurations: engage x86[_64] ChaCha20 and Poly1305 modules.
    
    Reviewed-by: Rich Salz &lt;<A HREF="../../../mailman/listinfo/openssl-commits.html">rsalz at openssl.org</A>&gt;

commit e87c056745845ecaa6a884fa9cf0dc0c404f0c46
Author: Andy Polyakov &lt;<A HREF="../../../mailman/listinfo/openssl-commits.html">appro at openssl.org</A>&gt;
Date:   Sat Dec 19 14:16:47 2015 +0100

    poly1305/poly1305.c: work around -Wshadow warnings with POLY1305_ASM.
    
    Reviewed-by: Rich Salz &lt;<A HREF="../../../mailman/listinfo/openssl-commits.html">rsalz at openssl.org</A>&gt;

commit a98c648e40ea5158c8ba29b5a70ccc239d426a20
Author: Andy Polyakov &lt;<A HREF="../../../mailman/listinfo/openssl-commits.html">appro at openssl.org</A>&gt;
Date:   Sun Dec 13 21:40:20 2015 +0100

    x86[_64] assembly pack: add ChaCha20 and Poly1305 modules.
    
    Reviewed-by: Rich Salz &lt;<A HREF="../../../mailman/listinfo/openssl-commits.html">rsalz at openssl.org</A>&gt;

-----------------------------------------------------------------------

Summary of changes:
 Configurations/00-base-templates.conf  |    4 +
 crypto/chacha/Makefile.in              |    5 +
 crypto/chacha/asm/chacha-x86.pl        | 1128 ++++++++++++++++
 crypto/chacha/asm/chacha-x86_64.pl     | 2234 +++++++++++++++++++++++++++++++
 crypto/perlasm/x86gas.pl               |    2 +-
 crypto/poly1305/Makefile.in            |    4 +
 crypto/poly1305/asm/poly1305-x86.pl    | 1794 +++++++++++++++++++++++++
 crypto/poly1305/asm/poly1305-x86_64.pl | 2244 ++++++++++++++++++++++++++++++++
 crypto/poly1305/poly1305.c             |   15 +-
 test/evptests.txt                      |   48 +
 10 files changed, 7474 insertions(+), 4 deletions(-)
 create mode 100755 crypto/chacha/asm/chacha-x86.pl
 create mode 100755 crypto/chacha/asm/chacha-x86_64.pl
 create mode 100755 crypto/poly1305/asm/poly1305-x86.pl
 create mode 100755 crypto/poly1305/asm/poly1305-x86_64.pl

diff --git a/Configurations/00-base-templates.conf b/Configurations/00-base-templates.conf
index 527ed74..3cc078f 100644
--- a/Configurations/00-base-templates.conf
+++ b/Configurations/00-base-templates.conf
@@ -46,6 +46,8 @@
 	cmll_asm_src	=&gt; &quot;cmll-x86.s&quot;,
 	modes_asm_src	=&gt; &quot;ghash-x86.s&quot;,
 	padlock_asm_src	=&gt; &quot;e_padlock-x86.s&quot;,
+	chacha_asm_src	=&gt; &quot;chacha-x86.s&quot;,
+	poly1305_asm_src=&gt; &quot;poly1305-x86.s&quot;,
     },
     x86_elf_asm =&gt; {
 	template	=&gt; 1,
@@ -65,6 +67,8 @@
 	cmll_asm_src    =&gt; &quot;cmll-x86_64.s cmll_misc.c&quot;,
 	modes_asm_src   =&gt; &quot;ghash-x86_64.s aesni-gcm-x86_64.s&quot;,
 	padlock_asm_src =&gt; &quot;e_padlock-x86_64.s&quot;,
+	chacha_asm_src	=&gt; &quot;chacha-x86_64.s&quot;,
+	poly1305_asm_src=&gt; &quot;poly1305-x86_64.s&quot;,
     },
     ia64_asm =&gt; {
 	template	=&gt; 1,
diff --git a/crypto/chacha/Makefile.in b/crypto/chacha/Makefile.in
index 8987a85..6fb63c1 100644
--- a/crypto/chacha/Makefile.in
+++ b/crypto/chacha/Makefile.in
@@ -36,6 +36,11 @@ lib:	$(LIBOBJ)
 	$(RANLIB) $(LIB) || echo Never mind.
 	@touch lib
 
+chacha-x86.s:		asm/chacha-x86.pl
+	$(PERL) asm/chacha-x86.pl $(PERLASM_SCHEME) $(CFLAGS) $(PROCESSOR) &gt; $@
+chacha-x86_64.s:	asm/chacha-x86_64.pl
+	$(PERL) asm/chacha-x86_64.pl $(PERLASM_SCHEME) &gt; $@
+
 chacha-%.S:	asm/chacha-%.pl;	$(PERL) $&lt; $(PERLASM_SCHEME) $@
 
 files:
diff --git a/crypto/chacha/asm/chacha-x86.pl b/crypto/chacha/asm/chacha-x86.pl
new file mode 100755
index 0000000..5d097ad
--- /dev/null
+++ b/crypto/chacha/asm/chacha-x86.pl
@@ -0,0 +1,1128 @@
+#!/usr/bin/env perl
+#
+# ====================================================================
+# Written by Andy Polyakov &lt;<A HREF="../../../mailman/listinfo/openssl-commits.html">appro at openssl.org</A>&gt; for the OpenSSL
+# project. The module is, however, dual licensed under OpenSSL and
+# CRYPTOGAMS licenses depending on where you obtain it. For further
+# details see <A HREF="http://www.openssl.org/~appro/cryptogams/.">http://www.openssl.org/~appro/cryptogams/.</A>
+# ====================================================================
+#
+# January 2015
+#
+# ChaCha20 for x86.
+#
+# Performance in cycles per byte out of large buffer.
+#
+#		1xIALU/gcc	4xSSSE3
+# Pentium	17.5/+80%
+# PIII		14.2/+60%
+# P4		18.6/+84%
+# Core2		9.56/+89%	4.83
+# Westmere	9.50/+45%	3.35
+# Sandy Bridge	10.5/+47%	3.20
+# Haswell	8.15/+50%	2.83
+# Silvermont	17.4/+36%	8.35
+# Sledgehammer	10.2/+54%
+# Bulldozer	13.4/+50%	4.38(*)
+#
+# (*)	Bulldozer actually executes 4xXOP code path that delivers 3.55;
+
+$0 =~ m/(.*[\/\\])[^\/\\]+$/; $dir=$1;
+push(@INC,&quot;${dir}&quot;,&quot;${dir}../../perlasm&quot;);
+require &quot;x86asm.pl&quot;;
+
+&amp;asm_init($ARGV[0],&quot;chacha-x86.pl&quot;,$ARGV[$#ARGV] eq &quot;386&quot;);
+
+$xmm=$ymm=0;
+for (@ARGV) { $xmm=1 if (/-DOPENSSL_IA32_SSE2/); }
+
+$ymm=1 if ($xmm &amp;&amp;
+		`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2&gt;&amp;1`
+			=~ /GNU assembler version ([2-9]\.[0-9]+)/ &amp;&amp;
+		$1&gt;=2.19);	# first version supporting AVX
+
+$ymm=1 if ($xmm &amp;&amp; !$ymm &amp;&amp; $ARGV[0] eq &quot;win32n&quot; &amp;&amp;
+		`nasm -v 2&gt;&amp;1` =~ /NASM version ([2-9]\.[0-9]+)/ &amp;&amp;
+		$1&gt;=2.03);	# first version supporting AVX
+
+$ymm=1 if ($xmm &amp;&amp; !$ymm &amp;&amp; $ARGV[0] eq &quot;win32&quot; &amp;&amp;
+		`ml 2&gt;&amp;1` =~ /Version ([0-9]+)\./ &amp;&amp;
+		$1&gt;=10);	# first version supporting AVX
+
+$ymm=1 if ($xmm &amp;&amp; !$ymm &amp;&amp;
+		`$ENV{CC} -v 2&gt;&amp;1` =~ /(^clang version|based on LLVM) ([3-9]\.[0-9]+)/ &amp;&amp;
+		$2&gt;=3.0);	# first version supporting AVX
+
+$a=&quot;eax&quot;;
+($b,$b_)=(&quot;ebx&quot;,&quot;ebp&quot;);
+($c,$c_)=(&quot;ecx&quot;,&quot;esi&quot;);
+($d,$d_)=(&quot;edx&quot;,&quot;edi&quot;);
+
+sub QUARTERROUND {
+my ($ai,$bi,$ci,$di,$i)=@_;
+my ($an,$bn,$cn,$dn)=map(($_&amp;~3)+(($_+1)&amp;3),($ai,$bi,$ci,$di));	# next
+my ($ap,$bp,$cp,$dp)=map(($_&amp;~3)+(($_-1)&amp;3),($ai,$bi,$ci,$di));	# previous
+
+	#       a   b   c   d
+	#
+	#       0   4   8  12 &lt; even round
+	#       1   5   9  13
+	#       2   6  10  14
+	#       3   7  11  15
+	#       0   5  10  15 &lt; odd round
+	#       1   6  11  12
+	#       2   7   8  13
+	#       3   4   9  14
+
+	if ($i==0) {
+            my $j=4;
+	    ($ap,$bp,$cp,$dp)=map(($_&amp;~3)+(($_-$j--)&amp;3),($ap,$bp,$cp,$dp));
+	} elsif ($i==3) {
+            my $j=0;
+	    ($an,$bn,$cn,$dn)=map(($_&amp;~3)+(($_+$j++)&amp;3),($an,$bn,$cn,$dn));
+	} elsif ($i==4) {
+            my $j=4;
+	    ($ap,$bp,$cp,$dp)=map(($_&amp;~3)+(($_+$j--)&amp;3),($ap,$bp,$cp,$dp));
+	} elsif ($i==7) {
+            my $j=0;
+	    ($an,$bn,$cn,$dn)=map(($_&amp;~3)+(($_-$j++)&amp;3),($an,$bn,$cn,$dn));
+	}
+
+	#&amp;add	($a,$b);			# see elsewhere
+	&amp;xor	($d,$a);
+	 &amp;mov	(&amp;DWP(4*$cp,&quot;esp&quot;),$c_)		if ($ai&gt;0 &amp;&amp; $ai&lt;3);
+	&amp;rol	($d,16);
+	 &amp;mov	(&amp;DWP(4*$bp,&quot;esp&quot;),$b_)		if ($i!=0);
+	&amp;add	($c,$d);
+	 &amp;mov	($c_,&amp;DWP(4*$cn,&quot;esp&quot;))		if ($ai&gt;0 &amp;&amp; $ai&lt;3);
+	&amp;xor	($b,$c);
+	 &amp;mov	($d_,&amp;DWP(4*$dn,&quot;esp&quot;))		if ($di!=$dn);
+	&amp;rol	($b,12);
+	 &amp;mov	($b_,&amp;DWP(4*$bn,&quot;esp&quot;))		if ($i&lt;7);
+	 &amp;mov	($b_,&amp;DWP(128,&quot;esp&quot;))		if ($i==7);	# loop counter
+	&amp;add	($a,$b);
+	&amp;xor	($d,$a);
+	&amp;mov	(&amp;DWP(4*$ai,&quot;esp&quot;),$a);
+	&amp;rol	($d,8);
+	&amp;mov	($a,&amp;DWP(4*$an,&quot;esp&quot;));
+	&amp;add	($c,$d);
+	&amp;mov	(&amp;DWP(4*$di,&quot;esp&quot;),$d)		if ($di!=$dn);
+	&amp;mov	($d_,$d)			if ($di==$dn);
+	&amp;xor	($b,$c);
+	 &amp;add	($a,$b_)			if ($i&lt;7);	# elsewhere
+	&amp;rol	($b,7);
+
+	($b,$b_)=($b_,$b);
+	($c,$c_)=($c_,$c);
+	($d,$d_)=($d_,$d);
+}
+
+&amp;static_label(&quot;ssse3_shortcut&quot;);
+&amp;static_label(&quot;xop_shortcut&quot;);
+&amp;static_label(&quot;ssse3_data&quot;);
+&amp;static_label(&quot;pic_point&quot;);
+
+&amp;function_begin(&quot;ChaCha20_ctr32&quot;);
+if ($xmm) {
+	&amp;call	(&amp;label(&quot;pic_point&quot;));
+&amp;set_label(&quot;pic_point&quot;);
+	&amp;blindpop(&quot;eax&quot;);
+	&amp;picmeup(&quot;ebp&quot;,&quot;OPENSSL_ia32cap_P&quot;,&quot;eax&quot;,&amp;label(&quot;pic_point&quot;));
+	&amp;test	(&amp;DWP(0,&quot;ebp&quot;),1&lt;&lt;24);		# test FXSR bit
+	&amp;jz	(&amp;label(&quot;x86&quot;));
+	&amp;test	(&amp;DWP(4,&quot;ebp&quot;),1&lt;&lt;9);		# test SSSE3 bit
+	&amp;jz	(&amp;label(&quot;x86&quot;));
+	&amp;jmp	(&amp;label(&quot;ssse3_shortcut&quot;));
+&amp;set_label(&quot;x86&quot;);
+}
+	&amp;mov	(&quot;esi&quot;,&amp;wparam(3));		# key
+	&amp;mov	(&quot;edi&quot;,&amp;wparam(4));		# counter and nonce
+
+	&amp;stack_push(33);
+
+	&amp;mov	(&quot;eax&quot;,&amp;DWP(4*0,&quot;esi&quot;));	# copy key
+	&amp;mov	(&quot;ebx&quot;,&amp;DWP(4*1,&quot;esi&quot;));
+	&amp;mov	(&quot;ecx&quot;,&amp;DWP(4*2,&quot;esi&quot;));
+	&amp;mov	(&quot;edx&quot;,&amp;DWP(4*3,&quot;esi&quot;));
+	&amp;mov	(&amp;DWP(64+4*4,&quot;esp&quot;),&quot;eax&quot;);
+	&amp;mov	(&amp;DWP(64+4*5,&quot;esp&quot;),&quot;ebx&quot;);
+	&amp;mov	(&amp;DWP(64+4*6,&quot;esp&quot;),&quot;ecx&quot;);
+	&amp;mov	(&amp;DWP(64+4*7,&quot;esp&quot;),&quot;edx&quot;);
+	&amp;mov	(&quot;eax&quot;,&amp;DWP(4*4,&quot;esi&quot;));
+	&amp;mov	(&quot;ebx&quot;,&amp;DWP(4*5,&quot;esi&quot;));
+	&amp;mov	(&quot;ecx&quot;,&amp;DWP(4*6,&quot;esi&quot;));
+	&amp;mov	(&quot;edx&quot;,&amp;DWP(4*7,&quot;esi&quot;));
+	&amp;mov	(&amp;DWP(64+4*8,&quot;esp&quot;),&quot;eax&quot;);
+	&amp;mov	(&amp;DWP(64+4*9,&quot;esp&quot;),&quot;ebx&quot;);
+	&amp;mov	(&amp;DWP(64+4*10,&quot;esp&quot;),&quot;ecx&quot;);
+	&amp;mov	(&amp;DWP(64+4*11,&quot;esp&quot;),&quot;edx&quot;);
+	&amp;mov	(&quot;eax&quot;,&amp;DWP(4*0,&quot;edi&quot;));	# copy counter and nonce
+	&amp;mov	(&quot;ebx&quot;,&amp;DWP(4*1,&quot;edi&quot;));
+	&amp;mov	(&quot;ecx&quot;,&amp;DWP(4*2,&quot;edi&quot;));
+	&amp;mov	(&quot;edx&quot;,&amp;DWP(4*3,&quot;edi&quot;));
+	&amp;sub	(&quot;eax&quot;,1);
+	&amp;mov	(&amp;DWP(64+4*12,&quot;esp&quot;),&quot;eax&quot;);
+	&amp;mov	(&amp;DWP(64+4*13,&quot;esp&quot;),&quot;ebx&quot;);
+	&amp;mov	(&amp;DWP(64+4*14,&quot;esp&quot;),&quot;ecx&quot;);
+	&amp;mov	(&amp;DWP(64+4*15,&quot;esp&quot;),&quot;edx&quot;);
+	&amp;jmp	(&amp;label(&quot;entry&quot;));
+
+&amp;set_label(&quot;outer_loop&quot;,16);
+	&amp;mov	(&amp;wparam(1),$b);		# save input
+	&amp;mov	(&amp;wparam(0),$a);		# save output
+	&amp;mov	(&amp;wparam(2),$c);		# save len
+&amp;set_label(&quot;entry&quot;);
+	&amp;mov	($a,0x61707865);
+	&amp;mov	(&amp;DWP(4*1,&quot;esp&quot;),0x3320646e);
+	&amp;mov	(&amp;DWP(4*2,&quot;esp&quot;),0x79622d32);
+	&amp;mov	(&amp;DWP(4*3,&quot;esp&quot;),0x6b206574);
+
+	&amp;mov	($b, &amp;DWP(64+4*5,&quot;esp&quot;));	# copy key material
+	&amp;mov	($b_,&amp;DWP(64+4*6,&quot;esp&quot;));
+	&amp;mov	($c, &amp;DWP(64+4*10,&quot;esp&quot;));
+	&amp;mov	($c_,&amp;DWP(64+4*11,&quot;esp&quot;));
+	&amp;mov	($d, &amp;DWP(64+4*13,&quot;esp&quot;));
+	&amp;mov	($d_,&amp;DWP(64+4*14,&quot;esp&quot;));
+	&amp;mov	(&amp;DWP(4*5,&quot;esp&quot;),$b);
+	&amp;mov	(&amp;DWP(4*6,&quot;esp&quot;),$b_);
+	&amp;mov	(&amp;DWP(4*10,&quot;esp&quot;),$c);
+	&amp;mov	(&amp;DWP(4*11,&quot;esp&quot;),$c_);
+	&amp;mov	(&amp;DWP(4*13,&quot;esp&quot;),$d);
+	&amp;mov	(&amp;DWP(4*14,&quot;esp&quot;),$d_);
+
+	&amp;mov	($b, &amp;DWP(64+4*7,&quot;esp&quot;));
+	&amp;mov	($d_,&amp;DWP(64+4*15,&quot;esp&quot;));
+	&amp;mov	($d, &amp;DWP(64+4*12,&quot;esp&quot;));
+	&amp;mov	($b_,&amp;DWP(64+4*4,&quot;esp&quot;));
+	&amp;mov	($c, &amp;DWP(64+4*8,&quot;esp&quot;));
+	&amp;mov	($c_,&amp;DWP(64+4*9,&quot;esp&quot;));
+	&amp;add	($d,1);				# counter value
+	&amp;mov	(&amp;DWP(4*7,&quot;esp&quot;),$b);
+	&amp;mov	(&amp;DWP(4*15,&quot;esp&quot;),$d_);
+	&amp;mov	(&amp;DWP(64+4*12,&quot;esp&quot;),$d);	# save counter value
+
+	&amp;mov	($b,10);			# loop counter
+	&amp;jmp	(&amp;label(&quot;loop&quot;));
+
+&amp;set_label(&quot;loop&quot;,16);
+	&amp;add	($a,$b_);			# elsewhere
+	&amp;mov	(&amp;DWP(128,&quot;esp&quot;),$b);		# save loop counter
+	&amp;mov	($b,$b_);
+	&amp;QUARTERROUND(0, 4, 8, 12, 0);
+	&amp;QUARTERROUND(1, 5, 9, 13, 1);
+	&amp;QUARTERROUND(2, 6,10, 14, 2);
+	&amp;QUARTERROUND(3, 7,11, 15, 3);
+	&amp;QUARTERROUND(0, 5,10, 15, 4);
+	&amp;QUARTERROUND(1, 6,11, 12, 5);
+	&amp;QUARTERROUND(2, 7, 8, 13, 6);
+	&amp;QUARTERROUND(3, 4, 9, 14, 7);
+	&amp;dec	($b);
+	&amp;jnz	(&amp;label(&quot;loop&quot;));
+
+	&amp;mov	($b,&amp;wparam(3));		# load len
+
+	&amp;add	($a,0x61707865);		# accumulate key material
+	&amp;add	($b_,&amp;DWP(64+4*4,&quot;esp&quot;));
+	&amp;add	($c, &amp;DWP(64+4*8,&quot;esp&quot;));
+	&amp;add	($c_,&amp;DWP(64+4*9,&quot;esp&quot;));
+
+	&amp;cmp	($b,64);
+	&amp;jb	(&amp;label(&quot;tail&quot;));
+
+	&amp;mov	($b,&amp;wparam(1));		# load input pointer
+	&amp;add	($d, &amp;DWP(64+4*12,&quot;esp&quot;));
+	&amp;add	($d_,&amp;DWP(64+4*14,&quot;esp&quot;));
+
+	&amp;xor	($a, &amp;DWP(4*0,$b));		# xor with input
+	&amp;xor	($b_,&amp;DWP(4*4,$b));
+	&amp;mov	(&amp;DWP(4*0,&quot;esp&quot;),$a);
+	&amp;mov	($a,&amp;wparam(0));		# load output pointer
+	&amp;xor	($c, &amp;DWP(4*8,$b));
+	&amp;xor	($c_,&amp;DWP(4*9,$b));
+	&amp;xor	($d, &amp;DWP(4*12,$b));
+	&amp;xor	($d_,&amp;DWP(4*14,$b));
+	&amp;mov	(&amp;DWP(4*4,$a),$b_);		# write output
+	&amp;mov	(&amp;DWP(4*8,$a),$c);
+	&amp;mov	(&amp;DWP(4*9,$a),$c_);
+	&amp;mov	(&amp;DWP(4*12,$a),$d);
+	&amp;mov	(&amp;DWP(4*14,$a),$d_);
+
+	&amp;mov	($b_,&amp;DWP(4*1,&quot;esp&quot;));
+	&amp;mov	($c, &amp;DWP(4*2,&quot;esp&quot;));
+	&amp;mov	($c_,&amp;DWP(4*3,&quot;esp&quot;));
+	&amp;mov	($d, &amp;DWP(4*5,&quot;esp&quot;));
+	&amp;mov	($d_,&amp;DWP(4*6,&quot;esp&quot;));
+	&amp;add	($b_,0x3320646e);		# accumulate key material
+	&amp;add	($c, 0x79622d32);
+	&amp;add	($c_,0x6b206574);
+	&amp;add	($d, &amp;DWP(64+4*5,&quot;esp&quot;));
+	&amp;add	($d_,&amp;DWP(64+4*6,&quot;esp&quot;));
+	&amp;xor	($b_,&amp;DWP(4*1,$b));
+	&amp;xor	($c, &amp;DWP(4*2,$b));
+	&amp;xor	($c_,&amp;DWP(4*3,$b));
+	&amp;xor	($d, &amp;DWP(4*5,$b));
+	&amp;xor	($d_,&amp;DWP(4*6,$b));
+	&amp;mov	(&amp;DWP(4*1,$a),$b_);
+	&amp;mov	(&amp;DWP(4*2,$a),$c);
+	&amp;mov	(&amp;DWP(4*3,$a),$c_);
+	&amp;mov	(&amp;DWP(4*5,$a),$d);
+	&amp;mov	(&amp;DWP(4*6,$a),$d_);
+
+	&amp;mov	($b_,&amp;DWP(4*7,&quot;esp&quot;));
+	&amp;mov	($c, &amp;DWP(4*10,&quot;esp&quot;));
+	&amp;mov	($c_,&amp;DWP(4*11,&quot;esp&quot;));
+	&amp;mov	($d, &amp;DWP(4*13,&quot;esp&quot;));
+	&amp;mov	($d_,&amp;DWP(4*15,&quot;esp&quot;));
+	&amp;add	($b_,&amp;DWP(64+4*7,&quot;esp&quot;));
+	&amp;add	($c, &amp;DWP(64+4*10,&quot;esp&quot;));
+	&amp;add	($c_,&amp;DWP(64+4*11,&quot;esp&quot;));
+	&amp;add	($d, &amp;DWP(64+4*13,&quot;esp&quot;));
+	&amp;add	($d_,&amp;DWP(64+4*15,&quot;esp&quot;));
+	&amp;xor	($b_,&amp;DWP(4*7,$b));
+	&amp;xor	($c, &amp;DWP(4*10,$b));
+	&amp;xor	($c_,&amp;DWP(4*11,$b));
+	&amp;xor	($d, &amp;DWP(4*13,$b));
+	&amp;xor	($d_,&amp;DWP(4*15,$b));
+	&amp;lea	($b,&amp;DWP(4*16,$b));
+	&amp;mov	(&amp;DWP(4*7,$a),$b_);
+	&amp;mov	($b_,&amp;DWP(4*0,&quot;esp&quot;));
+	&amp;mov	(&amp;DWP(4*10,$a),$c);
+	&amp;mov	($c,&amp;wparam(2));		# len
+	&amp;mov	(&amp;DWP(4*11,$a),$c_);
+	&amp;mov	(&amp;DWP(4*13,$a),$d);
+	&amp;mov	(&amp;DWP(4*15,$a),$d_);
+	&amp;mov	(&amp;DWP(4*0,$a),$b_);
+	&amp;lea	($a,&amp;DWP(4*16,$a));
+	&amp;sub	($c,64);
+	&amp;jnz	(&amp;label(&quot;outer_loop&quot;));
+
+	&amp;jmp	(&amp;label(&quot;done&quot;));
+
+&amp;set_label(&quot;tail&quot;);
+	&amp;add	($d, &amp;DWP(64+4*12,&quot;esp&quot;));
+	&amp;add	($d_,&amp;DWP(64+4*14,&quot;esp&quot;));
+	&amp;mov	(&amp;DWP(4*0,&quot;esp&quot;),$a);
+	&amp;mov	(&amp;DWP(4*4,&quot;esp&quot;),$b_);
+	&amp;mov	(&amp;DWP(4*8,&quot;esp&quot;),$c);
+	&amp;mov	(&amp;DWP(4*9,&quot;esp&quot;),$c_);
+	&amp;mov	(&amp;DWP(4*12,&quot;esp&quot;),$d);
+	&amp;mov	(&amp;DWP(4*14,&quot;esp&quot;),$d_);
+
+	&amp;mov	($b_,&amp;DWP(4*1,&quot;esp&quot;));
+	&amp;mov	($c, &amp;DWP(4*2,&quot;esp&quot;));
+	&amp;mov	($c_,&amp;DWP(4*3,&quot;esp&quot;));
+	&amp;mov	($d, &amp;DWP(4*5,&quot;esp&quot;));
+	&amp;mov	($d_,&amp;DWP(4*6,&quot;esp&quot;));
+	&amp;add	($b_,0x3320646e);		# accumulate key material
+	&amp;add	($c, 0x79622d32);
+	&amp;add	($c_,0x6b206574);
+	&amp;add	($d, &amp;DWP(64+4*5,&quot;esp&quot;));
+	&amp;add	($d_,&amp;DWP(64+4*6,&quot;esp&quot;));
+	&amp;mov	(&amp;DWP(4*1,&quot;esp&quot;),$b_);
+	&amp;mov	(&amp;DWP(4*2,&quot;esp&quot;),$c);
+	&amp;mov	(&amp;DWP(4*3,&quot;esp&quot;),$c_);
+	&amp;mov	(&amp;DWP(4*5,&quot;esp&quot;),$d);
+	&amp;mov	(&amp;DWP(4*6,&quot;esp&quot;),$d_);
+
+	&amp;mov	($b_,&amp;DWP(4*7,&quot;esp&quot;));
+	&amp;mov	($c, &amp;DWP(4*10,&quot;esp&quot;));
+	&amp;mov	($c_,&amp;DWP(4*11,&quot;esp&quot;));
+	&amp;mov	($d, &amp;DWP(4*13,&quot;esp&quot;));
+	&amp;mov	($d_,&amp;DWP(4*15,&quot;esp&quot;));
+	&amp;add	($b_,&amp;DWP(64+4*7,&quot;esp&quot;));
+	&amp;add	($c, &amp;DWP(64+4*10,&quot;esp&quot;));
+	&amp;add	($c_,&amp;DWP(64+4*11,&quot;esp&quot;));
+	&amp;add	($d, &amp;DWP(64+4*13,&quot;esp&quot;));
+	&amp;add	($d_,&amp;DWP(64+4*15,&quot;esp&quot;));
+	&amp;mov	(&amp;DWP(4*7,&quot;esp&quot;),$b_);
+	&amp;mov	($b_,&amp;wparam(1));		# load input
+	&amp;mov	(&amp;DWP(4*10,&quot;esp&quot;),$c);
+	&amp;mov	($c,&amp;wparam(0));		# load output
+	&amp;mov	(&amp;DWP(4*11,&quot;esp&quot;),$c_);
+	&amp;xor	($c_,$c_);
+	&amp;mov	(&amp;DWP(4*13,&quot;esp&quot;),$d);
+	&amp;mov	(&amp;DWP(4*15,&quot;esp&quot;),$d_);
+
+	&amp;xor	(&quot;eax&quot;,&quot;eax&quot;);
+	&amp;xor	(&quot;edx&quot;,&quot;edx&quot;);
+&amp;set_label(&quot;tail_loop&quot;);
+	&amp;movb	(&quot;al&quot;,&amp;DWP(0,$c_,$b_));
+	&amp;movb	(&quot;dl&quot;,&amp;DWP(0,&quot;esp&quot;,$c_));
+	&amp;lea	($c_,&amp;DWP(1,$c_));
+	&amp;xor	(&quot;al&quot;,&quot;dl&quot;);
+	&amp;mov	(&amp;DWP(-1,$c,$c_),&quot;al&quot;);
+	&amp;dec	($b);
+	&amp;jnz	(&amp;label(&quot;tail_loop&quot;));
+
+&amp;set_label(&quot;done&quot;);
+	&amp;stack_pop(33);
+&amp;function_end(&quot;ChaCha20_ctr32&quot;);
+
+if ($xmm) {
+my ($xa,$xa_,$xb,$xb_,$xc,$xc_,$xd,$xd_)=map(&quot;xmm$_&quot;,(0..7));
+my ($out,$inp,$len)=(&quot;edi&quot;,&quot;esi&quot;,&quot;ecx&quot;);
+
+sub QUARTERROUND_SSSE3 {
+my ($ai,$bi,$ci,$di,$i)=@_;
+my ($an,$bn,$cn,$dn)=map(($_&amp;~3)+(($_+1)&amp;3),($ai,$bi,$ci,$di));	# next
+my ($ap,$bp,$cp,$dp)=map(($_&amp;~3)+(($_-1)&amp;3),($ai,$bi,$ci,$di));	# previous
+
+	#       a   b   c   d
+	#
+	#       0   4   8  12 &lt; even round
+	#       1   5   9  13
+	#       2   6  10  14
+	#       3   7  11  15
+	#       0   5  10  15 &lt; odd round
+	#       1   6  11  12
+	#       2   7   8  13
+	#       3   4   9  14
+
+	if ($i==0) {
+            my $j=4;
+	    ($ap,$bp,$cp,$dp)=map(($_&amp;~3)+(($_-$j--)&amp;3),($ap,$bp,$cp,$dp));
+	} elsif ($i==3) {
+            my $j=0;
+	    ($an,$bn,$cn,$dn)=map(($_&amp;~3)+(($_+$j++)&amp;3),($an,$bn,$cn,$dn));
+	} elsif ($i==4) {
+            my $j=4;
+	    ($ap,$bp,$cp,$dp)=map(($_&amp;~3)+(($_+$j--)&amp;3),($ap,$bp,$cp,$dp));
+	} elsif ($i==7) {
+            my $j=0;
+	    ($an,$bn,$cn,$dn)=map(($_&amp;~3)+(($_-$j++)&amp;3),($an,$bn,$cn,$dn));
+	}
+
+	#&amp;paddd	($xa,$xb);			# see elsewhere
+	#&amp;pxor	($xd,$xa);			# see elsewhere
+	 &amp;movdqa(&amp;QWP(16*$cp-128,&quot;ebx&quot;),$xc_)	if ($ai&gt;0 &amp;&amp; $ai&lt;3);
+	&amp;pshufb	($xd,&amp;QWP(0,&quot;eax&quot;));		# rot16
+	 &amp;movdqa(&amp;QWP(16*$bp-128,&quot;ebx&quot;),$xb_)	if ($i!=0);
+	&amp;paddd	($xc,$xd);
+	 &amp;movdqa($xc_,&amp;QWP(16*$cn-128,&quot;ebx&quot;))	if ($ai&gt;0 &amp;&amp; $ai&lt;3);
+	&amp;pxor	($xb,$xc);
+	 &amp;movdqa($xb_,&amp;QWP(16*$bn-128,&quot;ebx&quot;))	if ($i&lt;7);
+	&amp;movdqa	($xa_,$xb);			# borrow as temporary
+	&amp;pslld	($xb,12);
+	&amp;psrld	($xa_,20);
+	&amp;por	($xb,$xa_);
+	 &amp;movdqa($xa_,&amp;QWP(16*$an-128,&quot;ebx&quot;));
+	&amp;paddd	($xa,$xb);
+	 &amp;movdqa($xd_,&amp;QWP(16*$dn-128,&quot;ebx&quot;))	if ($di!=$dn);
+	&amp;pxor	($xd,$xa);
+	&amp;movdqa	(&amp;QWP(16*$ai-128,&quot;ebx&quot;),$xa);
+	&amp;pshufb	($xd,&amp;QWP(16,&quot;eax&quot;));		# rot8
+	&amp;paddd	($xc,$xd);
+	&amp;movdqa	(&amp;QWP(16*$di-128,&quot;ebx&quot;),$xd)	if ($di!=$dn);
+	&amp;movdqa	($xd_,$xd)			if ($di==$dn);
+	&amp;pxor	($xb,$xc);
+	 &amp;paddd	($xa_,$xb_)			if ($i&lt;7);	# elsewhere
+	&amp;movdqa	($xa,$xb);			# borrow as temporary
+	&amp;pslld	($xb,7);
+	&amp;psrld	($xa,25);
+	 &amp;pxor	($xd_,$xa_)			if ($i&lt;7);	# elsewhere
+	&amp;por	($xb,$xa);
+
+	($xa,$xa_)=($xa_,$xa);
+	($xb,$xb_)=($xb_,$xb);
+	($xc,$xc_)=($xc_,$xc);
+	($xd,$xd_)=($xd_,$xd);
+}
+
+&amp;function_begin(&quot;ChaCha20_ssse3&quot;);
+&amp;set_label(&quot;ssse3_shortcut&quot;);
+	&amp;test		(&amp;DWP(4,&quot;ebp&quot;),1&lt;&lt;11);		# test XOP bit
+	&amp;jnz		(&amp;label(&quot;xop_shortcut&quot;));
+
+	&amp;mov		($out,&amp;wparam(0));
+	&amp;mov		($inp,&amp;wparam(1));
+	&amp;mov		($len,&amp;wparam(2));
+	&amp;mov		(&quot;edx&quot;,&amp;wparam(3));		# key
+	&amp;mov		(&quot;ebx&quot;,&amp;wparam(4));		# counter and nonce
+
+	&amp;mov		(&quot;ebp&quot;,&quot;esp&quot;);
+	&amp;stack_push	(131);
+	&amp;and		(&quot;esp&quot;,-64);
+	&amp;mov		(&amp;DWP(512,&quot;esp&quot;),&quot;ebp&quot;);
+
+	&amp;lea		(&quot;eax&quot;,&amp;DWP(&amp;label(&quot;ssse3_data&quot;).&quot;-&quot;.
+				    &amp;label(&quot;pic_point&quot;),&quot;eax&quot;));
+	&amp;movdqu		(&quot;xmm3&quot;,&amp;QWP(0,&quot;ebx&quot;));		# counter and nonce
+
+	&amp;cmp		($len,64*4);
+	&amp;jb		(&amp;label(&quot;1x&quot;));
+
+	&amp;mov		(&amp;DWP(512+4,&quot;esp&quot;),&quot;edx&quot;);	# offload pointers
+	&amp;mov		(&amp;DWP(512+8,&quot;esp&quot;),&quot;ebx&quot;);
+	&amp;sub		($len,64*4);			# bias len
+	&amp;lea		(&quot;ebp&quot;,&amp;DWP(256+128,&quot;esp&quot;));	# size optimization
+
+	&amp;movdqu		(&quot;xmm7&quot;,&amp;DWP(0,&quot;edx&quot;));		# key
+	&amp;pshufd		(&quot;xmm0&quot;,&quot;xmm3&quot;,0x00);
+	&amp;pshufd		(&quot;xmm1&quot;,&quot;xmm3&quot;,0x55);
+	&amp;pshufd		(&quot;xmm2&quot;,&quot;xmm3&quot;,0xaa);
+	&amp;pshufd		(&quot;xmm3&quot;,&quot;xmm3&quot;,0xff);
+	 &amp;paddd		(&quot;xmm0&quot;,&amp;QWP(16*3,&quot;eax&quot;));	# fix counters
+	&amp;pshufd		(&quot;xmm4&quot;,&quot;xmm7&quot;,0x00);
+	&amp;pshufd		(&quot;xmm5&quot;,&quot;xmm7&quot;,0x55);
+	 &amp;psubd		(&quot;xmm0&quot;,&amp;QWP(16*4,&quot;eax&quot;));
+	&amp;pshufd		(&quot;xmm6&quot;,&quot;xmm7&quot;,0xaa);
+	&amp;pshufd		(&quot;xmm7&quot;,&quot;xmm7&quot;,0xff);
+	&amp;movdqa		(&amp;QWP(16*12-128,&quot;ebp&quot;),&quot;xmm0&quot;);
+	&amp;movdqa		(&amp;QWP(16*13-128,&quot;ebp&quot;),&quot;xmm1&quot;);
+	&amp;movdqa		(&amp;QWP(16*14-128,&quot;ebp&quot;),&quot;xmm2&quot;);
+	&amp;movdqa		(&amp;QWP(16*15-128,&quot;ebp&quot;),&quot;xmm3&quot;);
+	 &amp;movdqu	(&quot;xmm3&quot;,&amp;DWP(16,&quot;edx&quot;));	# key
+	&amp;movdqa		(&amp;QWP(16*4-128,&quot;ebp&quot;),&quot;xmm4&quot;);
+	&amp;movdqa		(&amp;QWP(16*5-128,&quot;ebp&quot;),&quot;xmm5&quot;);
+	&amp;movdqa		(&amp;QWP(16*6-128,&quot;ebp&quot;),&quot;xmm6&quot;);
+	&amp;movdqa		(&amp;QWP(16*7-128,&quot;ebp&quot;),&quot;xmm7&quot;);
+	 &amp;movdqa	(&quot;xmm7&quot;,&amp;DWP(16*2,&quot;eax&quot;));	# sigma
+	 &amp;lea		(&quot;ebx&quot;,&amp;DWP(128,&quot;esp&quot;));	# size optimization
+
+	&amp;pshufd		(&quot;xmm0&quot;,&quot;xmm3&quot;,0x00);
+	&amp;pshufd		(&quot;xmm1&quot;,&quot;xmm3&quot;,0x55);
+	&amp;pshufd		(&quot;xmm2&quot;,&quot;xmm3&quot;,0xaa);
+	&amp;pshufd		(&quot;xmm3&quot;,&quot;xmm3&quot;,0xff);
+	&amp;pshufd		(&quot;xmm4&quot;,&quot;xmm7&quot;,0x00);
+	&amp;pshufd		(&quot;xmm5&quot;,&quot;xmm7&quot;,0x55);
+	&amp;pshufd		(&quot;xmm6&quot;,&quot;xmm7&quot;,0xaa);
+	&amp;pshufd		(&quot;xmm7&quot;,&quot;xmm7&quot;,0xff);
+	&amp;movdqa		(&amp;QWP(16*8-128,&quot;ebp&quot;),&quot;xmm0&quot;);
+	&amp;movdqa		(&amp;QWP(16*9-128,&quot;ebp&quot;),&quot;xmm1&quot;);
+	&amp;movdqa		(&amp;QWP(16*10-128,&quot;ebp&quot;),&quot;xmm2&quot;);
+	&amp;movdqa		(&amp;QWP(16*11-128,&quot;ebp&quot;),&quot;xmm3&quot;);
+	&amp;movdqa		(&amp;QWP(16*0-128,&quot;ebp&quot;),&quot;xmm4&quot;);
+	&amp;movdqa		(&amp;QWP(16*1-128,&quot;ebp&quot;),&quot;xmm5&quot;);
+	&amp;movdqa		(&amp;QWP(16*2-128,&quot;ebp&quot;),&quot;xmm6&quot;);
+	&amp;movdqa		(&amp;QWP(16*3-128,&quot;ebp&quot;),&quot;xmm7&quot;);
+
+	&amp;lea		($inp,&amp;DWP(128,$inp));		# size optimization
+	&amp;lea		($out,&amp;DWP(128,$out));		# size optimization
+	&amp;jmp		(&amp;label(&quot;outer_loop&quot;));
+
+&amp;set_label(&quot;outer_loop&quot;,16);
+	#&amp;movdqa	(&quot;xmm0&quot;,&amp;QWP(16*0-128,&quot;ebp&quot;));	# copy key material
+	&amp;movdqa		(&quot;xmm1&quot;,&amp;QWP(16*1-128,&quot;ebp&quot;));
+	&amp;movdqa		(&quot;xmm2&quot;,&amp;QWP(16*2-128,&quot;ebp&quot;));
+	&amp;movdqa		(&quot;xmm3&quot;,&amp;QWP(16*3-128,&quot;ebp&quot;));
+	#&amp;movdqa	(&quot;xmm4&quot;,&amp;QWP(16*4-128,&quot;ebp&quot;));
+	&amp;movdqa		(&quot;xmm5&quot;,&amp;QWP(16*5-128,&quot;ebp&quot;));
+	&amp;movdqa		(&quot;xmm6&quot;,&amp;QWP(16*6-128,&quot;ebp&quot;));
+	&amp;movdqa		(&quot;xmm7&quot;,&amp;QWP(16*7-128,&quot;ebp&quot;));
+	#&amp;movdqa	(&amp;QWP(16*0-128,&quot;ebx&quot;),&quot;xmm0&quot;);
+	&amp;movdqa		(&amp;QWP(16*1-128,&quot;ebx&quot;),&quot;xmm1&quot;);
+	&amp;movdqa		(&amp;QWP(16*2-128,&quot;ebx&quot;),&quot;xmm2&quot;);
+	&amp;movdqa		(&amp;QWP(16*3-128,&quot;ebx&quot;),&quot;xmm3&quot;);
+	#&amp;movdqa	(&amp;QWP(16*4-128,&quot;ebx&quot;),&quot;xmm4&quot;);
+	&amp;movdqa		(&amp;QWP(16*5-128,&quot;ebx&quot;),&quot;xmm5&quot;);
+	&amp;movdqa		(&amp;QWP(16*6-128,&quot;ebx&quot;),&quot;xmm6&quot;);
+	&amp;movdqa		(&amp;QWP(16*7-128,&quot;ebx&quot;),&quot;xmm7&quot;);
+	#&amp;movdqa	(&quot;xmm0&quot;,&amp;QWP(16*8-128,&quot;ebp&quot;));
+	#&amp;movdqa	(&quot;xmm1&quot;,&amp;QWP(16*9-128,&quot;ebp&quot;));
+	&amp;movdqa		(&quot;xmm2&quot;,&amp;QWP(16*10-128,&quot;ebp&quot;));
+	&amp;movdqa		(&quot;xmm3&quot;,&amp;QWP(16*11-128,&quot;ebp&quot;));
+	&amp;movdqa		(&quot;xmm4&quot;,&amp;QWP(16*12-128,&quot;ebp&quot;));
+	&amp;movdqa		(&quot;xmm5&quot;,&amp;QWP(16*13-128,&quot;ebp&quot;));
+	&amp;movdqa		(&quot;xmm6&quot;,&amp;QWP(16*14-128,&quot;ebp&quot;));
+	&amp;movdqa		(&quot;xmm7&quot;,&amp;QWP(16*15-128,&quot;ebp&quot;));
+	&amp;paddd		(&quot;xmm4&quot;,&amp;QWP(16*4,&quot;eax&quot;));	# counter value
+	#&amp;movdqa	(&amp;QWP(16*8-128,&quot;ebx&quot;),&quot;xmm0&quot;);
+	#&amp;movdqa	(&amp;QWP(16*9-128,&quot;ebx&quot;),&quot;xmm1&quot;);
+	&amp;movdqa		(&amp;QWP(16*10-128,&quot;ebx&quot;),&quot;xmm2&quot;);
+	&amp;movdqa		(&amp;QWP(16*11-128,&quot;ebx&quot;),&quot;xmm3&quot;);
+	&amp;movdqa		(&amp;QWP(16*12-128,&quot;ebx&quot;),&quot;xmm4&quot;);
+	&amp;movdqa		(&amp;QWP(16*13-128,&quot;ebx&quot;),&quot;xmm5&quot;);
+	&amp;movdqa		(&amp;QWP(16*14-128,&quot;ebx&quot;),&quot;xmm6&quot;);
+	&amp;movdqa		(&amp;QWP(16*15-128,&quot;ebx&quot;),&quot;xmm7&quot;);
+	&amp;movdqa		(&amp;QWP(16*12-128,&quot;ebp&quot;),&quot;xmm4&quot;);	# save counter value
+
+	&amp;movdqa		($xa, &amp;QWP(16*0-128,&quot;ebp&quot;));
+	&amp;movdqa		($xd, &quot;xmm4&quot;);
+	&amp;movdqa		($xb_,&amp;QWP(16*4-128,&quot;ebp&quot;));
+	&amp;movdqa		($xc, &amp;QWP(16*8-128,&quot;ebp&quot;));
+	&amp;movdqa		($xc_,&amp;QWP(16*9-128,&quot;ebp&quot;));
+
+	&amp;mov		(&quot;edx&quot;,10);			# loop counter
+	&amp;nop		();
+
+&amp;set_label(&quot;loop&quot;,16);
+	&amp;paddd		($xa,$xb_);			# elsewhere
+	&amp;movdqa		($xb,$xb_);
+	&amp;pxor		($xd,$xa);			# elsewhere
+	&amp;QUARTERROUND_SSSE3(0, 4, 8, 12, 0);
+	&amp;QUARTERROUND_SSSE3(1, 5, 9, 13, 1);
+	&amp;QUARTERROUND_SSSE3(2, 6,10, 14, 2);
+	&amp;QUARTERROUND_SSSE3(3, 7,11, 15, 3);
+	&amp;QUARTERROUND_SSSE3(0, 5,10, 15, 4);
+	&amp;QUARTERROUND_SSSE3(1, 6,11, 12, 5);
+	&amp;QUARTERROUND_SSSE3(2, 7, 8, 13, 6);
+	&amp;QUARTERROUND_SSSE3(3, 4, 9, 14, 7);
+	&amp;dec		(&quot;edx&quot;);
+	&amp;jnz		(&amp;label(&quot;loop&quot;));
+
+	&amp;movdqa		(&amp;QWP(16*4-128,&quot;ebx&quot;),$xb_);
+	&amp;movdqa		(&amp;QWP(16*8-128,&quot;ebx&quot;),$xc);
+	&amp;movdqa		(&amp;QWP(16*9-128,&quot;ebx&quot;),$xc_);
+	&amp;movdqa		(&amp;QWP(16*12-128,&quot;ebx&quot;),$xd);
+	&amp;movdqa		(&amp;QWP(16*14-128,&quot;ebx&quot;),$xd_);
+
+    my ($xa0,$xa1,$xa2,$xa3,$xt0,$xt1,$xt2,$xt3)=map(&quot;xmm$_&quot;,(0..7));
+
+	#&amp;movdqa	($xa0,&amp;QWP(16*0-128,&quot;ebx&quot;));	# it's there
+	&amp;movdqa		($xa1,&amp;QWP(16*1-128,&quot;ebx&quot;));
+	&amp;movdqa		($xa2,&amp;QWP(16*2-128,&quot;ebx&quot;));
+	&amp;movdqa		($xa3,&amp;QWP(16*3-128,&quot;ebx&quot;));
+
+    for($i=0;$i&lt;256;$i+=64) {
+	&amp;paddd		($xa0,&amp;QWP($i+16*0-128,&quot;ebp&quot;));	# accumulate key material
+	&amp;paddd		($xa1,&amp;QWP($i+16*1-128,&quot;ebp&quot;));
+	&amp;paddd		($xa2,&amp;QWP($i+16*2-128,&quot;ebp&quot;));
+	&amp;paddd		($xa3,&amp;QWP($i+16*3-128,&quot;ebp&quot;));
+
+	&amp;movdqa		($xt2,$xa0);		# &quot;de-interlace&quot; data
+	&amp;punpckldq	($xa0,$xa1);
+	&amp;movdqa		($xt3,$xa2);
+	&amp;punpckldq	($xa2,$xa3);
+	&amp;punpckhdq	($xt2,$xa1);
+	&amp;punpckhdq	($xt3,$xa3);
+	&amp;movdqa		($xa1,$xa0);
+	&amp;punpcklqdq	($xa0,$xa2);		# &quot;a0&quot;
+	&amp;movdqa		($xa3,$xt2);
+	&amp;punpcklqdq	($xt2,$xt3);		# &quot;a2&quot;
+	&amp;punpckhqdq	($xa1,$xa2);		# &quot;a1&quot;
+	&amp;punpckhqdq	($xa3,$xt3);		# &quot;a3&quot;
+
+	#($xa2,$xt2)=($xt2,$xa2);
+
+	&amp;movdqu		($xt0,&amp;QWP(64*0-128,$inp));	# load input
+	&amp;movdqu		($xt1,&amp;QWP(64*1-128,$inp));
+	&amp;movdqu		($xa2,&amp;QWP(64*2-128,$inp));
+	&amp;movdqu		($xt3,&amp;QWP(64*3-128,$inp));
+	&amp;lea		($inp,&amp;QWP($i&lt;192?16:(64*4-16*3),$inp));
+	&amp;pxor		($xt0,$xa0);
+	&amp;movdqa		($xa0,&amp;QWP($i+16*4-128,&quot;ebx&quot;))	if ($i&lt;192);
+	&amp;pxor		($xt1,$xa1);
+	&amp;movdqa		($xa1,&amp;QWP($i+16*5-128,&quot;ebx&quot;))	if ($i&lt;192);
+	&amp;pxor		($xt2,$xa2);
+	&amp;movdqa		($xa2,&amp;QWP($i+16*6-128,&quot;ebx&quot;))	if ($i&lt;192);
+	&amp;pxor		($xt3,$xa3);
+	&amp;movdqa		($xa3,&amp;QWP($i+16*7-128,&quot;ebx&quot;))	if ($i&lt;192);
+	&amp;movdqu		(&amp;QWP(64*0-128,$out),$xt0);	# store output
+	&amp;movdqu		(&amp;QWP(64*1-128,$out),$xt1);
+	&amp;movdqu		(&amp;QWP(64*2-128,$out),$xt2);
+	&amp;movdqu		(&amp;QWP(64*3-128,$out),$xt3);
+	&amp;lea		($out,&amp;QWP($i&lt;192?16:(64*4-16*3),$out));
+    }
+	&amp;sub		($len,64*4);
+	&amp;jnc		(&amp;label(&quot;outer_loop&quot;));
+
+	&amp;add		($len,64*4);
+	&amp;jz		(&amp;label(&quot;done&quot;));
+
+	&amp;mov		(&quot;ebx&quot;,&amp;DWP(512+8,&quot;esp&quot;));	# restore pointers
+	&amp;lea		($inp,&amp;DWP(-128,$inp));
+	&amp;mov		(&quot;edx&quot;,&amp;DWP(512+4,&quot;esp&quot;));
+	&amp;lea		($out,&amp;DWP(-128,$out));
+
+	&amp;movd		(&quot;xmm2&quot;,&amp;DWP(16*12-128,&quot;ebp&quot;));	# counter value
+	&amp;movdqu		(&quot;xmm3&quot;,&amp;QWP(0,&quot;ebx&quot;));
+	&amp;paddd		(&quot;xmm2&quot;,&amp;QWP(16*6,&quot;eax&quot;));	# +four
+	&amp;pand		(&quot;xmm3&quot;,&amp;QWP(16*7,&quot;eax&quot;));
+	&amp;por		(&quot;xmm3&quot;,&quot;xmm2&quot;);		# counter value
+{
+my ($a,$b,$c,$d,$t,$t1,$rot16,$rot24)=map(&quot;%xmm$_&quot;,(0..7));
+
+sub SSSE3ROUND {	# critical path is 20 &quot;SIMD ticks&quot; per round
+	&amp;paddd		($a,$b);
+	&amp;pxor		($d,$a);
+	&amp;pshufb		($d,$rot16);
+
+	&amp;paddd		($c,$d);
+	&amp;pxor		($b,$c);
+	&amp;movdqa		($t,$b);
+	&amp;psrld		($b,20);
+	&amp;pslld		($t,12);
+	&amp;por		($b,$t);
+
+	&amp;paddd		($a,$b);
+	&amp;pxor		($d,$a);
+	&amp;pshufb		($d,$rot24);
+
+	&amp;paddd		($c,$d);
+	&amp;pxor		($b,$c);
+	&amp;movdqa		($t,$b);
+	&amp;psrld		($b,25);
+	&amp;pslld		($t,7);
+	&amp;por		($b,$t);
+}
+
+&amp;set_label(&quot;1x&quot;);
+	&amp;movdqa		($a,&amp;QWP(16*2,&quot;eax&quot;));		# sigma
+	&amp;movdqu		($b,&amp;QWP(0,&quot;edx&quot;));
+	&amp;movdqu		($c,&amp;QWP(16,&quot;edx&quot;));
+	#&amp;movdqu	($d,&amp;QWP(0,&quot;ebx&quot;));		# already loaded
+	&amp;movdqa		($rot16,&amp;QWP(0,&quot;eax&quot;));
+	&amp;movdqa		($rot24,&amp;QWP(16,&quot;eax&quot;));
+	&amp;mov		(&amp;DWP(16*3,&quot;esp&quot;),&quot;ebp&quot;);
+
+	&amp;movdqa		(&amp;QWP(16*0,&quot;esp&quot;),$a);
+	&amp;movdqa		(&amp;QWP(16*1,&quot;esp&quot;),$b);
+	&amp;movdqa		(&amp;QWP(16*2,&quot;esp&quot;),$c);
+	&amp;movdqa		(&amp;QWP(16*3,&quot;esp&quot;),$d);
+	&amp;mov		(&quot;edx&quot;,10);
+	&amp;jmp		(&amp;label(&quot;loop1x&quot;));
+
+&amp;set_label(&quot;outer1x&quot;,16);
+	&amp;movdqa		($d,&amp;QWP(16*5,&quot;eax&quot;));		# one
+	&amp;movdqa		($a,&amp;QWP(16*0,&quot;esp&quot;));
+	&amp;movdqa		($b,&amp;QWP(16*1,&quot;esp&quot;));
+	&amp;movdqa		($c,&amp;QWP(16*2,&quot;esp&quot;));
+	&amp;paddd		($d,&amp;QWP(16*3,&quot;esp&quot;));
+	&amp;mov		(&quot;edx&quot;,10);
+	&amp;movdqa		(&amp;QWP(16*3,&quot;esp&quot;),$d);
+	&amp;jmp		(&amp;label(&quot;loop1x&quot;));
+
+&amp;set_label(&quot;loop1x&quot;,16);
+	&amp;SSSE3ROUND();
+	&amp;pshufd	($c,$c,0b01001110);
+	&amp;pshufd	($b,$b,0b00111001);
+	&amp;pshufd	($d,$d,0b10010011);
+	&amp;nop	();
+
+	&amp;SSSE3ROUND();
+	&amp;pshufd	($c,$c,0b01001110);
+	&amp;pshufd	($b,$b,0b10010011);
+	&amp;pshufd	($d,$d,0b00111001);
+
+	&amp;dec		(&quot;edx&quot;);
+	&amp;jnz		(&amp;label(&quot;loop1x&quot;));
+
+	&amp;paddd		($a,&amp;QWP(16*0,&quot;esp&quot;));
+	&amp;paddd		($b,&amp;QWP(16*1,&quot;esp&quot;));
+	&amp;paddd		($c,&amp;QWP(16*2,&quot;esp&quot;));
+	&amp;paddd		($d,&amp;QWP(16*3,&quot;esp&quot;));
+
+	&amp;cmp		($len,64);
+	&amp;jb		(&amp;label(&quot;tail&quot;));
+
+	&amp;movdqu		($t,&amp;QWP(16*0,$inp));
+	&amp;movdqu		($t1,&amp;QWP(16*1,$inp));
+	&amp;pxor		($a,$t);		# xor with input
+	&amp;movdqu		($t,&amp;QWP(16*2,$inp));
+	&amp;pxor		($b,$t1);
+	&amp;movdqu		($t1,&amp;QWP(16*3,$inp));
+	&amp;pxor		($c,$t);
+	&amp;pxor		($d,$t1);
+	&amp;lea		($inp,&amp;DWP(16*4,$inp));	# inp+=64
+
+	&amp;movdqu		(&amp;QWP(16*0,$out),$a);	# write output
+	&amp;movdqu		(&amp;QWP(16*1,$out),$b);
+	&amp;movdqu		(&amp;QWP(16*2,$out),$c);
+	&amp;movdqu		(&amp;QWP(16*3,$out),$d);
+	&amp;lea		($out,&amp;DWP(16*4,$out));	# inp+=64
+
+	&amp;sub		($len,64);
+	&amp;jnz		(&amp;label(&quot;outer1x&quot;));
+
+	&amp;jmp		(&amp;label(&quot;done&quot;));
+
+&amp;set_label(&quot;tail&quot;);
+	&amp;movdqa		(&amp;QWP(16*0,&quot;esp&quot;),$a);
+	&amp;movdqa		(&amp;QWP(16*1,&quot;esp&quot;),$b);
+	&amp;movdqa		(&amp;QWP(16*2,&quot;esp&quot;),$c);
+	&amp;movdqa		(&amp;QWP(16*3,&quot;esp&quot;),$d);
+
+	&amp;xor		(&quot;eax&quot;,&quot;eax&quot;);
+	&amp;xor		(&quot;edx&quot;,&quot;edx&quot;);
+	&amp;xor		(&quot;ebp&quot;,&quot;ebp&quot;);
+
+&amp;set_label(&quot;tail_loop&quot;);
+	&amp;movb		(&quot;al&quot;,&amp;BP(0,&quot;esp&quot;,&quot;ebp&quot;));
+	&amp;movb		(&quot;dl&quot;,&amp;BP(0,$inp,&quot;ebp&quot;));
+	&amp;lea		(&quot;ebp&quot;,&amp;DWP(1,&quot;ebp&quot;));
+	&amp;xor		(&quot;al&quot;,&quot;dl&quot;);
+	&amp;movb		(&amp;BP(-1,$out,&quot;ebp&quot;),&quot;al&quot;);
+	&amp;dec		($len);
+	&amp;jnz		(&amp;label(&quot;tail_loop&quot;));
+}
+&amp;set_label(&quot;done&quot;);
+	&amp;mov		(&quot;esp&quot;,&amp;DWP(512,&quot;esp&quot;));
+&amp;function_end(&quot;ChaCha20_ssse3&quot;);
+
+&amp;align	(64);
+&amp;set_label(&quot;ssse3_data&quot;);
+&amp;data_byte(0x2,0x3,0x0,0x1, 0x6,0x7,0x4,0x5, 0xa,0xb,0x8,0x9, 0xe,0xf,0xc,0xd);
+&amp;data_byte(0x3,0x0,0x1,0x2, 0x7,0x4,0x5,0x6, 0xb,0x8,0x9,0xa, 0xf,0xc,0xd,0xe);
+&amp;data_word(0x61707865,0x3320646e,0x79622d32,0x6b206574);
+&amp;data_word(0,1,2,3);
+&amp;data_word(4,4,4,4);
+&amp;data_word(1,0,0,0);
+&amp;data_word(4,0,0,0);
+&amp;data_word(0,-1,-1,-1);
+&amp;align	(64);
+}
+&amp;asciz	(&quot;ChaCha20 for x86, CRYPTOGAMS by &lt;appro\@openssl.org&gt;&quot;);
+
+if ($xmm) {
+my ($xa,$xa_,$xb,$xb_,$xc,$xc_,$xd,$xd_)=map(&quot;xmm$_&quot;,(0..7));
+my ($out,$inp,$len)=(&quot;edi&quot;,&quot;esi&quot;,&quot;ecx&quot;);
+
+sub QUARTERROUND_XOP {
+my ($ai,$bi,$ci,$di,$i)=@_;
+my ($an,$bn,$cn,$dn)=map(($_&amp;~3)+(($_+1)&amp;3),($ai,$bi,$ci,$di));	# next
+my ($ap,$bp,$cp,$dp)=map(($_&amp;~3)+(($_-1)&amp;3),($ai,$bi,$ci,$di));	# previous
+
+	#       a   b   c   d
+	#
+	#       0   4   8  12 &lt; even round
+	#       1   5   9  13
+	#       2   6  10  14
+	#       3   7  11  15
+	#       0   5  10  15 &lt; odd round
+	#       1   6  11  12
+	#       2   7   8  13
+	#       3   4   9  14
+
+	if ($i==0) {
+            my $j=4;
+	    ($ap,$bp,$cp,$dp)=map(($_&amp;~3)+(($_-$j--)&amp;3),($ap,$bp,$cp,$dp));
+	} elsif ($i==3) {
+            my $j=0;
+	    ($an,$bn,$cn,$dn)=map(($_&amp;~3)+(($_+$j++)&amp;3),($an,$bn,$cn,$dn));
+	} elsif ($i==4) {
+            my $j=4;
+	    ($ap,$bp,$cp,$dp)=map(($_&amp;~3)+(($_+$j--)&amp;3),($ap,$bp,$cp,$dp));
+	} elsif ($i==7) {
+            my $j=0;
+	    ($an,$bn,$cn,$dn)=map(($_&amp;~3)+(($_-$j++)&amp;3),($an,$bn,$cn,$dn));
+	}
+
+	#&amp;vpaddd	($xa,$xa,$xb);			# see elsewhere
+	#&amp;vpxor		($xd,$xd,$xa);			# see elsewhere
+	 &amp;vmovdqa	(&amp;QWP(16*$cp-128,&quot;ebx&quot;),$xc_)	if ($ai&gt;0 &amp;&amp; $ai&lt;3);
+	&amp;vprotd		($xd,$xd,16);
+	 &amp;vmovdqa	(&amp;QWP(16*$bp-128,&quot;ebx&quot;),$xb_)	if ($i!=0);
+	&amp;vpaddd		($xc,$xc,$xd);
+	 &amp;vmovdqa	($xc_,&amp;QWP(16*$cn-128,&quot;ebx&quot;))	if ($ai&gt;0 &amp;&amp; $ai&lt;3);
+	&amp;vpxor		($xb,$i!=0?$xb:$xb_,$xc);
+	 &amp;vmovdqa	($xa_,&amp;QWP(16*$an-128,&quot;ebx&quot;));
+	&amp;vprotd		($xb,$xb,12);
+	 &amp;vmovdqa	($xb_,&amp;QWP(16*$bn-128,&quot;ebx&quot;))	if ($i&lt;7);
+	&amp;vpaddd		($xa,$xa,$xb);
+	 &amp;vmovdqa	($xd_,&amp;QWP(16*$dn-128,&quot;ebx&quot;))	if ($di!=$dn);
+	&amp;vpxor		($xd,$xd,$xa);
+	 &amp;vpaddd	($xa_,$xa_,$xb_)		if ($i&lt;7);	# elsewhere
+	&amp;vprotd		($xd,$xd,8);
+	&amp;vmovdqa	(&amp;QWP(16*$ai-128,&quot;ebx&quot;),$xa);
+	&amp;vpaddd		($xc,$xc,$xd);
+	&amp;vmovdqa	(&amp;QWP(16*$di-128,&quot;ebx&quot;),$xd)	if ($di!=$dn);
+	&amp;vpxor		($xb,$xb,$xc);
+	 &amp;vpxor		($xd_,$di==$dn?$xd:$xd_,$xa_)	if ($i&lt;7);	# elsewhere
+	&amp;vprotd		($xb,$xb,7);
+
+	($xa,$xa_)=($xa_,$xa);
+	($xb,$xb_)=($xb_,$xb);
+	($xc,$xc_)=($xc_,$xc);
+	($xd,$xd_)=($xd_,$xd);
+}
+
+&amp;function_begin(&quot;ChaCha20_xop&quot;);
+&amp;set_label(&quot;xop_shortcut&quot;);
+	&amp;mov		($out,&amp;wparam(0));
+	&amp;mov		($inp,&amp;wparam(1));
+	&amp;mov		($len,&amp;wparam(2));
+	&amp;mov		(&quot;edx&quot;,&amp;wparam(3));		# key
+	&amp;mov		(&quot;ebx&quot;,&amp;wparam(4));		# counter and nonce
+	&amp;vzeroupper	();
+
+	&amp;mov		(&quot;ebp&quot;,&quot;esp&quot;);
+	&amp;stack_push	(131);
+	&amp;and		(&quot;esp&quot;,-64);
+	&amp;mov		(&amp;DWP(512,&quot;esp&quot;),&quot;ebp&quot;);
+
+	&amp;lea		(&quot;eax&quot;,&amp;DWP(&amp;label(&quot;ssse3_data&quot;).&quot;-&quot;.
+				    &amp;label(&quot;pic_point&quot;),&quot;eax&quot;));
+	&amp;vmovdqu	(&quot;xmm3&quot;,&amp;QWP(0,&quot;ebx&quot;));		# counter and nonce
+
+	&amp;cmp		($len,64*4);
+	&amp;jb		(&amp;label(&quot;1x&quot;));
+
+	&amp;mov		(&amp;DWP(512+4,&quot;esp&quot;),&quot;edx&quot;);	# offload pointers
+	&amp;mov		(&amp;DWP(512+8,&quot;esp&quot;),&quot;ebx&quot;);
+	&amp;sub		($len,64*4);			# bias len
+	&amp;lea		(&quot;ebp&quot;,&amp;DWP(256+128,&quot;esp&quot;));	# size optimization
+
+	&amp;vmovdqu	(&quot;xmm7&quot;,&amp;DWP(0,&quot;edx&quot;));		# key
+	&amp;vpshufd	(&quot;xmm0&quot;,&quot;xmm3&quot;,0x00);
+	&amp;vpshufd	(&quot;xmm1&quot;,&quot;xmm3&quot;,0x55);
+	&amp;vpshufd	(&quot;xmm2&quot;,&quot;xmm3&quot;,0xaa);
+	&amp;vpshufd	(&quot;xmm3&quot;,&quot;xmm3&quot;,0xff);
+	 &amp;vpaddd	(&quot;xmm0&quot;,&quot;xmm0&quot;,&amp;QWP(16*3,&quot;eax&quot;));	# fix counters
+	&amp;vpshufd	(&quot;xmm4&quot;,&quot;xmm7&quot;,0x00);
+	&amp;vpshufd	(&quot;xmm5&quot;,&quot;xmm7&quot;,0x55);
+	 &amp;vpsubd	(&quot;xmm0&quot;,&quot;xmm0&quot;,&amp;QWP(16*4,&quot;eax&quot;));
+	&amp;vpshufd	(&quot;xmm6&quot;,&quot;xmm7&quot;,0xaa);
+	&amp;vpshufd	(&quot;xmm7&quot;,&quot;xmm7&quot;,0xff);
+	&amp;vmovdqa	(&amp;QWP(16*12-128,&quot;ebp&quot;),&quot;xmm0&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*13-128,&quot;ebp&quot;),&quot;xmm1&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*14-128,&quot;ebp&quot;),&quot;xmm2&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*15-128,&quot;ebp&quot;),&quot;xmm3&quot;);
+	 &amp;vmovdqu	(&quot;xmm3&quot;,&amp;DWP(16,&quot;edx&quot;));	# key
+	&amp;vmovdqa	(&amp;QWP(16*4-128,&quot;ebp&quot;),&quot;xmm4&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*5-128,&quot;ebp&quot;),&quot;xmm5&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*6-128,&quot;ebp&quot;),&quot;xmm6&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*7-128,&quot;ebp&quot;),&quot;xmm7&quot;);
+	 &amp;vmovdqa	(&quot;xmm7&quot;,&amp;DWP(16*2,&quot;eax&quot;));	# sigma
+	 &amp;lea		(&quot;ebx&quot;,&amp;DWP(128,&quot;esp&quot;));	# size optimization
+
+	&amp;vpshufd	(&quot;xmm0&quot;,&quot;xmm3&quot;,0x00);
+	&amp;vpshufd	(&quot;xmm1&quot;,&quot;xmm3&quot;,0x55);
+	&amp;vpshufd	(&quot;xmm2&quot;,&quot;xmm3&quot;,0xaa);
+	&amp;vpshufd	(&quot;xmm3&quot;,&quot;xmm3&quot;,0xff);
+	&amp;vpshufd	(&quot;xmm4&quot;,&quot;xmm7&quot;,0x00);
+	&amp;vpshufd	(&quot;xmm5&quot;,&quot;xmm7&quot;,0x55);
+	&amp;vpshufd	(&quot;xmm6&quot;,&quot;xmm7&quot;,0xaa);
+	&amp;vpshufd	(&quot;xmm7&quot;,&quot;xmm7&quot;,0xff);
+	&amp;vmovdqa	(&amp;QWP(16*8-128,&quot;ebp&quot;),&quot;xmm0&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*9-128,&quot;ebp&quot;),&quot;xmm1&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*10-128,&quot;ebp&quot;),&quot;xmm2&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*11-128,&quot;ebp&quot;),&quot;xmm3&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*0-128,&quot;ebp&quot;),&quot;xmm4&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*1-128,&quot;ebp&quot;),&quot;xmm5&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*2-128,&quot;ebp&quot;),&quot;xmm6&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*3-128,&quot;ebp&quot;),&quot;xmm7&quot;);
+
+	&amp;lea		($inp,&amp;DWP(128,$inp));		# size optimization
+	&amp;lea		($out,&amp;DWP(128,$out));		# size optimization
+	&amp;jmp		(&amp;label(&quot;outer_loop&quot;));
+
+&amp;set_label(&quot;outer_loop&quot;,32);
+	#&amp;vmovdqa	(&quot;xmm0&quot;,&amp;QWP(16*0-128,&quot;ebp&quot;));	# copy key material
+	&amp;vmovdqa	(&quot;xmm1&quot;,&amp;QWP(16*1-128,&quot;ebp&quot;));
+	&amp;vmovdqa	(&quot;xmm2&quot;,&amp;QWP(16*2-128,&quot;ebp&quot;));
+	&amp;vmovdqa	(&quot;xmm3&quot;,&amp;QWP(16*3-128,&quot;ebp&quot;));
+	#&amp;vmovdqa	(&quot;xmm4&quot;,&amp;QWP(16*4-128,&quot;ebp&quot;));
+	&amp;vmovdqa	(&quot;xmm5&quot;,&amp;QWP(16*5-128,&quot;ebp&quot;));
+	&amp;vmovdqa	(&quot;xmm6&quot;,&amp;QWP(16*6-128,&quot;ebp&quot;));
+	&amp;vmovdqa	(&quot;xmm7&quot;,&amp;QWP(16*7-128,&quot;ebp&quot;));
+	#&amp;vmovdqa	(&amp;QWP(16*0-128,&quot;ebx&quot;),&quot;xmm0&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*1-128,&quot;ebx&quot;),&quot;xmm1&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*2-128,&quot;ebx&quot;),&quot;xmm2&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*3-128,&quot;ebx&quot;),&quot;xmm3&quot;);
+	#&amp;vmovdqa	(&amp;QWP(16*4-128,&quot;ebx&quot;),&quot;xmm4&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*5-128,&quot;ebx&quot;),&quot;xmm5&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*6-128,&quot;ebx&quot;),&quot;xmm6&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*7-128,&quot;ebx&quot;),&quot;xmm7&quot;);
+	#&amp;vmovdqa	(&quot;xmm0&quot;,&amp;QWP(16*8-128,&quot;ebp&quot;));
+	#&amp;vmovdqa	(&quot;xmm1&quot;,&amp;QWP(16*9-128,&quot;ebp&quot;));
+	&amp;vmovdqa	(&quot;xmm2&quot;,&amp;QWP(16*10-128,&quot;ebp&quot;));
+	&amp;vmovdqa	(&quot;xmm3&quot;,&amp;QWP(16*11-128,&quot;ebp&quot;));
+	&amp;vmovdqa	(&quot;xmm4&quot;,&amp;QWP(16*12-128,&quot;ebp&quot;));
+	&amp;vmovdqa	(&quot;xmm5&quot;,&amp;QWP(16*13-128,&quot;ebp&quot;));
+	&amp;vmovdqa	(&quot;xmm6&quot;,&amp;QWP(16*14-128,&quot;ebp&quot;));
+	&amp;vmovdqa	(&quot;xmm7&quot;,&amp;QWP(16*15-128,&quot;ebp&quot;));
+	&amp;vpaddd		(&quot;xmm4&quot;,&quot;xmm4&quot;,&amp;QWP(16*4,&quot;eax&quot;));	# counter value
+	#&amp;vmovdqa	(&amp;QWP(16*8-128,&quot;ebx&quot;),&quot;xmm0&quot;);
+	#&amp;vmovdqa	(&amp;QWP(16*9-128,&quot;ebx&quot;),&quot;xmm1&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*10-128,&quot;ebx&quot;),&quot;xmm2&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*11-128,&quot;ebx&quot;),&quot;xmm3&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*12-128,&quot;ebx&quot;),&quot;xmm4&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*13-128,&quot;ebx&quot;),&quot;xmm5&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*14-128,&quot;ebx&quot;),&quot;xmm6&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*15-128,&quot;ebx&quot;),&quot;xmm7&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*12-128,&quot;ebp&quot;),&quot;xmm4&quot;);	# save counter value
+
+	&amp;vmovdqa	($xa, &amp;QWP(16*0-128,&quot;ebp&quot;));
+	&amp;vmovdqa	($xd, &quot;xmm4&quot;);
+	&amp;vmovdqa	($xb_,&amp;QWP(16*4-128,&quot;ebp&quot;));
+	&amp;vmovdqa	($xc, &amp;QWP(16*8-128,&quot;ebp&quot;));
+	&amp;vmovdqa	($xc_,&amp;QWP(16*9-128,&quot;ebp&quot;));
+
+	&amp;mov		(&quot;edx&quot;,10);			# loop counter
+	&amp;nop		();
+
+&amp;set_label(&quot;loop&quot;,32);
+	&amp;vpaddd		($xa,$xa,$xb_);			# elsewhere
+	&amp;vpxor		($xd,$xd,$xa);			# elsewhere
+	&amp;QUARTERROUND_XOP(0, 4, 8, 12, 0);
+	&amp;QUARTERROUND_XOP(1, 5, 9, 13, 1);
+	&amp;QUARTERROUND_XOP(2, 6,10, 14, 2);
+	&amp;QUARTERROUND_XOP(3, 7,11, 15, 3);
+	&amp;QUARTERROUND_XOP(0, 5,10, 15, 4);
+	&amp;QUARTERROUND_XOP(1, 6,11, 12, 5);
+	&amp;QUARTERROUND_XOP(2, 7, 8, 13, 6);
+	&amp;QUARTERROUND_XOP(3, 4, 9, 14, 7);
+	&amp;dec		(&quot;edx&quot;);
+	&amp;jnz		(&amp;label(&quot;loop&quot;));
+
+	&amp;vmovdqa	(&amp;QWP(16*4-128,&quot;ebx&quot;),$xb_);
+	&amp;vmovdqa	(&amp;QWP(16*8-128,&quot;ebx&quot;),$xc);
+	&amp;vmovdqa	(&amp;QWP(16*9-128,&quot;ebx&quot;),$xc_);
+	&amp;vmovdqa	(&amp;QWP(16*12-128,&quot;ebx&quot;),$xd);
+	&amp;vmovdqa	(&amp;QWP(16*14-128,&quot;ebx&quot;),$xd_);
+
+    my ($xa0,$xa1,$xa2,$xa3,$xt0,$xt1,$xt2,$xt3)=map(&quot;xmm$_&quot;,(0..7));
+
+	#&amp;vmovdqa	($xa0,&amp;QWP(16*0-128,&quot;ebx&quot;));	# it's there
+	&amp;vmovdqa	($xa1,&amp;QWP(16*1-128,&quot;ebx&quot;));
+	&amp;vmovdqa	($xa2,&amp;QWP(16*2-128,&quot;ebx&quot;));
+	&amp;vmovdqa	($xa3,&amp;QWP(16*3-128,&quot;ebx&quot;));
+
+    for($i=0;$i&lt;256;$i+=64) {
+	&amp;vpaddd		($xa0,$xa0,&amp;QWP($i+16*0-128,&quot;ebp&quot;));	# accumulate key material
+	&amp;vpaddd		($xa1,$xa1,&amp;QWP($i+16*1-128,&quot;ebp&quot;));
+	&amp;vpaddd		($xa2,$xa2,&amp;QWP($i+16*2-128,&quot;ebp&quot;));
+	&amp;vpaddd		($xa3,$xa3,&amp;QWP($i+16*3-128,&quot;ebp&quot;));
+
+	&amp;vpunpckldq	($xt2,$xa0,$xa1);	# &quot;de-interlace&quot; data
+	&amp;vpunpckldq	($xt3,$xa2,$xa3);
+	&amp;vpunpckhdq	($xa0,$xa0,$xa1);
+	&amp;vpunpckhdq	($xa2,$xa2,$xa3);
+	&amp;vpunpcklqdq	($xa1,$xt2,$xt3);	# &quot;a0&quot;
+	&amp;vpunpckhqdq	($xt2,$xt2,$xt3);	# &quot;a1&quot;
+	&amp;vpunpcklqdq	($xt3,$xa0,$xa2);	# &quot;a2&quot;
+	&amp;vpunpckhqdq	($xa3,$xa0,$xa2);	# &quot;a3&quot;
+
+	&amp;vpxor		($xt0,$xa1,&amp;QWP(64*0-128,$inp));
+	&amp;vpxor		($xt1,$xt2,&amp;QWP(64*1-128,$inp));
+	&amp;vpxor		($xt2,$xt3,&amp;QWP(64*2-128,$inp));
+	&amp;vpxor		($xt3,$xa3,&amp;QWP(64*3-128,$inp));
+	&amp;lea		($inp,&amp;QWP($i&lt;192?16:(64*4-16*3),$inp));
+	&amp;vmovdqa	($xa0,&amp;QWP($i+16*4-128,&quot;ebx&quot;))	if ($i&lt;192);
+	&amp;vmovdqa	($xa1,&amp;QWP($i+16*5-128,&quot;ebx&quot;))	if ($i&lt;192);
+	&amp;vmovdqa	($xa2,&amp;QWP($i+16*6-128,&quot;ebx&quot;))	if ($i&lt;192);
+	&amp;vmovdqa	($xa3,&amp;QWP($i+16*7-128,&quot;ebx&quot;))	if ($i&lt;192);
+	&amp;vmovdqu	(&amp;QWP(64*0-128,$out),$xt0);	# store output
+	&amp;vmovdqu	(&amp;QWP(64*1-128,$out),$xt1);
+	&amp;vmovdqu	(&amp;QWP(64*2-128,$out),$xt2);
+	&amp;vmovdqu	(&amp;QWP(64*3-128,$out),$xt3);
+	&amp;lea		($out,&amp;QWP($i&lt;192?16:(64*4-16*3),$out));
+    }
+	&amp;sub		($len,64*4);
+	&amp;jnc		(&amp;label(&quot;outer_loop&quot;));
+
+	&amp;add		($len,64*4);
+	&amp;jz		(&amp;label(&quot;done&quot;));
+
+	&amp;mov		(&quot;ebx&quot;,&amp;DWP(512+8,&quot;esp&quot;));	# restore pointers
+	&amp;lea		($inp,&amp;DWP(-128,$inp));
+	&amp;mov		(&quot;edx&quot;,&amp;DWP(512+4,&quot;esp&quot;));
+	&amp;lea		($out,&amp;DWP(-128,$out));
+
+	&amp;vmovd		(&quot;xmm2&quot;,&amp;DWP(16*12-128,&quot;ebp&quot;));	# counter value
+	&amp;vmovdqu	(&quot;xmm3&quot;,&amp;QWP(0,&quot;ebx&quot;));
+	&amp;vpaddd		(&quot;xmm2&quot;,&quot;xmm2&quot;,&amp;QWP(16*6,&quot;eax&quot;));# +four
+	&amp;vpand		(&quot;xmm3&quot;,&quot;xmm3&quot;,&amp;QWP(16*7,&quot;eax&quot;));
+	&amp;vpor		(&quot;xmm3&quot;,&quot;xmm3&quot;,&quot;xmm2&quot;);		# counter value
+{
+my ($a,$b,$c,$d,$t,$t1,$rot16,$rot24)=map(&quot;%xmm$_&quot;,(0..7));
+
+sub XOPROUND {
+	&amp;vpaddd		($a,$a,$b);
+	&amp;vpxor		($d,$d,$a);
+	&amp;vprotd		($d,$d,16);
+
+	&amp;vpaddd		($c,$c,$d);
+	&amp;vpxor		($b,$b,$c);
+	&amp;vprotd		($b,$b,12);
+
+	&amp;vpaddd		($a,$a,$b);
+	&amp;vpxor		($d,$d,$a);
+	&amp;vprotd		($d,$d,8);
+
+	&amp;vpaddd		($c,$c,$d);
+	&amp;vpxor		($b,$b,$c);
+	&amp;vprotd		($b,$b,7);
+}
+
+&amp;set_label(&quot;1x&quot;);
+	&amp;vmovdqa	($a,&amp;QWP(16*2,&quot;eax&quot;));		# sigma
+	&amp;vmovdqu	($b,&amp;QWP(0,&quot;edx&quot;));
+	&amp;vmovdqu	($c,&amp;QWP(16,&quot;edx&quot;));
+	#&amp;vmovdqu	($d,&amp;QWP(0,&quot;ebx&quot;));		# already loaded
+	&amp;vmovdqa	($rot16,&amp;QWP(0,&quot;eax&quot;));
+	&amp;vmovdqa	($rot24,&amp;QWP(16,&quot;eax&quot;));
+	&amp;mov		(&amp;DWP(16*3,&quot;esp&quot;),&quot;ebp&quot;);
+
+	&amp;vmovdqa	(&amp;QWP(16*0,&quot;esp&quot;),$a);
+	&amp;vmovdqa	(&amp;QWP(16*1,&quot;esp&quot;),$b);
+	&amp;vmovdqa	(&amp;QWP(16*2,&quot;esp&quot;),$c);
+	&amp;vmovdqa	(&amp;QWP(16*3,&quot;esp&quot;),$d);
+	&amp;mov		(&quot;edx&quot;,10);
+	&amp;jmp		(&amp;label(&quot;loop1x&quot;));
+
+&amp;set_label(&quot;outer1x&quot;,16);
+	&amp;vmovdqa	($d,&amp;QWP(16*5,&quot;eax&quot;));		# one
+	&amp;vmovdqa	($a,&amp;QWP(16*0,&quot;esp&quot;));
+	&amp;vmovdqa	($b,&amp;QWP(16*1,&quot;esp&quot;));
+	&amp;vmovdqa	($c,&amp;QWP(16*2,&quot;esp&quot;));
+	&amp;vpaddd		($d,$d,&amp;QWP(16*3,&quot;esp&quot;));
+	&amp;mov		(&quot;edx&quot;,10);
+	&amp;vmovdqa	(&amp;QWP(16*3,&quot;esp&quot;),$d);
+	&amp;jmp		(&amp;label(&quot;loop1x&quot;));
+
+&amp;set_label(&quot;loop1x&quot;,16);
+	&amp;XOPROUND();
+	&amp;vpshufd	($c,$c,0b01001110);
+	&amp;vpshufd	($b,$b,0b00111001);
+	&amp;vpshufd	($d,$d,0b10010011);
+
+	&amp;XOPROUND();
+	&amp;vpshufd	($c,$c,0b01001110);
+	&amp;vpshufd	($b,$b,0b10010011);
+	&amp;vpshufd	($d,$d,0b00111001);
+
+	&amp;dec		(&quot;edx&quot;);
+	&amp;jnz		(&amp;label(&quot;loop1x&quot;));
+
+	&amp;vpaddd		($a,$a,&amp;QWP(16*0,&quot;esp&quot;));
+	&amp;vpaddd		($b,$b,&amp;QWP(16*1,&quot;esp&quot;));
+	&amp;vpaddd		($c,$c,&amp;QWP(16*2,&quot;esp&quot;));
+	&amp;vpaddd		($d,$d,&amp;QWP(16*3,&quot;esp&quot;));
+
+	&amp;cmp		($len,64);
+	&amp;jb		(&amp;label(&quot;tail&quot;));
+
+	&amp;vpxor		($a,$a,&amp;QWP(16*0,$inp));	# xor with input
+	&amp;vpxor		($b,$b,&amp;QWP(16*1,$inp));
+	&amp;vpxor		($c,$c,&amp;QWP(16*2,$inp));
+	&amp;vpxor		($d,$d,&amp;QWP(16*3,$inp));
+	&amp;lea		($inp,&amp;DWP(16*4,$inp));		# inp+=64
+
+	&amp;vmovdqu	(&amp;QWP(16*0,$out),$a);		# write output
+	&amp;vmovdqu	(&amp;QWP(16*1,$out),$b);
+	&amp;vmovdqu	(&amp;QWP(16*2,$out),$c);
+	&amp;vmovdqu	(&amp;QWP(16*3,$out),$d);
+	&amp;lea		($out,&amp;DWP(16*4,$out));		# inp+=64
+
+	&amp;sub		($len,64);
+	&amp;jnz		(&amp;label(&quot;outer1x&quot;));
+
+	&amp;jmp		(&amp;label(&quot;done&quot;));
+
+&amp;set_label(&quot;tail&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*0,&quot;esp&quot;),$a);
+	&amp;vmovdqa	(&amp;QWP(16*1,&quot;esp&quot;),$b);
+	&amp;vmovdqa	(&amp;QWP(16*2,&quot;esp&quot;),$c);
+	&amp;vmovdqa	(&amp;QWP(16*3,&quot;esp&quot;),$d);
+
+	&amp;xor		(&quot;eax&quot;,&quot;eax&quot;);
+	&amp;xor		(&quot;edx&quot;,&quot;edx&quot;);
+	&amp;xor		(&quot;ebp&quot;,&quot;ebp&quot;);
+
+&amp;set_label(&quot;tail_loop&quot;);
+	&amp;movb		(&quot;al&quot;,&amp;BP(0,&quot;esp&quot;,&quot;ebp&quot;));
+	&amp;movb		(&quot;dl&quot;,&amp;BP(0,$inp,&quot;ebp&quot;));
+	&amp;lea		(&quot;ebp&quot;,&amp;DWP(1,&quot;ebp&quot;));
+	&amp;xor		(&quot;al&quot;,&quot;dl&quot;);
+	&amp;movb		(&amp;BP(-1,$out,&quot;ebp&quot;),&quot;al&quot;);
+	&amp;dec		($len);
+	&amp;jnz		(&amp;label(&quot;tail_loop&quot;));
+}
+&amp;set_label(&quot;done&quot;);
+	&amp;vzeroupper	();
+	&amp;mov		(&quot;esp&quot;,&amp;DWP(512,&quot;esp&quot;));
+&amp;function_end(&quot;ChaCha20_xop&quot;);
+}
+
+&amp;asm_finish();
diff --git a/crypto/chacha/asm/chacha-x86_64.pl b/crypto/chacha/asm/chacha-x86_64.pl
new file mode 100755
index 0000000..41dbef5
--- /dev/null
+++ b/crypto/chacha/asm/chacha-x86_64.pl
@@ -0,0 +1,2234 @@
+#!/usr/bin/env perl
+#
+# ====================================================================
+# Written by Andy Polyakov &lt;<A HREF="../../../mailman/listinfo/openssl-commits.html">appro at openssl.org</A>&gt; for the OpenSSL
+# project. The module is, however, dual licensed under OpenSSL and
+# CRYPTOGAMS licenses depending on where you obtain it. For further
+# details see <A HREF="http://www.openssl.org/~appro/cryptogams/.">http://www.openssl.org/~appro/cryptogams/.</A>
+# ====================================================================
+#
+# November 2014
+#
+# ChaCha20 for x86_64.
+#
+# Performance in cycles per byte out of large buffer.
+#
+#		IALU/gcc 4.8(i)	1xSSSE3/SSE2	4xSSSE3	    8xAVX2
+#
+# P4		9.48/+99%	-/22.7(ii)	-
+# Core2		7.83/+55%	7.90/8.08	4.35
+# Westmere	7.19/+50%	5.60/6.70	3.00
+# Sandy Bridge	8.31/+42%	5.45/6.76	2.72
+# Ivy Bridge	6.71/+46%	5.40/6.49	2.41
+# Haswell	5.92/+43%	5.20/6.45	2.42	    1.23
+# Silvermont	12.0/+33%	7.75/7.40	7.03(iii)
+# Sledgehammer	7.28/+52%	-/14.2(ii)	-
+# Bulldozer	9.66/+28%	9.85/11.1	3.06(iv)
+# VIA Nano	10.5/+46%	6.72/8.60	6.05
+#
+# (i)	compared to older gcc 3.x one can observe &gt;2x improvement on
+#	most platforms;
+# (ii)	as it can be seen, SSE2 performance is too low on legacy
+#	processors; NxSSE2 results are naturally better, but not
+#	impressively better than IALU ones, which is why you won't
+#	find SSE2 code below;
+# (iii)	this is not optimal result for Atom because of MSROM
+#	limitations, SSE2 can do better, but gain is considered too
+#	low to justify the [maintenance] effort;
+# (iv)	Bulldozer actually executes 4xXOP code path that delivers 2.20;
+
+$flavour = shift;
+$output  = shift;
+if ($flavour =~ /\./) { $output = $flavour; undef $flavour; }
+
+$win64=0; $win64=1 if ($flavour =~ /[nm]asm|mingw64/ || $output =~ /\.asm$/);
+
+$0 =~ m/(.*[\/\\])[^\/\\]+$/; $dir=$1;
+( $xlate=&quot;${dir}x86_64-xlate.pl&quot; and -f $xlate ) or
+( $xlate=&quot;${dir}../../perlasm/x86_64-xlate.pl&quot; and -f $xlate) or
+die &quot;can't locate x86_64-xlate.pl&quot;;
+
+if (`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2&gt;&amp;1`
+		=~ /GNU assembler version ([2-9]\.[0-9]+)/) {
+	$avx = ($1&gt;=2.19) + ($1&gt;=2.22);
+}
+
+if (!$avx &amp;&amp; $win64 &amp;&amp; ($flavour =~ /nasm/ || $ENV{ASM} =~ /nasm/) &amp;&amp;
+	   `nasm -v 2&gt;&amp;1` =~ /NASM version ([2-9]\.[0-9]+)/) {
+	$avx = ($1&gt;=2.09) + ($1&gt;=2.10);
+}
+
+if (!$avx &amp;&amp; $win64 &amp;&amp; ($flavour =~ /masm/ || $ENV{ASM} =~ /ml64/) &amp;&amp;
+	   `ml64 2&gt;&amp;1` =~ /Version ([0-9]+)\./) {
+	$avx = ($1&gt;=10) + ($1&gt;=11);
+}
+
+if (!$avx &amp;&amp; `$ENV{CC} -v 2&gt;&amp;1` =~ /((?:^clang|LLVM) version|.*based on LLVM) ([3-9]\.[0-9]+)/) {
+	$avx = ($2&gt;=3.0) + ($2&gt;3.0);
+}
+
+open OUT,&quot;| \&quot;$^X\&quot; $xlate $flavour $output&quot;;
+*STDOUT=*OUT;
+
+# input parameter block
+($out,$inp,$len,$key,$counter)=(&quot;%rdi&quot;,&quot;%rsi&quot;,&quot;%rdx&quot;,&quot;%rcx&quot;,&quot;%r8&quot;);
+
+$code.=&lt;&lt;___;
+.text
+
+.extern OPENSSL_ia32cap_P
+
+.align	64
+.Lzero:
+.long	0,0,0,0
+.Lone:
+.long	1,0,0,0
+.Linc:
+.long	0,1,2,3
+.Lfour:
+.long	4,4,4,4
+.Lincy:
+.long	0,2,4,6,1,3,5,7
+.Leight:
+.long	8,8,8,8,8,8,8,8
+.Lrot16:
+.byte	0x2,0x3,0x0,0x1, 0x6,0x7,0x4,0x5, 0xa,0xb,0x8,0x9, 0xe,0xf,0xc,0xd
+.Lrot24:
+.byte	0x3,0x0,0x1,0x2, 0x7,0x4,0x5,0x6, 0xb,0x8,0x9,0xa, 0xf,0xc,0xd,0xe
+.Lsigma:
+.asciz	&quot;expand 32-byte k&quot;
+.asciz	&quot;ChaCha20 for x86_64, CRYPTOGAMS by &lt;appro\@openssl.org&gt;&quot;
+___
+
+sub AUTOLOAD()          # thunk [simplified] 32-bit style perlasm
+{ my $opcode = $AUTOLOAD; $opcode =~ s/.*:://;
+  my $arg = pop;
+    $arg = &quot;\$$arg&quot; if ($arg*1 eq $arg);
+    $code .= &quot;\t$opcode\t&quot;.join(',',$arg,reverse @_).&quot;\n&quot;;
+}
+
<A HREF="../../../mailman/listinfo/openssl-commits.html">+ at x</A>=(&quot;%eax&quot;,&quot;%ebx&quot;,&quot;%ecx&quot;,&quot;%edx&quot;,map(&quot;%r${_}d&quot;,(8..11)),
+    &quot;%nox&quot;,&quot;%nox&quot;,&quot;%nox&quot;,&quot;%nox&quot;,map(&quot;%r${_}d&quot;,(12..15)));
<A HREF="../../../mailman/listinfo/openssl-commits.html">+ at t</A>=(&quot;%esi&quot;,&quot;%edi&quot;);
+
+sub ROUND {			# critical path is 24 cycles per round
+my ($a0,$b0,$c0,$d0)=@_;
+my ($a1,$b1,$c1,$d1)=map(($_&amp;~3)+(($_+1)&amp;3),($a0,$b0,$c0,$d0));
+my ($a2,$b2,$c2,$d2)=map(($_&amp;~3)+(($_+1)&amp;3),($a1,$b1,$c1,$d1));
+my ($a3,$b3,$c3,$d3)=map(($_&amp;~3)+(($_+1)&amp;3),($a2,$b2,$c2,$d2));
+my ($xc,$xc_)=map(&quot;\&quot;$_\&quot;&quot;<A HREF="../../../mailman/listinfo/openssl-commits.html">, at t</A>);
+my @x=map(&quot;\&quot;$_\&quot;&quot;<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>);
+
+	# Consider order in which variables are addressed by their
+	# index:
+	#
+	#	a   b   c   d
+	#
+	#	0   4   8  12 &lt; even round
+	#	1   5   9  13
+	#	2   6  10  14
+	#	3   7  11  15
+	#	0   5  10  15 &lt; odd round
+	#	1   6  11  12
+	#	2   7   8  13
+	#	3   4   9  14
+	#
+	# 'a', 'b' and 'd's are permanently allocated in registers,
+	# @x[0..7,12..15], while 'c's are maintained in memory. If
+	# you observe 'c' column, you'll notice that pair of 'c's is
+	# invariant between rounds. This means that we have to reload
+	# them once per round, in the middle. This is why you'll see
+	# bunch of 'c' stores and loads in the middle, but none in
+	# the beginning or end.
+
+	# Normally instructions would be interleaved to favour in-order
+	# execution. Generally out-of-order cores manage it gracefully,
+	# but not this time for some reason. As in-order execution
+	# cores are dying breed, old Atom is the only one around,
+	# instructions are left uninterleaved. Besides, Atom is better
+	# off executing 1xSSSE3 code anyway...
+
+	(
+	&quot;&amp;add	(@x[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b0])&quot;,	# Q1
+	&quot;&amp;xor	(@x[$d0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a0])&quot;,
+	&quot;&amp;rol	(@x[$d0],16)&quot;,
+	 &quot;&amp;add	(@x[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b1])&quot;,	# Q2
+	 &quot;&amp;xor	(@x[$d1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a1])&quot;,
+	 &quot;&amp;rol	(@x[$d1],16)&quot;,
+
+	&quot;&amp;add	($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$d0])&quot;,
+	&quot;&amp;xor	(@x[$b0],$xc)&quot;,
+	&quot;&amp;rol	(@x[$b0],12)&quot;,
+	 &quot;&amp;add	($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$d1])&quot;,
+	 &quot;&amp;xor	(@x[$b1],$xc_)&quot;,
+	 &quot;&amp;rol	(@x[$b1],12)&quot;,
+
+	&quot;&amp;add	(@x[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b0])&quot;,
+	&quot;&amp;xor	(@x[$d0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a0])&quot;,
+	&quot;&amp;rol	(@x[$d0],8)&quot;,
+	 &quot;&amp;add	(@x[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b1])&quot;,
+	 &quot;&amp;xor	(@x[$d1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a1])&quot;,
+	 &quot;&amp;rol	(@x[$d1],8)&quot;,
+
+	&quot;&amp;add	($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$d0])&quot;,
+	&quot;&amp;xor	(@x[$b0],$xc)&quot;,
+	&quot;&amp;rol	(@x[$b0],7)&quot;,
+	 &quot;&amp;add	($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$d1])&quot;,
+	 &quot;&amp;xor	(@x[$b1],$xc_)&quot;,
+	 &quot;&amp;rol	(@x[$b1],7)&quot;,
+
+	&quot;&amp;mov	(\&quot;4*$c0(%rsp)\&quot;,$xc)&quot;,	# reload pair of 'c's
+	 &quot;&amp;mov	(\&quot;4*$c1(%rsp)\&quot;,$xc_)&quot;,
+	&quot;&amp;mov	($xc,\&quot;4*$c2(%rsp)\&quot;)&quot;,
+	 &quot;&amp;mov	($xc_,\&quot;4*$c3(%rsp)\&quot;)&quot;,
+
+	&quot;&amp;add	(@x[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b2])&quot;,	# Q3
+	&quot;&amp;xor	(@x[$d2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a2])&quot;,
+	&quot;&amp;rol	(@x[$d2],16)&quot;,
+	 &quot;&amp;add	(@x[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b3])&quot;,	# Q4
+	 &quot;&amp;xor	(@x[$d3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a3])&quot;,
+	 &quot;&amp;rol	(@x[$d3],16)&quot;,
+
+	&quot;&amp;add	($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$d2])&quot;,
+	&quot;&amp;xor	(@x[$b2],$xc)&quot;,
+	&quot;&amp;rol	(@x[$b2],12)&quot;,
+	 &quot;&amp;add	($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$d3])&quot;,
+	 &quot;&amp;xor	(@x[$b3],$xc_)&quot;,
+	 &quot;&amp;rol	(@x[$b3],12)&quot;,
+
+	&quot;&amp;add	(@x[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b2])&quot;,
+	&quot;&amp;xor	(@x[$d2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a2])&quot;,
+	&quot;&amp;rol	(@x[$d2],8)&quot;,
+	 &quot;&amp;add	(@x[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b3])&quot;,
+	 &quot;&amp;xor	(@x[$d3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a3])&quot;,
+	 &quot;&amp;rol	(@x[$d3],8)&quot;,
+
+	&quot;&amp;add	($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$d2])&quot;,
+	&quot;&amp;xor	(@x[$b2],$xc)&quot;,
+	&quot;&amp;rol	(@x[$b2],7)&quot;,
+	 &quot;&amp;add	($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$d3])&quot;,
+	 &quot;&amp;xor	(@x[$b3],$xc_)&quot;,
+	 &quot;&amp;rol	(@x[$b3],7)&quot;
+	);
+}
+
+########################################################################
+# Generic code path that handles all lengths on pre-SSSE3 processors.
+$code.=&lt;&lt;___;
+.globl	ChaCha20_ctr32
+.type	ChaCha20_ctr32,\@function,5
+.align	64
+ChaCha20_ctr32:
+	mov	OPENSSL_ia32cap_P+4(%rip),%r10
+	test	\$`1&lt;&lt;(41-32)`,%r10d
+	jnz	.LChaCha20_ssse3
+
+	push	%rbx
+	push	%rbp
+	push	%r12
+	push	%r13
+	push	%r14
+	push	%r15
+	sub	\$64+24,%rsp
+
+	#movdqa	.Lsigma(%rip),%xmm0
+	movdqu	($key),%xmm1
+	movdqu	16($key),%xmm2
+	movdqu	($counter),%xmm3
+	movdqa	.Lone(%rip),%xmm4
+
+	#movdqa	%xmm0,4*0(%rsp)		# key[0]
+	movdqa	%xmm1,4*4(%rsp)		# key[1]
+	movdqa	%xmm2,4*8(%rsp)		# key[2]
+	movdqa	%xmm3,4*12(%rsp)	# key[3]
+	mov	$len,%rbp		# reassign $len
+	jmp	.Loop_outer
+
+.align	32
+.Loop_outer:
+	mov	\$<A HREF="../../../mailman/listinfo/openssl-commits.html">0x61707865, at x</A>[0]      # 'expa'
+	mov	\$<A HREF="../../../mailman/listinfo/openssl-commits.html">0x3320646e, at x</A>[1]      # 'nd 3'
+	mov	\$<A HREF="../../../mailman/listinfo/openssl-commits.html">0x79622d32, at x</A>[2]      # '2-by'
+	mov	\$<A HREF="../../../mailman/listinfo/openssl-commits.html">0x6b206574, at x</A>[3]      # 'te k'
+	mov	4*4(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[4]
+	mov	4*5(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[5]
+	mov	4*6(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[6]
+	mov	4*7(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[7]
+	movd	%<A HREF="../../../mailman/listinfo/openssl-commits.html">xmm3, at x</A>[12]
+	mov	4*13(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[13]
+	mov	4*14(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[14]
+	mov	4*15(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[15]
+
+	mov	%rbp,64+0(%rsp)		# save len
+	mov	\$10,%ebp
+	mov	$inp,64+8(%rsp)		# save inp
+	movq	%xmm2,%rsi		# &quot;@x[8]&quot;
+	mov	$out,64+16(%rsp)	# save out
+	mov	%rsi,%rdi
+	shr	\$32,%rdi		# &quot;@x[9]&quot;
+	jmp	.Loop
+
+.align	32
+.Loop:
+___
+	foreach (&amp;ROUND (0, 4, 8,12)) { eval; }
+	foreach (&amp;ROUND	(0, 5,10,15)) { eval; }
+	&amp;dec	(&quot;%ebp&quot;);
+	&amp;jnz	(&quot;.Loop&quot;);
+
+$code.=&lt;&lt;___;
+	mov	@t[1],4*9(%rsp)		# modulo-scheduled
+	mov	@t[0],4*8(%rsp)
+	mov	64(%rsp),%rbp		# load len
+	movdqa	%xmm2,%xmm1
+	mov	64+8(%rsp),$inp		# load inp
+	paddd	%xmm4,%xmm3		# increment counter
+	mov	64+16(%rsp),$out	# load out
+
+	add	\$<A HREF="../../../mailman/listinfo/openssl-commits.html">0x61707865, at x</A>[0]      # 'expa'
+	add	\$<A HREF="../../../mailman/listinfo/openssl-commits.html">0x3320646e, at x</A>[1]      # 'nd 3'
+	add	\$<A HREF="../../../mailman/listinfo/openssl-commits.html">0x79622d32, at x</A>[2]      # '2-by'
+	add	\$<A HREF="../../../mailman/listinfo/openssl-commits.html">0x6b206574, at x</A>[3]      # 'te k'
+	add	4*4(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[4]
+	add	4*5(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[5]
+	add	4*6(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[6]
+	add	4*7(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[7]
+	add	4*12(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[12]
+	add	4*13(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[13]
+	add	4*14(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[14]
+	add	4*15(%rsp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[15]
+	paddd	4*8(%rsp),%xmm1
+
+	cmp	\$64,%rbp
+	jb	.Ltail
+
+	xor	4*0($inp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[0]		# xor with input
+	xor	4*1($inp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[1]
+	xor	4*2($inp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[2]
+	xor	4*3($inp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[3]
+	xor	4*4($inp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[4]
+	xor	4*5($inp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[5]
+	xor	4*6($inp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[6]
+	xor	4*7($inp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[7]
+	movdqu	4*8($inp),%xmm0
+	xor	4*12($inp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[12]
+	xor	4*13($inp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[13]
+	xor	4*14($inp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[14]
+	xor	4*15($inp)<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[15]
+	lea	4*16($inp),$inp		# inp+=64
+	pxor	%xmm1,%xmm0
+
+	movdqa	%xmm2,4*8(%rsp)
+	movd	%xmm3,4*12(%rsp)
+
+	mov	@x[0],4*0($out)		# write output
+	mov	@x[1],4*1($out)
+	mov	@x[2],4*2($out)
+	mov	@x[3],4*3($out)
+	mov	@x[4],4*4($out)
+	mov	@x[5],4*5($out)
+	mov	@x[6],4*6($out)
+	mov	@x[7],4*7($out)
+	movdqu	%xmm0,4*8($out)
+	mov	@x[12],4*12($out)
+	mov	@x[13],4*13($out)
+	mov	@x[14],4*14($out)
+	mov	@x[15],4*15($out)
+	lea	4*16($out),$out		# out+=64
+
+	sub	\$64,%rbp
+	jnz	.Loop_outer
+
+	jmp	.Ldone
+
+.align	16
+.Ltail:
+	mov	@x[0],4*0(%rsp)
+	xor	%rbx,%rbx
+	mov	@x[1],4*1(%rsp)
+	mov	@x[2],4*2(%rsp)
+	mov	@x[3],4*3(%rsp)
+	mov	@x[4],4*4(%rsp)
+	mov	@x[5],4*5(%rsp)
+	mov	@x[6],4*6(%rsp)
+	mov	@x[7],4*7(%rsp)
+	movdqa	%xmm1,4*8(%rsp)
+	mov	@x[12],4*12(%rsp)
+	mov	@x[13],4*13(%rsp)
+	mov	@x[14],4*14(%rsp)
+	mov	@x[15],4*15(%rsp)
+
+.Loop_tail:
+	movzb	($inp,%rbx),%eax
+	movzb	(%rsp,%rbx),%edx
+	lea	1(%rbx),%rbx
+	xor	%edx,%eax
+	mov	%al,-1($out,%rbx)
+	dec	%rbp
+	jnz	.Loop_tail
+
+.Ldone:
+	add	\$64+24,%rsp
+	pop	%r15
+	pop	%r14
+	pop	%r13
+	pop	%r12
+	pop	%rbp
+	pop	%rbx
+	ret
+.size	ChaCha20_ctr32,.-ChaCha20_ctr32
+___
+
+########################################################################
+# SSSE3 code path that handles shorter lengths
+{
+my ($a,$b,$c,$d,$t,$t1,$rot16,$rot24)=map(&quot;%xmm$_&quot;,(0..7));
+
+sub SSSE3ROUND {	# critical path is 20 &quot;SIMD ticks&quot; per round
+	&amp;paddd	($a,$b);
+	&amp;pxor	($d,$a);
+	&amp;pshufb	($d,$rot16);
+
+	&amp;paddd	($c,$d);
+	&amp;pxor	($b,$c);
+	&amp;movdqa	($t,$b);
+	&amp;psrld	($b,20);
+	&amp;pslld	($t,12);
+	&amp;por	($b,$t);
+
+	&amp;paddd	($a,$b);
+	&amp;pxor	($d,$a);
+	&amp;pshufb	($d,$rot24);
+
+	&amp;paddd	($c,$d);
+	&amp;pxor	($b,$c);
+	&amp;movdqa	($t,$b);
+	&amp;psrld	($b,25);
+	&amp;pslld	($t,7);
+	&amp;por	($b,$t);
+}
+
+my $xframe = $win64 ? 32+32+8 : 24;
+
+$code.=&lt;&lt;___;
+.type	ChaCha20_ssse3,\@function,5
+.align	32
+ChaCha20_ssse3:
+.LChaCha20_ssse3:
+___
+$code.=&lt;&lt;___	if ($avx);
+	test	\$`1&lt;&lt;(43-32)`,%r10d
+	jnz	.LChaCha20_4xop		# XOP is fastest even if we use 1/4
+___
+$code.=&lt;&lt;___;
+	cmp	\$128,$len		# we might throw away some data,
+	ja	.LChaCha20_4x		# but overall it won't be slower
+
+.Ldo_sse3_after_all:
+	push	%rbx
+	push	%rbp
+	push	%r12
+	push	%r13
+	push	%r14
+	push	%r15
+
+	sub	\$64+$xframe,%rsp
+___
+$code.=&lt;&lt;___	if ($win64);
+	movaps	%xmm6,64+32(%rsp)
+	movaps	%xmm7,64+48(%rsp)
+___
+$code.=&lt;&lt;___;
+	movdqa	.Lsigma(%rip),$a
+	movdqu	($key),$b
+	movdqu	16($key),$c
+	movdqu	($counter),$d
+	movdqa	.Lrot16(%rip),$rot16
+	movdqa	.Lrot24(%rip),$rot24
+
+	movdqa	$a,0x00(%rsp)
+	movdqa	$b,0x10(%rsp)
+	movdqa	$c,0x20(%rsp)
+	movdqa	$d,0x30(%rsp)
+	mov	\$10,%ebp
+	jmp	.Loop_ssse3
+
+.align	32
+.Loop_outer_ssse3:
+	movdqa	.Lone(%rip),$d
+	movdqa	0x00(%rsp),$a
+	movdqa	0x10(%rsp),$b
+	movdqa	0x20(%rsp),$c
+	paddd	0x30(%rsp),$d
+	mov	\$10,%ebp
+	movdqa	$d,0x30(%rsp)
+	jmp	.Loop_ssse3
+
+.align	32
+.Loop_ssse3:
+___
+	&amp;SSSE3ROUND();
+	&amp;pshufd	($c,$c,0b01001110);
+	&amp;pshufd	($b,$b,0b00111001);
+	&amp;pshufd	($d,$d,0b10010011);
+	&amp;nop	();
+
+	&amp;SSSE3ROUND();
+	&amp;pshufd	($c,$c,0b01001110);
+	&amp;pshufd	($b,$b,0b10010011);
+	&amp;pshufd	($d,$d,0b00111001);
+
+	&amp;dec	(&quot;%ebp&quot;);
+	&amp;jnz	(&quot;.Loop_ssse3&quot;);
+
+$code.=&lt;&lt;___;
+	paddd	0x00(%rsp),$a
+	paddd	0x10(%rsp),$b
+	paddd	0x20(%rsp),$c
+	paddd	0x30(%rsp),$d
+
+	cmp	\$64,$len
+	jb	.Ltail_ssse3
+
+	movdqu	0x00($inp),$t
+	movdqu	0x10($inp),$t1
+	pxor	$t,$a			# xor with input
+	movdqu	0x20($inp),$t
+	pxor	$t1,$b
+	movdqu	0x30($inp),$t1
+	lea	0x40($inp),$inp		# inp+=64
+	pxor	$t,$c
+	pxor	$t1,$d
+
+	movdqu	$a,0x00($out)		# write output
+	movdqu	$b,0x10($out)
+	movdqu	$c,0x20($out)
+	movdqu	$d,0x30($out)
+	lea	0x40($out),$out		# out+=64
+
+	sub	\$64,$len
+	jnz	.Loop_outer_ssse3
+
+	jmp	.Ldone_ssse3
+
+.align	16
+.Ltail_ssse3:
+	movdqa	$a,0x00(%rsp)
+	movdqa	$b,0x10(%rsp)
+	movdqa	$c,0x20(%rsp)
+	movdqa	$d,0x30(%rsp)
+	xor	%rbx,%rbx
+
+.Loop_tail_ssse3:
+	movzb	($inp,%rbx),%eax
+	movzb	(%rsp,%rbx),%edx
+	lea	1(%rbx),%rbx
+	xor	%edx,%eax
+	mov	%al,-1($out,%rbx)
+	inc	%rbp
+	jnz	.Loop_tail_ssse3
+
+.Ldone_ssse3:
+___
+$code.=&lt;&lt;___	if ($win64);
+	movaps	64+32(%rsp),%xmm6
+	movaps	64+48(%rsp),%xmm7
+___
+$code.=&lt;&lt;___;
+	add	\$64+$xframe,%rsp
+	pop	%r15
+	pop	%r14
+	pop	%r13
+	pop	%r12
+	pop	%rbp
+	pop	%rbx
+	ret
+.size	ChaCha20_ssse3,.-ChaCha20_ssse3
+___
+}
+
+########################################################################
+# SSSE3 code path that handles longer messages.
+{
+# assign variables to favor Atom front-end
+my ($xd0,$xd1,$xd2,$xd3, $xt0,$xt1,$xt2,$xt3,
+    $xa0,$xa1,$xa2,$xa3, $xb0,$xb1,$xb2,$xb3)=map(&quot;%xmm$_&quot;,(0..15));
+my  @xx=($xa0,$xa1,$xa2,$xa3, $xb0,$xb1,$xb2,$xb3,
+	&quot;%nox&quot;,&quot;%nox&quot;,&quot;%nox&quot;,&quot;%nox&quot;, $xd0,$xd1,$xd2,$xd3);
+
+sub SSSE3_lane_ROUND {
+my ($a0,$b0,$c0,$d0)=@_;
+my ($a1,$b1,$c1,$d1)=map(($_&amp;~3)+(($_+1)&amp;3),($a0,$b0,$c0,$d0));
+my ($a2,$b2,$c2,$d2)=map(($_&amp;~3)+(($_+1)&amp;3),($a1,$b1,$c1,$d1));
+my ($a3,$b3,$c3,$d3)=map(($_&amp;~3)+(($_+1)&amp;3),($a2,$b2,$c2,$d2));
+my ($xc,$xc_,$t0,$t1)=map(&quot;\&quot;$_\&quot;&quot;,$xt0,$xt1,$xt2,$xt3);
+my @x=map(&quot;\&quot;$_\&quot;&quot;<A HREF="../../../mailman/listinfo/openssl-commits.html">, at xx</A>);
+
+	# Consider order in which variables are addressed by their
+	# index:
+	#
+	#	a   b   c   d
+	#
+	#	0   4   8  12 &lt; even round
+	#	1   5   9  13
+	#	2   6  10  14
+	#	3   7  11  15
+	#	0   5  10  15 &lt; odd round
+	#	1   6  11  12
+	#	2   7   8  13
+	#	3   4   9  14
+	#
+	# 'a', 'b' and 'd's are permanently allocated in registers,
+	# @x[0..7,12..15], while 'c's are maintained in memory. If
+	# you observe 'c' column, you'll notice that pair of 'c's is
+	# invariant between rounds. This means that we have to reload
+	# them once per round, in the middle. This is why you'll see
+	# bunch of 'c' stores and loads in the middle, but none in
+	# the beginning or end.
+
+	(
+	&quot;&amp;paddd		(@x[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b0])&quot;,	# Q1
+	 &quot;&amp;paddd	(@x[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b1])&quot;,	# Q2
+	&quot;&amp;pxor		(@x[$d0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a0])&quot;,
+	 &quot;&amp;pxor		(@x[$d1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a1])&quot;,
+	&quot;&amp;pshufb	(@x[$d0],$t1)&quot;,
+	 &quot;&amp;pshufb	(@x[$d1],$t1)&quot;,
+
+	&quot;&amp;paddd		($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$d0])&quot;,
+	 &quot;&amp;paddd	($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$d1])&quot;,
+	&quot;&amp;pxor		(@x[$b0],$xc)&quot;,
+	 &quot;&amp;pxor		(@x[$b1],$xc_)&quot;,
+	&quot;&amp;movdqa	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t0, at x</A>[$b0])&quot;,
+	&quot;&amp;pslld		(@x[$b0],12)&quot;,
+	&quot;&amp;psrld		($t0,20)&quot;,
+	 &quot;&amp;movdqa	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t1, at x</A>[$b1])&quot;,
+	 &quot;&amp;pslld	(@x[$b1],12)&quot;,
+	&quot;&amp;por		(@x[$b0],$t0)&quot;,
+	 &quot;&amp;psrld	($t1,20)&quot;,
+	&quot;&amp;movdqa	($t0,'(%r11)')&quot;,	# .Lrot24(%rip)
+	 &quot;&amp;por		(@x[$b1],$t1)&quot;,
+
+	&quot;&amp;paddd		(@x[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b0])&quot;,
+	 &quot;&amp;paddd	(@x[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b1])&quot;,
+	&quot;&amp;pxor		(@x[$d0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a0])&quot;,
+	 &quot;&amp;pxor		(@x[$d1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a1])&quot;,
+	&quot;&amp;pshufb	(@x[$d0],$t0)&quot;,
+	 &quot;&amp;pshufb	(@x[$d1],$t0)&quot;,
+
+	&quot;&amp;paddd		($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$d0])&quot;,
+	 &quot;&amp;paddd	($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$d1])&quot;,
+	&quot;&amp;pxor		(@x[$b0],$xc)&quot;,
+	 &quot;&amp;pxor		(@x[$b1],$xc_)&quot;,
+	&quot;&amp;movdqa	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t1, at x</A>[$b0])&quot;,
+	&quot;&amp;pslld		(@x[$b0],7)&quot;,
+	&quot;&amp;psrld		($t1,25)&quot;,
+	 &quot;&amp;movdqa	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t0, at x</A>[$b1])&quot;,
+	 &quot;&amp;pslld	(@x[$b1],7)&quot;,
+	&quot;&amp;por		(@x[$b0],$t1)&quot;,
+	 &quot;&amp;psrld	($t0,25)&quot;,
+	&quot;&amp;movdqa	($t1,'(%r10)')&quot;,	# .Lrot16(%rip)
+	 &quot;&amp;por		(@x[$b1],$t0)&quot;,
+
+	&quot;&amp;movdqa	(\&quot;`16*($c0-8)`(%rsp)\&quot;,$xc)&quot;,	# reload pair of 'c's
+	 &quot;&amp;movdqa	(\&quot;`16*($c1-8)`(%rsp)\&quot;,$xc_)&quot;,
+	&quot;&amp;movdqa	($xc,\&quot;`16*($c2-8)`(%rsp)\&quot;)&quot;,
+	 &quot;&amp;movdqa	($xc_,\&quot;`16*($c3-8)`(%rsp)\&quot;)&quot;,
+
+	&quot;&amp;paddd		(@x[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b2])&quot;,	# Q3
+	 &quot;&amp;paddd	(@x[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b3])&quot;,	# Q4
+	&quot;&amp;pxor		(@x[$d2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a2])&quot;,
+	 &quot;&amp;pxor		(@x[$d3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a3])&quot;,
+	&quot;&amp;pshufb	(@x[$d2],$t1)&quot;,
+	 &quot;&amp;pshufb	(@x[$d3],$t1)&quot;,
+
+	&quot;&amp;paddd		($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$d2])&quot;,
+	 &quot;&amp;paddd	($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$d3])&quot;,
+	&quot;&amp;pxor		(@x[$b2],$xc)&quot;,
+	 &quot;&amp;pxor		(@x[$b3],$xc_)&quot;,
+	&quot;&amp;movdqa	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t0, at x</A>[$b2])&quot;,
+	&quot;&amp;pslld		(@x[$b2],12)&quot;,
+	&quot;&amp;psrld		($t0,20)&quot;,
+	 &quot;&amp;movdqa	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t1, at x</A>[$b3])&quot;,
+	 &quot;&amp;pslld	(@x[$b3],12)&quot;,
+	&quot;&amp;por		(@x[$b2],$t0)&quot;,
+	 &quot;&amp;psrld	($t1,20)&quot;,
+	&quot;&amp;movdqa	($t0,'(%r11)')&quot;,	# .Lrot24(%rip)
+	 &quot;&amp;por		(@x[$b3],$t1)&quot;,
+
+	&quot;&amp;paddd		(@x[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b2])&quot;,
+	 &quot;&amp;paddd	(@x[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b3])&quot;,
+	&quot;&amp;pxor		(@x[$d2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a2])&quot;,
+	 &quot;&amp;pxor		(@x[$d3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a3])&quot;,
+	&quot;&amp;pshufb	(@x[$d2],$t0)&quot;,
+	 &quot;&amp;pshufb	(@x[$d3],$t0)&quot;,
+
+	&quot;&amp;paddd		($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$d2])&quot;,
+	 &quot;&amp;paddd	($<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$d3])&quot;,
+	&quot;&amp;pxor		(@x[$b2],$xc)&quot;,
+	 &quot;&amp;pxor		(@x[$b3],$xc_)&quot;,
+	&quot;&amp;movdqa	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t1, at x</A>[$b2])&quot;,
+	&quot;&amp;pslld		(@x[$b2],7)&quot;,
+	&quot;&amp;psrld		($t1,25)&quot;,
+	 &quot;&amp;movdqa	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t0, at x</A>[$b3])&quot;,
+	 &quot;&amp;pslld	(@x[$b3],7)&quot;,
+	&quot;&amp;por		(@x[$b2],$t1)&quot;,
+	 &quot;&amp;psrld	($t0,25)&quot;,
+	&quot;&amp;movdqa	($t1,'(%r10)')&quot;,	# .Lrot16(%rip)
+	 &quot;&amp;por		(@x[$b3],$t0)&quot;
+	);
+}
+
+my $xframe = $win64 ? 0xa0 : 0;
+
+$code.=&lt;&lt;___;
+.type	ChaCha20_4x,\@function,5
+.align	32
+ChaCha20_4x:
+.LChaCha20_4x:
+	mov		%r10,%r11
+___
+$code.=&lt;&lt;___	if ($avx&gt;1);
+	shr		\$32,%r10		# OPENSSL_ia32cap_P+8
+	test		\$`1&lt;&lt;5`,%r10		# test AVX2
+	jnz		.LChaCha20_8x
+___
+$code.=&lt;&lt;___;
+	cmp		\$192,$len
+	ja		.Lproceed4x
+
+	and		\$`1&lt;&lt;26|1&lt;&lt;22`,%r11	# isolate XSAVE+MOVBE
+	cmp		\$`1&lt;&lt;22`,%r11		# check for MOVBE without XSAVE
+	je		.Ldo_sse3_after_all	# to detect Atom
+
+.Lproceed4x:
+	lea		-0x78(%rsp),%r11
+	sub		\$0x148+$xframe,%rsp
+___
+	################ stack layout
+	# +0x00		SIMD equivalent of @x[8-12]
+	# ...
+	# +0x40		constant copy of key[0-2] smashed by lanes
+	# ...
+	# +0x100	SIMD counters (with nonce smashed by lanes)
+	# ...
+	# +0x140
+$code.=&lt;&lt;___	if ($win64);
+	movaps		%xmm6,-0x30(%r11)
+	movaps		%xmm7,-0x20(%r11)
+	movaps		%xmm8,-0x10(%r11)
+	movaps		%xmm9,0x00(%r11)
+	movaps		%xmm10,0x10(%r11)
+	movaps		%xmm11,0x20(%r11)
+	movaps		%xmm12,0x30(%r11)
+	movaps		%xmm13,0x40(%r11)
+	movaps		%xmm14,0x50(%r11)
+	movaps		%xmm15,0x60(%r11)
+___
+$code.=&lt;&lt;___;
+	movdqa		.Lsigma(%rip),$xa3	# key[0]
+	movdqu		($key),$xb3		# key[1]
+	movdqu		16($key),$xt3		# key[2]
+	movdqu		($counter),$xd3		# key[3]
+	lea		0x100(%rsp),%rcx	# size optimization
+	lea		.Lrot16(%rip),%r10
+	lea		.Lrot24(%rip),%r11
+
+	pshufd		\$0x00,$xa3,$xa0	# smash key by lanes...
+	pshufd		\$0x55,$xa3,$xa1
+	movdqa		$xa0,0x40(%rsp)		# ... and offload
+	pshufd		\$0xaa,$xa3,$xa2
+	movdqa		$xa1,0x50(%rsp)
+	pshufd		\$0xff,$xa3,$xa3
+	movdqa		$xa2,0x60(%rsp)
+	movdqa		$xa3,0x70(%rsp)
+
+	pshufd		\$0x00,$xb3,$xb0
+	pshufd		\$0x55,$xb3,$xb1
+	movdqa		$xb0,0x80-0x100(%rcx)
+	pshufd		\$0xaa,$xb3,$xb2
+	movdqa		$xb1,0x90-0x100(%rcx)
+	pshufd		\$0xff,$xb3,$xb3
+	movdqa		$xb2,0xa0-0x100(%rcx)
+	movdqa		$xb3,0xb0-0x100(%rcx)
+
+	pshufd		\$0x00,$xt3,$xt0	# &quot;$xc0&quot;
+	pshufd		\$0x55,$xt3,$xt1	# &quot;$xc1&quot;
+	movdqa		$xt0,0xc0-0x100(%rcx)
+	pshufd		\$0xaa,$xt3,$xt2	# &quot;$xc2&quot;
+	movdqa		$xt1,0xd0-0x100(%rcx)
+	pshufd		\$0xff,$xt3,$xt3	# &quot;$xc3&quot;
+	movdqa		$xt2,0xe0-0x100(%rcx)
+	movdqa		$xt3,0xf0-0x100(%rcx)
+
+	pshufd		\$0x00,$xd3,$xd0
+	pshufd		\$0x55,$xd3,$xd1
+	paddd		.Linc(%rip),$xd0	# don't save counters yet
+	pshufd		\$0xaa,$xd3,$xd2
+	movdqa		$xd1,0x110-0x100(%rcx)
+	pshufd		\$0xff,$xd3,$xd3
+	movdqa		$xd2,0x120-0x100(%rcx)
+	movdqa		$xd3,0x130-0x100(%rcx)
+
+	jmp		.Loop_enter4x
+
+.align	32
+.Loop_outer4x:
+	movdqa		0x40(%rsp),$xa0		# re-load smashed key
+	movdqa		0x50(%rsp),$xa1
+	movdqa		0x60(%rsp),$xa2
+	movdqa		0x70(%rsp),$xa3
+	movdqa		0x80-0x100(%rcx),$xb0
+	movdqa		0x90-0x100(%rcx),$xb1
+	movdqa		0xa0-0x100(%rcx),$xb2
+	movdqa		0xb0-0x100(%rcx),$xb3
+	movdqa		0xc0-0x100(%rcx),$xt0	# &quot;$xc0&quot;
+	movdqa		0xd0-0x100(%rcx),$xt1	# &quot;$xc1&quot;
+	movdqa		0xe0-0x100(%rcx),$xt2	# &quot;$xc2&quot;
+	movdqa		0xf0-0x100(%rcx),$xt3	# &quot;$xc3&quot;
+	movdqa		0x100-0x100(%rcx),$xd0
+	movdqa		0x110-0x100(%rcx),$xd1
+	movdqa		0x120-0x100(%rcx),$xd2
+	movdqa		0x130-0x100(%rcx),$xd3
+	paddd		.Lfour(%rip),$xd0	# next SIMD counters
+
+.Loop_enter4x:
+	movdqa		$xt2,0x20(%rsp)		# SIMD equivalent of &quot;@x[10]&quot;
+	movdqa		$xt3,0x30(%rsp)		# SIMD equivalent of &quot;@x[11]&quot;
+	movdqa		(%r10),$xt3		# .Lrot16(%rip)
+	mov		\$10,%eax
+	movdqa		$xd0,0x100-0x100(%rcx)	# save SIMD counters
+	jmp		.Loop4x
+
+.align	32
+.Loop4x:
+___
+	foreach (&amp;SSSE3_lane_ROUND(0, 4, 8,12)) { eval; }
+	foreach (&amp;SSSE3_lane_ROUND(0, 5,10,15)) { eval; }
+$code.=&lt;&lt;___;
+	dec		%eax
+	jnz		.Loop4x
+
+	paddd		0x40(%rsp),$xa0		# accumulate key material
+	paddd		0x50(%rsp),$xa1
+	paddd		0x60(%rsp),$xa2
+	paddd		0x70(%rsp),$xa3
+
+	movdqa		$xa0,$xt2		# &quot;de-interlace&quot; data
+	punpckldq	$xa1,$xa0
+	movdqa		$xa2,$xt3
+	punpckldq	$xa3,$xa2
+	punpckhdq	$xa1,$xt2
+	punpckhdq	$xa3,$xt3
+	movdqa		$xa0,$xa1
+	punpcklqdq	$xa2,$xa0		# &quot;a0&quot;
+	movdqa		$xt2,$xa3
+	punpcklqdq	$xt3,$xt2		# &quot;a2&quot;
+	punpckhqdq	$xa2,$xa1		# &quot;a1&quot;
+	punpckhqdq	$xt3,$xa3		# &quot;a3&quot;
+___
+	($xa2,$xt2)=($xt2,$xa2);
+$code.=&lt;&lt;___;
+	paddd		0x80-0x100(%rcx),$xb0
+	paddd		0x90-0x100(%rcx),$xb1
+	paddd		0xa0-0x100(%rcx),$xb2
+	paddd		0xb0-0x100(%rcx),$xb3
+
+	movdqa		$xa0,0x00(%rsp)		# offload $xaN
+	movdqa		$xa1,0x10(%rsp)
+	movdqa		0x20(%rsp),$xa0		# &quot;xc2&quot;
+	movdqa		0x30(%rsp),$xa1		# &quot;xc3&quot;
+
+	movdqa		$xb0,$xt2
+	punpckldq	$xb1,$xb0
+	movdqa		$xb2,$xt3
+	punpckldq	$xb3,$xb2
+	punpckhdq	$xb1,$xt2
+	punpckhdq	$xb3,$xt3
+	movdqa		$xb0,$xb1
+	punpcklqdq	$xb2,$xb0		# &quot;b0&quot;
+	movdqa		$xt2,$xb3
+	punpcklqdq	$xt3,$xt2		# &quot;b2&quot;
+	punpckhqdq	$xb2,$xb1		# &quot;b1&quot;
+	punpckhqdq	$xt3,$xb3		# &quot;b3&quot;
+___
+	($xb2,$xt2)=($xt2,$xb2);
+	my ($xc0,$xc1,$xc2,$xc3)=($xt0,$xt1,$xa0,$xa1);
+$code.=&lt;&lt;___;
+	paddd		0xc0-0x100(%rcx),$xc0
+	paddd		0xd0-0x100(%rcx),$xc1
+	paddd		0xe0-0x100(%rcx),$xc2
+	paddd		0xf0-0x100(%rcx),$xc3
+
+	movdqa		$xa2,0x20(%rsp)		# keep offloading $xaN
+	movdqa		$xa3,0x30(%rsp)
+
+	movdqa		$xc0,$xt2
+	punpckldq	$xc1,$xc0
+	movdqa		$xc2,$xt3
+	punpckldq	$xc3,$xc2
+	punpckhdq	$xc1,$xt2
+	punpckhdq	$xc3,$xt3
+	movdqa		$xc0,$xc1
+	punpcklqdq	$xc2,$xc0		# &quot;c0&quot;
+	movdqa		$xt2,$xc3
+	punpcklqdq	$xt3,$xt2		# &quot;c2&quot;
+	punpckhqdq	$xc2,$xc1		# &quot;c1&quot;
+	punpckhqdq	$xt3,$xc3		# &quot;c3&quot;
+___
+	($xc2,$xt2)=($xt2,$xc2);
+	($xt0,$xt1)=($xa2,$xa3);		# use $xaN as temporary
+$code.=&lt;&lt;___;
+	paddd		0x100-0x100(%rcx),$xd0
+	paddd		0x110-0x100(%rcx),$xd1
+	paddd		0x120-0x100(%rcx),$xd2
+	paddd		0x130-0x100(%rcx),$xd3
+
+	movdqa		$xd0,$xt2
+	punpckldq	$xd1,$xd0
+	movdqa		$xd2,$xt3
+	punpckldq	$xd3,$xd2
+	punpckhdq	$xd1,$xt2
+	punpckhdq	$xd3,$xt3
+	movdqa		$xd0,$xd1
+	punpcklqdq	$xd2,$xd0		# &quot;d0&quot;
+	movdqa		$xt2,$xd3
+	punpcklqdq	$xt3,$xt2		# &quot;d2&quot;
+	punpckhqdq	$xd2,$xd1		# &quot;d1&quot;
+	punpckhqdq	$xt3,$xd3		# &quot;d3&quot;
+___
+	($xd2,$xt2)=($xt2,$xd2);
+$code.=&lt;&lt;___;
+	cmp		\$64*4,$len
+	jb		.Ltail4x
+
+	movdqu		0x00($inp),$xt0		# xor with input
+	movdqu		0x10($inp),$xt1
+	movdqu		0x20($inp),$xt2
+	movdqu		0x30($inp),$xt3
+	pxor		0x00(%rsp),$xt0		# $xaN is offloaded, remember?
+	pxor		$xb0,$xt1
+	pxor		$xc0,$xt2
+	pxor		$xd0,$xt3
+
+	 movdqu		$xt0,0x00($out)
+	movdqu		0x40($inp),$xt0
+	 movdqu		$xt1,0x10($out)
+	movdqu		0x50($inp),$xt1
+	 movdqu		$xt2,0x20($out)
+	movdqu		0x60($inp),$xt2
+	 movdqu		$xt3,0x30($out)
+	movdqu		0x70($inp),$xt3
+	lea		0x80($inp),$inp		# size optimization
+	pxor		0x10(%rsp),$xt0
+	pxor		$xb1,$xt1
+	pxor		$xc1,$xt2
+	pxor		$xd1,$xt3
+
+	 movdqu		$xt0,0x40($out)
+	movdqu		0x00($inp),$xt0
+	 movdqu		$xt1,0x50($out)
+	movdqu		0x10($inp),$xt1
+	 movdqu		$xt2,0x60($out)
+	movdqu		0x20($inp),$xt2
+	 movdqu		$xt3,0x70($out)
+	 lea		0x80($out),$out		# size optimization
+	movdqu		0x30($inp),$xt3
+	pxor		0x20(%rsp),$xt0
+	pxor		$xb2,$xt1
+	pxor		$xc2,$xt2
+	pxor		$xd2,$xt3
+
+	 movdqu		$xt0,0x00($out)
+	movdqu		0x40($inp),$xt0
+	 movdqu		$xt1,0x10($out)
+	movdqu		0x50($inp),$xt1
+	 movdqu		$xt2,0x20($out)
+	movdqu		0x60($inp),$xt2
+	 movdqu		$xt3,0x30($out)
+	movdqu		0x70($inp),$xt3
+	lea		0x80($inp),$inp		# inp+=64*4
+	pxor		0x30(%rsp),$xt0
+	pxor		$xb3,$xt1
+	pxor		$xc3,$xt2
+	pxor		$xd3,$xt3
+	movdqu		$xt0,0x40($out)
+	movdqu		$xt1,0x50($out)
+	movdqu		$xt2,0x60($out)
+	movdqu		$xt3,0x70($out)
+	lea		0x80($out),$out		# out+=64*4
+
+	sub		\$64*4,$len
+	jnz		.Loop_outer4x
+
+	jmp		.Ldone4x
+
+.Ltail4x:
+	cmp		\$192,$len
+	jae		.L192_or_more4x
+	cmp		\$128,$len
+	jae		.L128_or_more4x
+	cmp		\$64,$len
+	jae		.L64_or_more4x
+
+	#movdqa		0x00(%rsp),$xt0		# $xaN is offloaded, remember?
+	xor		%r10,%r10
+	#movdqa		$xt0,0x00(%rsp)
+	movdqa		$xb0,0x10(%rsp)
+	movdqa		$xc0,0x20(%rsp)
+	movdqa		$xd0,0x30(%rsp)
+	jmp		.Loop_tail4x
+
+.align	32
+.L64_or_more4x:
+	movdqu		0x00($inp),$xt0		# xor with input
+	movdqu		0x10($inp),$xt1
+	movdqu		0x20($inp),$xt2
+	movdqu		0x30($inp),$xt3
+	pxor		0x00(%rsp),$xt0		# $xaxN is offloaded, remember?
+	pxor		$xb0,$xt1
+	pxor		$xc0,$xt2
+	pxor		$xd0,$xt3
+	movdqu		$xt0,0x00($out)
+	movdqu		$xt1,0x10($out)
+	movdqu		$xt2,0x20($out)
+	movdqu		$xt3,0x30($out)
+	je		.Ldone4x
+
+	movdqa		0x10(%rsp),$xt0		# $xaN is offloaded, remember?
+	lea		0x40($inp),$inp		# inp+=64*1
+	xor		%r10,%r10
+	movdqa		$xt0,0x00(%rsp)
+	movdqa		$xb1,0x10(%rsp)
+	lea		0x40($out),$out		# out+=64*1
+	movdqa		$xc1,0x20(%rsp)
+	sub		\$64,$len		# len-=64*1
+	movdqa		$xd1,0x30(%rsp)
+	jmp		.Loop_tail4x
+
+.align	32
+.L128_or_more4x:
+	movdqu		0x00($inp),$xt0		# xor with input
+	movdqu		0x10($inp),$xt1
+	movdqu		0x20($inp),$xt2
+	movdqu		0x30($inp),$xt3
+	pxor		0x00(%rsp),$xt0		# $xaN is offloaded, remember?
+	pxor		$xb0,$xt1
+	pxor		$xc0,$xt2
+	pxor		$xd0,$xt3
+
+	 movdqu		$xt0,0x00($out)
+	movdqu		0x40($inp),$xt0
+	 movdqu		$xt1,0x10($out)
+	movdqu		0x50($inp),$xt1
+	 movdqu		$xt2,0x20($out)
+	movdqu		0x60($inp),$xt2
+	 movdqu		$xt3,0x30($out)
+	movdqu		0x70($inp),$xt3
+	pxor		0x10(%rsp),$xt0
+	pxor		$xb1,$xt1
+	pxor		$xc1,$xt2
+	pxor		$xd1,$xt3
+	movdqu		$xt0,0x40($out)
+	movdqu		$xt1,0x50($out)
+	movdqu		$xt2,0x60($out)
+	movdqu		$xt3,0x70($out)
+	je		.Ldone4x
+
+	movdqa		0x20(%rsp),$xt0		# $xaN is offloaded, remember?
+	lea		0x80($inp),$inp		# inp+=64*2
+	xor		%r10,%r10
+	movdqa		$xt0,0x00(%rsp)
+	movdqa		$xb2,0x10(%rsp)
+	lea		0x80($out),$out		# out+=64*2
+	movdqa		$xc2,0x20(%rsp)
+	sub		\$128,$len		# len-=64*2
+	movdqa		$xd2,0x30(%rsp)
+	jmp		.Loop_tail4x
+
+.align	32
+.L192_or_more4x:
+	movdqu		0x00($inp),$xt0		# xor with input
+	movdqu		0x10($inp),$xt1
+	movdqu		0x20($inp),$xt2
+	movdqu		0x30($inp),$xt3
+	pxor		0x00(%rsp),$xt0		# $xaN is offloaded, remember?
+	pxor		$xb0,$xt1
+	pxor		$xc0,$xt2
+	pxor		$xd0,$xt3
+
+	 movdqu		$xt0,0x00($out)
+	movdqu		0x40($inp),$xt0
+	 movdqu		$xt1,0x10($out)
+	movdqu		0x50($inp),$xt1
+	 movdqu		$xt2,0x20($out)
+	movdqu		0x60($inp),$xt2
+	 movdqu		$xt3,0x30($out)
+	movdqu		0x70($inp),$xt3
+	lea		0x80($inp),$inp		# size optimization
+	pxor		0x10(%rsp),$xt0
+	pxor		$xb1,$xt1
+	pxor		$xc1,$xt2
+	pxor		$xd1,$xt3
+
+	 movdqu		$xt0,0x40($out)
+	movdqu		0x00($inp),$xt0
+	 movdqu		$xt1,0x50($out)
+	movdqu		0x10($inp),$xt1
+	 movdqu		$xt2,0x60($out)
+	movdqu		0x20($inp),$xt2
+	 movdqu		$xt3,0x70($out)
+	 lea		0x80($out),$out		# size optimization
+	movdqu		0x30($inp),$xt3
+	pxor		0x20(%rsp),$xt0
+	pxor		$xb2,$xt1
+	pxor		$xc2,$xt2
+	pxor		$xd2,$xt3
+	movdqu		$xt0,0x00($out)
+	movdqu		$xt1,0x10($out)
+	movdqu		$xt2,0x20($out)
+	movdqu		$xt3,0x30($out)
+	je		.Ldone4x
+
+	movdqa		0x30(%rsp),$xt0		# $xaN is offloaded, remember?
+	lea		0x40($inp),$inp		# inp+=64*3
+	xor		%r10,%r10
+	movdqa		$xt0,0x00(%rsp)
+	movdqa		$xb3,0x10(%rsp)
+	lea		0x40($out),$out		# out+=64*3
+	movdqa		$xc3,0x20(%rsp)
+	sub		\$192,$len		# len-=64*3
+	movdqa		$xd3,0x30(%rsp)
+
+.Loop_tail4x:
+	movzb		($inp,%r10),%eax
+	movzb		(%rsp,%r10),%ecx
+	lea		1(%r10),%r10
+	xor		%ecx,%eax
+	mov		%al,-1($out,%r10)
+	dec		$len
+	jnz		.Loop_tail4x
+
+.Ldone4x:
+___
+$code.=&lt;&lt;___	if ($win64);
+	lea		0x140+0x30(%rsp),%r11
+	movaps		-0x30(%r11),%xmm6
+	movaps		-0x20(%r11),%xmm7
+	movaps		-0x10(%r11),%xmm8
+	movaps		0x00(%r11),%xmm9
+	movaps		0x10(%r11),%xmm10
+	movaps		0x20(%r11),%xmm11
+	movaps		0x30(%r11),%xmm12
+	movaps		0x40(%r11),%xmm13
+	movaps		0x50(%r11),%xmm14
+	movaps		0x60(%r11),%xmm15
+___
+$code.=&lt;&lt;___;
+	add		\$0x148+$xframe,%rsp
+	ret
+.size	ChaCha20_4x,.-ChaCha20_4x
+___
+}
+
+########################################################################
+# XOP code path that handles all lengths.
+if ($avx) {
+# There is some &quot;anomaly&quot; observed depending on instructions' size or
+# alignment. If you look closely at below code you'll notice that
+# sometimes argument order varies. The order affects instruction
+# encoding by making it larger, and such fiddling gives 5% performance
+# improvement. This is on FX-4100...
+
+my ($xb0,$xb1,$xb2,$xb3, $xd0,$xd1,$xd2,$xd3,
+    $xa0,$xa1,$xa2,$xa3, $xt0,$xt1,$xt2,$xt3)=map(&quot;%xmm$_&quot;,(0..15));
+my  @xx=($xa0,$xa1,$xa2,$xa3, $xb0,$xb1,$xb2,$xb3,
+	 $xt0,$xt1,$xt2,$xt3, $xd0,$xd1,$xd2,$xd3);
+
+sub XOP_lane_ROUND {
+my ($a0,$b0,$c0,$d0)=@_;
+my ($a1,$b1,$c1,$d1)=map(($_&amp;~3)+(($_+1)&amp;3),($a0,$b0,$c0,$d0));
+my ($a2,$b2,$c2,$d2)=map(($_&amp;~3)+(($_+1)&amp;3),($a1,$b1,$c1,$d1));
+my ($a3,$b3,$c3,$d3)=map(($_&amp;~3)+(($_+1)&amp;3),($a2,$b2,$c2,$d2));
+my @x=map(&quot;\&quot;$_\&quot;&quot;<A HREF="../../../mailman/listinfo/openssl-commits.html">, at xx</A>);
+
+	(
+	&quot;&amp;vpaddd	(@x[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b0])&quot;,	# Q1
+	 &quot;&amp;vpaddd	(@x[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b1])&quot;,	# Q2
+	  &quot;&amp;vpaddd	(@x[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b2])&quot;,	# Q3
+	   &quot;&amp;vpaddd	(@x[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b3])&quot;,	# Q4
+	&quot;&amp;vpxor		(@x[$d0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d0])&quot;,
+	 &quot;&amp;vpxor	(@x[$d1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d1])&quot;,
+	  &quot;&amp;vpxor	(@x[$d2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d2])&quot;,
+	   &quot;&amp;vpxor	(@x[$d3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d3])&quot;,
+	&quot;&amp;vprotd	(@x[$d0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d0],16)&quot;,
+	 &quot;&amp;vprotd	(@x[$d1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d1],16)&quot;,
+	  &quot;&amp;vprotd	(@x[$d2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d2],16)&quot;,
+	   &quot;&amp;vprotd	(@x[$d3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d3],16)&quot;,
+
+	&quot;&amp;vpaddd	(@x[$c0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d0])&quot;,
+	 &quot;&amp;vpaddd	(@x[$c1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d1])&quot;,
+	  &quot;&amp;vpaddd	(@x[$c2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d2])&quot;,
+	   &quot;&amp;vpaddd	(@x[$c3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d3])&quot;,
+	&quot;&amp;vpxor		(@x[$b0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b0])&quot;,
+	 &quot;&amp;vpxor	(@x[$b1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b1])&quot;,
+	  &quot;&amp;vpxor	(@x[$b2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c2])&quot;,	# flip
+	   &quot;&amp;vpxor	(@x[$b3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c3])&quot;,	# flip
+	&quot;&amp;vprotd	(@x[$b0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b0],12)&quot;,
+	 &quot;&amp;vprotd	(@x[$b1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b1],12)&quot;,
+	  &quot;&amp;vprotd	(@x[$b2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b2],12)&quot;,
+	   &quot;&amp;vprotd	(@x[$b3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b3],12)&quot;,
+
+	&quot;&amp;vpaddd	(@x[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a0])&quot;,	# flip
+	 &quot;&amp;vpaddd	(@x[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a1])&quot;,	# flip
+	  &quot;&amp;vpaddd	(@x[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b2])&quot;,
+	   &quot;&amp;vpaddd	(@x[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b3])&quot;,
+	&quot;&amp;vpxor		(@x[$d0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d0])&quot;,
+	 &quot;&amp;vpxor	(@x[$d1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d1])&quot;,
+	  &quot;&amp;vpxor	(@x[$d2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d2])&quot;,
+	   &quot;&amp;vpxor	(@x[$d3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d3])&quot;,
+	&quot;&amp;vprotd	(@x[$d0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d0],8)&quot;,
+	 &quot;&amp;vprotd	(@x[$d1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d1],8)&quot;,
+	  &quot;&amp;vprotd	(@x[$d2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d2],8)&quot;,
+	   &quot;&amp;vprotd	(@x[$d3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d3],8)&quot;,
+
+	&quot;&amp;vpaddd	(@x[$c0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d0])&quot;,
+	 &quot;&amp;vpaddd	(@x[$c1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d1])&quot;,
+	  &quot;&amp;vpaddd	(@x[$c2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d2])&quot;,
+	   &quot;&amp;vpaddd	(@x[$c3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d3])&quot;,
+	&quot;&amp;vpxor		(@x[$b0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b0])&quot;,
+	 &quot;&amp;vpxor	(@x[$b1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b1])&quot;,
+	  &quot;&amp;vpxor	(@x[$b2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c2])&quot;,	# flip
+	   &quot;&amp;vpxor	(@x[$b3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$c3])&quot;,	# flip
+	&quot;&amp;vprotd	(@x[$b0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b0],7)&quot;,
+	 &quot;&amp;vprotd	(@x[$b1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b1],7)&quot;,
+	  &quot;&amp;vprotd	(@x[$b2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b2],7)&quot;,
+	   &quot;&amp;vprotd	(@x[$b3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b3],7)&quot;
+	);
+}
+
+my $xframe = $win64 ? 0xa0 : 0;
+
+$code.=&lt;&lt;___;
+.type	ChaCha20_4xop,\@function,5
+.align	32
+ChaCha20_4xop:
+.LChaCha20_4xop:
+	lea		-0x78(%rsp),%r11
+	sub		\$0x148+$xframe,%rsp
+___
+	################ stack layout
+	# +0x00		SIMD equivalent of @x[8-12]
+	# ...
+	# +0x40		constant copy of key[0-2] smashed by lanes
+	# ...
+	# +0x100	SIMD counters (with nonce smashed by lanes)
+	# ...
+	# +0x140
+$code.=&lt;&lt;___	if ($win64);
+	movaps		%xmm6,-0x30(%r11)
+	movaps		%xmm7,-0x20(%r11)
+	movaps		%xmm8,-0x10(%r11)
+	movaps		%xmm9,0x00(%r11)
+	movaps		%xmm10,0x10(%r11)
+	movaps		%xmm11,0x20(%r11)
+	movaps		%xmm12,0x30(%r11)
+	movaps		%xmm13,0x40(%r11)
+	movaps		%xmm14,0x50(%r11)
+	movaps		%xmm15,0x60(%r11)
+___
+$code.=&lt;&lt;___;
+	vzeroupper
+
+	vmovdqa		.Lsigma(%rip),$xa3	# key[0]
+	vmovdqu		($key),$xb3		# key[1]
+	vmovdqu		16($key),$xt3		# key[2]
+	vmovdqu		($counter),$xd3		# key[3]
+	lea		0x100(%rsp),%rcx	# size optimization
+
+	vpshufd		\$0x00,$xa3,$xa0	# smash key by lanes...
+	vpshufd		\$0x55,$xa3,$xa1
+	vmovdqa		$xa0,0x40(%rsp)		# ... and offload
+	vpshufd		\$0xaa,$xa3,$xa2
+	vmovdqa		$xa1,0x50(%rsp)
+	vpshufd		\$0xff,$xa3,$xa3
+	vmovdqa		$xa2,0x60(%rsp)
+	vmovdqa		$xa3,0x70(%rsp)
+
+	vpshufd		\$0x00,$xb3,$xb0
+	vpshufd		\$0x55,$xb3,$xb1
+	vmovdqa		$xb0,0x80-0x100(%rcx)
+	vpshufd		\$0xaa,$xb3,$xb2
+	vmovdqa		$xb1,0x90-0x100(%rcx)
+	vpshufd		\$0xff,$xb3,$xb3
+	vmovdqa		$xb2,0xa0-0x100(%rcx)
+	vmovdqa		$xb3,0xb0-0x100(%rcx)
+
+	vpshufd		\$0x00,$xt3,$xt0	# &quot;$xc0&quot;
+	vpshufd		\$0x55,$xt3,$xt1	# &quot;$xc1&quot;
+	vmovdqa		$xt0,0xc0-0x100(%rcx)
+	vpshufd		\$0xaa,$xt3,$xt2	# &quot;$xc2&quot;
+	vmovdqa		$xt1,0xd0-0x100(%rcx)
+	vpshufd		\$0xff,$xt3,$xt3	# &quot;$xc3&quot;
+	vmovdqa		$xt2,0xe0-0x100(%rcx)
+	vmovdqa		$xt3,0xf0-0x100(%rcx)
+
+	vpshufd		\$0x00,$xd3,$xd0
+	vpshufd		\$0x55,$xd3,$xd1
+	vpaddd		.Linc(%rip),$xd0,$xd0	# don't save counters yet
+	vpshufd		\$0xaa,$xd3,$xd2
+	vmovdqa		$xd1,0x110-0x100(%rcx)
+	vpshufd		\$0xff,$xd3,$xd3
+	vmovdqa		$xd2,0x120-0x100(%rcx)
+	vmovdqa		$xd3,0x130-0x100(%rcx)
+
+	jmp		.Loop_enter4xop
+
+.align	32
+.Loop_outer4xop:
+	vmovdqa		0x40(%rsp),$xa0		# re-load smashed key
+	vmovdqa		0x50(%rsp),$xa1
+	vmovdqa		0x60(%rsp),$xa2
+	vmovdqa		0x70(%rsp),$xa3
+	vmovdqa		0x80-0x100(%rcx),$xb0
+	vmovdqa		0x90-0x100(%rcx),$xb1
+	vmovdqa		0xa0-0x100(%rcx),$xb2
+	vmovdqa		0xb0-0x100(%rcx),$xb3
+	vmovdqa		0xc0-0x100(%rcx),$xt0	# &quot;$xc0&quot;
+	vmovdqa		0xd0-0x100(%rcx),$xt1	# &quot;$xc1&quot;
+	vmovdqa		0xe0-0x100(%rcx),$xt2	# &quot;$xc2&quot;
+	vmovdqa		0xf0-0x100(%rcx),$xt3	# &quot;$xc3&quot;
+	vmovdqa		0x100-0x100(%rcx),$xd0
+	vmovdqa		0x110-0x100(%rcx),$xd1
+	vmovdqa		0x120-0x100(%rcx),$xd2
+	vmovdqa		0x130-0x100(%rcx),$xd3
+	vpaddd		.Lfour(%rip),$xd0,$xd0	# next SIMD counters
+
+.Loop_enter4xop:
+	mov		\$10,%eax
+	vmovdqa		$xd0,0x100-0x100(%rcx)	# save SIMD counters
+	jmp		.Loop4xop
+
+.align	32
+.Loop4xop:
+___
+	foreach (&amp;XOP_lane_ROUND(0, 4, 8,12)) { eval; }
+	foreach (&amp;XOP_lane_ROUND(0, 5,10,15)) { eval; }
+$code.=&lt;&lt;___;
+	dec		%eax
+	jnz		.Loop4xop
+
+	vpaddd		0x40(%rsp),$xa0,$xa0	# accumulate key material
+	vpaddd		0x50(%rsp),$xa1,$xa1
+	vpaddd		0x60(%rsp),$xa2,$xa2
+	vpaddd		0x70(%rsp),$xa3,$xa3
+
+	vmovdqa		$xt2,0x20(%rsp)		# offload $xc2,3
+	vmovdqa		$xt3,0x30(%rsp)
+
+	vpunpckldq	$xa1,$xa0,$xt2		# &quot;de-interlace&quot; data
+	vpunpckldq	$xa3,$xa2,$xt3
+	vpunpckhdq	$xa1,$xa0,$xa0
+	vpunpckhdq	$xa3,$xa2,$xa2
+	vpunpcklqdq	$xt3,$xt2,$xa1		# &quot;a0&quot;
+	vpunpckhqdq	$xt3,$xt2,$xt2		# &quot;a1&quot;
+	vpunpcklqdq	$xa2,$xa0,$xa3		# &quot;a2&quot;
+	vpunpckhqdq	$xa2,$xa0,$xa0		# &quot;a3&quot;
+___
+        ($xa0,$xa1,$xa2,$xa3,$xt2)=($xa1,$xt2,$xa3,$xa0,$xa2);
+$code.=&lt;&lt;___;
+	vpaddd		0x80-0x100(%rcx),$xb0,$xb0
+	vpaddd		0x90-0x100(%rcx),$xb1,$xb1
+	vpaddd		0xa0-0x100(%rcx),$xb2,$xb2
+	vpaddd		0xb0-0x100(%rcx),$xb3,$xb3
+
+	vmovdqa		$xa0,0x00(%rsp)		# offload $xa0,1
+	vmovdqa		$xa1,0x10(%rsp)
+	vmovdqa		0x20(%rsp),$xa0		# &quot;xc2&quot;
+	vmovdqa		0x30(%rsp),$xa1		# &quot;xc3&quot;
+
+	vpunpckldq	$xb1,$xb0,$xt2
+	vpunpckldq	$xb3,$xb2,$xt3
+	vpunpckhdq	$xb1,$xb0,$xb0
+	vpunpckhdq	$xb3,$xb2,$xb2
+	vpunpcklqdq	$xt3,$xt2,$xb1		# &quot;b0&quot;
+	vpunpckhqdq	$xt3,$xt2,$xt2		# &quot;b1&quot;
+	vpunpcklqdq	$xb2,$xb0,$xb3		# &quot;b2&quot;
+	vpunpckhqdq	$xb2,$xb0,$xb0		# &quot;b3&quot;
+___
+	($xb0,$xb1,$xb2,$xb3,$xt2)=($xb1,$xt2,$xb3,$xb0,$xb2);
+	my ($xc0,$xc1,$xc2,$xc3)=($xt0,$xt1,$xa0,$xa1);
+$code.=&lt;&lt;___;
+	vpaddd		0xc0-0x100(%rcx),$xc0,$xc0
+	vpaddd		0xd0-0x100(%rcx),$xc1,$xc1
+	vpaddd		0xe0-0x100(%rcx),$xc2,$xc2
+	vpaddd		0xf0-0x100(%rcx),$xc3,$xc3
+
+	vpunpckldq	$xc1,$xc0,$xt2
+	vpunpckldq	$xc3,$xc2,$xt3
+	vpunpckhdq	$xc1,$xc0,$xc0
+	vpunpckhdq	$xc3,$xc2,$xc2
+	vpunpcklqdq	$xt3,$xt2,$xc1		# &quot;c0&quot;
+	vpunpckhqdq	$xt3,$xt2,$xt2		# &quot;c1&quot;
+	vpunpcklqdq	$xc2,$xc0,$xc3		# &quot;c2&quot;
+	vpunpckhqdq	$xc2,$xc0,$xc0		# &quot;c3&quot;
+___
+	($xc0,$xc1,$xc2,$xc3,$xt2)=($xc1,$xt2,$xc3,$xc0,$xc2);
+$code.=&lt;&lt;___;
+	vpaddd		0x100-0x100(%rcx),$xd0,$xd0
+	vpaddd		0x110-0x100(%rcx),$xd1,$xd1
+	vpaddd		0x120-0x100(%rcx),$xd2,$xd2
+	vpaddd		0x130-0x100(%rcx),$xd3,$xd3
+
+	vpunpckldq	$xd1,$xd0,$xt2
+	vpunpckldq	$xd3,$xd2,$xt3
+	vpunpckhdq	$xd1,$xd0,$xd0
+	vpunpckhdq	$xd3,$xd2,$xd2
+	vpunpcklqdq	$xt3,$xt2,$xd1		# &quot;d0&quot;
+	vpunpckhqdq	$xt3,$xt2,$xt2		# &quot;d1&quot;
+	vpunpcklqdq	$xd2,$xd0,$xd3		# &quot;d2&quot;
+	vpunpckhqdq	$xd2,$xd0,$xd0		# &quot;d3&quot;
+___
+	($xd0,$xd1,$xd2,$xd3,$xt2)=($xd1,$xt2,$xd3,$xd0,$xd2);
+	($xa0,$xa1)=($xt2,$xt3);
+$code.=&lt;&lt;___;
+	vmovdqa		0x00(%rsp),$xa0		# restore $xa0,1
+	vmovdqa		0x10(%rsp),$xa1
+
+	cmp		\$64*4,$len
+	jb		.Ltail4xop
+
+	vpxor		0x00($inp),$xa0,$xa0	# xor with input
+	vpxor		0x10($inp),$xb0,$xb0
+	vpxor		0x20($inp),$xc0,$xc0
+	vpxor		0x30($inp),$xd0,$xd0
+	vpxor		0x40($inp),$xa1,$xa1
+	vpxor		0x50($inp),$xb1,$xb1
+	vpxor		0x60($inp),$xc1,$xc1
+	vpxor		0x70($inp),$xd1,$xd1
+	lea		0x80($inp),$inp		# size optimization
+	vpxor		0x00($inp),$xa2,$xa2
+	vpxor		0x10($inp),$xb2,$xb2
+	vpxor		0x20($inp),$xc2,$xc2
+	vpxor		0x30($inp),$xd2,$xd2
+	vpxor		0x40($inp),$xa3,$xa3
+	vpxor		0x50($inp),$xb3,$xb3
+	vpxor		0x60($inp),$xc3,$xc3
+	vpxor		0x70($inp),$xd3,$xd3
+	lea		0x80($inp),$inp		# inp+=64*4
+
+	vmovdqu		$xa0,0x00($out)
+	vmovdqu		$xb0,0x10($out)
+	vmovdqu		$xc0,0x20($out)
+	vmovdqu		$xd0,0x30($out)
+	vmovdqu		$xa1,0x40($out)
+	vmovdqu		$xb1,0x50($out)
+	vmovdqu		$xc1,0x60($out)
+	vmovdqu		$xd1,0x70($out)
+	lea		0x80($out),$out		# size optimization
+	vmovdqu		$xa2,0x00($out)
+	vmovdqu		$xb2,0x10($out)
+	vmovdqu		$xc2,0x20($out)
+	vmovdqu		$xd2,0x30($out)
+	vmovdqu		$xa3,0x40($out)
+	vmovdqu		$xb3,0x50($out)
+	vmovdqu		$xc3,0x60($out)
+	vmovdqu		$xd3,0x70($out)
+	lea		0x80($out),$out		# out+=64*4
+
+	sub		\$64*4,$len
+	jnz		.Loop_outer4xop
+
+	jmp		.Ldone4xop
+
+.align	32
+.Ltail4xop:
+	cmp		\$192,$len
+	jae		.L192_or_more4xop
+	cmp		\$128,$len
+	jae		.L128_or_more4xop
+	cmp		\$64,$len
+	jae		.L64_or_more4xop
+
+	xor		%r10,%r10
+	vmovdqa		$xa0,0x00(%rsp)
+	vmovdqa		$xb0,0x10(%rsp)
+	vmovdqa		$xc0,0x20(%rsp)
+	vmovdqa		$xd0,0x30(%rsp)
+	jmp		.Loop_tail4xop
+
+.align	32
+.L64_or_more4xop:
+	vpxor		0x00($inp),$xa0,$xa0	# xor with input
+	vpxor		0x10($inp),$xb0,$xb0
+	vpxor		0x20($inp),$xc0,$xc0
+	vpxor		0x30($inp),$xd0,$xd0
+	vmovdqu		$xa0,0x00($out)
+	vmovdqu		$xb0,0x10($out)
+	vmovdqu		$xc0,0x20($out)
+	vmovdqu		$xd0,0x30($out)
+	je		.Ldone4xop
+
+	lea		0x40($inp),$inp		# inp+=64*1
+	vmovdqa		$xa1,0x00(%rsp)
+	xor		%r10,%r10
+	vmovdqa		$xb1,0x10(%rsp)
+	lea		0x40($out),$out		# out+=64*1
+	vmovdqa		$xc1,0x20(%rsp)
+	sub		\$64,$len		# len-=64*1
+	vmovdqa		$xd1,0x30(%rsp)
+	jmp		.Loop_tail4xop
+
+.align	32
+.L128_or_more4xop:
+	vpxor		0x00($inp),$xa0,$xa0	# xor with input
+	vpxor		0x10($inp),$xb0,$xb0
+	vpxor		0x20($inp),$xc0,$xc0
+	vpxor		0x30($inp),$xd0,$xd0
+	vpxor		0x40($inp),$xa1,$xa1
+	vpxor		0x50($inp),$xb1,$xb1
+	vpxor		0x60($inp),$xc1,$xc1
+	vpxor		0x70($inp),$xd1,$xd1
+
+	vmovdqu		$xa0,0x00($out)
+	vmovdqu		$xb0,0x10($out)
+	vmovdqu		$xc0,0x20($out)
+	vmovdqu		$xd0,0x30($out)
+	vmovdqu		$xa1,0x40($out)
+	vmovdqu		$xb1,0x50($out)
+	vmovdqu		$xc1,0x60($out)
+	vmovdqu		$xd1,0x70($out)
+	je		.Ldone4xop
+
+	lea		0x80($inp),$inp		# inp+=64*2
+	vmovdqa		$xa2,0x00(%rsp)
+	xor		%r10,%r10
+	vmovdqa		$xb2,0x10(%rsp)
+	lea		0x80($out),$out		# out+=64*2
+	vmovdqa		$xc2,0x20(%rsp)
+	sub		\$128,$len		# len-=64*2
+	vmovdqa		$xd2,0x30(%rsp)
+	jmp		.Loop_tail4xop
+
+.align	32
+.L192_or_more4xop:
+	vpxor		0x00($inp),$xa0,$xa0	# xor with input
+	vpxor		0x10($inp),$xb0,$xb0
+	vpxor		0x20($inp),$xc0,$xc0
+	vpxor		0x30($inp),$xd0,$xd0
+	vpxor		0x40($inp),$xa1,$xa1
+	vpxor		0x50($inp),$xb1,$xb1
+	vpxor		0x60($inp),$xc1,$xc1
+	vpxor		0x70($inp),$xd1,$xd1
+	lea		0x80($inp),$inp		# size optimization
+	vpxor		0x00($inp),$xa2,$xa2
+	vpxor		0x10($inp),$xb2,$xb2
+	vpxor		0x20($inp),$xc2,$xc2
+	vpxor		0x30($inp),$xd2,$xd2
+
+	vmovdqu		$xa0,0x00($out)
+	vmovdqu		$xb0,0x10($out)
+	vmovdqu		$xc0,0x20($out)
+	vmovdqu		$xd0,0x30($out)
+	vmovdqu		$xa1,0x40($out)
+	vmovdqu		$xb1,0x50($out)
+	vmovdqu		$xc1,0x60($out)
+	vmovdqu		$xd1,0x70($out)
+	lea		0x80($out),$out		# size optimization
+	vmovdqu		$xa2,0x00($out)
+	vmovdqu		$xb2,0x10($out)
+	vmovdqu		$xc2,0x20($out)
+	vmovdqu		$xd2,0x30($out)
+	je		.Ldone4xop
+
+	lea		0x40($inp),$inp		# inp+=64*3
+	vmovdqa		$xa2,0x00(%rsp)
+	xor		%r10,%r10
+	vmovdqa		$xb2,0x10(%rsp)
+	lea		0x40($out),$out		# out+=64*3
+	vmovdqa		$xc2,0x20(%rsp)
+	sub		\$192,$len		# len-=64*3
+	vmovdqa		$xd2,0x30(%rsp)
+
+.Loop_tail4xop:
+	movzb		($inp,%r10),%eax
+	movzb		(%rsp,%r10),%ecx
+	lea		1(%r10),%r10
+	xor		%ecx,%eax
+	mov		%al,-1($out,%r10)
+	dec		$len
+	jnz		.Loop_tail4xop
+
+.Ldone4xop:
+	vzeroupper
+___
+$code.=&lt;&lt;___	if ($win64);
+	lea		0x140+0x30(%rsp),%r11
+	movaps		-0x30(%r11),%xmm6
+	movaps		-0x20(%r11),%xmm7
+	movaps		-0x10(%r11),%xmm8
+	movaps		0x00(%r11),%xmm9
+	movaps		0x10(%r11),%xmm10
+	movaps		0x20(%r11),%xmm11
+	movaps		0x30(%r11),%xmm12
+	movaps		0x40(%r11),%xmm13
+	movaps		0x50(%r11),%xmm14
+	movaps		0x60(%r11),%xmm15
+___
+$code.=&lt;&lt;___;
+	add		\$0x148+$xframe,%rsp
+	ret
+.size	ChaCha20_4xop,.-ChaCha20_4xop
+___
+}
+
+########################################################################
+# AVX2 code path
+if ($avx&gt;1) {
+my ($xb0,$xb1,$xb2,$xb3, $xd0,$xd1,$xd2,$xd3,
+    $xa0,$xa1,$xa2,$xa3, $xt0,$xt1,$xt2,$xt3)=map(&quot;%ymm$_&quot;,(0..15));
+my @xx=($xa0,$xa1,$xa2,$xa3, $xb0,$xb1,$xb2,$xb3,
+	&quot;%nox&quot;,&quot;%nox&quot;,&quot;%nox&quot;,&quot;%nox&quot;, $xd0,$xd1,$xd2,$xd3);
+
+sub AVX2_lane_ROUND {
+my ($a0,$b0,$c0,$d0)=@_;
+my ($a1,$b1,$c1,$d1)=map(($_&amp;~3)+(($_+1)&amp;3),($a0,$b0,$c0,$d0));
+my ($a2,$b2,$c2,$d2)=map(($_&amp;~3)+(($_+1)&amp;3),($a1,$b1,$c1,$d1));
+my ($a3,$b3,$c3,$d3)=map(($_&amp;~3)+(($_+1)&amp;3),($a2,$b2,$c2,$d2));
+my ($xc,$xc_,$t0,$t1)=map(&quot;\&quot;$_\&quot;&quot;,$xt0,$xt1,$xt2,$xt3);
+my @x=map(&quot;\&quot;$_\&quot;&quot;<A HREF="../../../mailman/listinfo/openssl-commits.html">, at xx</A>);
+
+	# Consider order in which variables are addressed by their
+	# index:
+	#
+	#	a   b   c   d
+	#
+	#	0   4   8  12 &lt; even round
+	#	1   5   9  13
+	#	2   6  10  14
+	#	3   7  11  15
+	#	0   5  10  15 &lt; odd round
+	#	1   6  11  12
+	#	2   7   8  13
+	#	3   4   9  14
+	#
+	# 'a', 'b' and 'd's are permanently allocated in registers,
+	# @x[0..7,12..15], while 'c's are maintained in memory. If
+	# you observe 'c' column, you'll notice that pair of 'c's is
+	# invariant between rounds. This means that we have to reload
+	# them once per round, in the middle. This is why you'll see
+	# bunch of 'c' stores and loads in the middle, but none in
+	# the beginning or end.
+
+	(
+	&quot;&amp;vpaddd	(@x[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b0])&quot;,	# Q1
+	&quot;&amp;vpxor		(@x[$d0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d0])&quot;,
+	&quot;&amp;vpshufb	(@x[$d0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d0],$t1)&quot;,
+	 &quot;&amp;vpaddd	(@x[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b1])&quot;,	# Q2
+	 &quot;&amp;vpxor	(@x[$d1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d1])&quot;,
+	 &quot;&amp;vpshufb	(@x[$d1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d1],$t1)&quot;,
+
+	&quot;&amp;vpaddd	($xc,$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$d0])&quot;,
+	&quot;&amp;vpxor		(@x[$b0],$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$b0])&quot;,
+	&quot;&amp;vpslld	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t0, at x</A>[$b0],12)&quot;,
+	&quot;&amp;vpsrld	(@x[$b0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b0],20)&quot;,
+	&quot;&amp;vpor		(@x[$b0],$<A HREF="../../../mailman/listinfo/openssl-commits.html">t0, at x</A>[$b0])&quot;,
+	&quot;&amp;vbroadcasti128($t0,'(%r11)')&quot;,		# .Lrot24(%rip)
+	 &quot;&amp;vpaddd	($xc_,$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$d1])&quot;,
+	 &quot;&amp;vpxor	(@x[$b1],$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$b1])&quot;,
+	 &quot;&amp;vpslld	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t1, at x</A>[$b1],12)&quot;,
+	 &quot;&amp;vpsrld	(@x[$b1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b1],20)&quot;,
+	 &quot;&amp;vpor		(@x[$b1],$<A HREF="../../../mailman/listinfo/openssl-commits.html">t1, at x</A>[$b1])&quot;,
+
+	&quot;&amp;vpaddd	(@x[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b0])&quot;,
+	&quot;&amp;vpxor		(@x[$d0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d0])&quot;,
+	&quot;&amp;vpshufb	(@x[$d0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d0],$t0)&quot;,
+	 &quot;&amp;vpaddd	(@x[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b1])&quot;,
+	 &quot;&amp;vpxor	(@x[$d1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d1])&quot;,
+	 &quot;&amp;vpshufb	(@x[$d1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d1],$t0)&quot;,
+
+	&quot;&amp;vpaddd	($xc,$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$d0])&quot;,
+	&quot;&amp;vpxor		(@x[$b0],$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$b0])&quot;,
+	&quot;&amp;vpslld	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t1, at x</A>[$b0],7)&quot;,
+	&quot;&amp;vpsrld	(@x[$b0]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b0],25)&quot;,
+	&quot;&amp;vpor		(@x[$b0],$<A HREF="../../../mailman/listinfo/openssl-commits.html">t1, at x</A>[$b0])&quot;,
+	&quot;&amp;vbroadcasti128($t1,'(%r10)')&quot;,		# .Lrot16(%rip)
+	 &quot;&amp;vpaddd	($xc_,$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$d1])&quot;,
+	 &quot;&amp;vpxor	(@x[$b1],$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$b1])&quot;,
+	 &quot;&amp;vpslld	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t0, at x</A>[$b1],7)&quot;,
+	 &quot;&amp;vpsrld	(@x[$b1]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b1],25)&quot;,
+	 &quot;&amp;vpor		(@x[$b1],$<A HREF="../../../mailman/listinfo/openssl-commits.html">t0, at x</A>[$b1])&quot;,
+
+	&quot;&amp;vmovdqa	(\&quot;`32*($c0-8)`(%rsp)\&quot;,$xc)&quot;,	# reload pair of 'c's
+	 &quot;&amp;vmovdqa	(\&quot;`32*($c1-8)`(%rsp)\&quot;,$xc_)&quot;,
+	&quot;&amp;vmovdqa	($xc,\&quot;`32*($c2-8)`(%rsp)\&quot;)&quot;,
+	 &quot;&amp;vmovdqa	($xc_,\&quot;`32*($c3-8)`(%rsp)\&quot;)&quot;,
+
+	&quot;&amp;vpaddd	(@x[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b2])&quot;,	# Q3
+	&quot;&amp;vpxor		(@x[$d2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d2])&quot;,
+	&quot;&amp;vpshufb	(@x[$d2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d2],$t1)&quot;,
+	 &quot;&amp;vpaddd	(@x[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b3])&quot;,	# Q4
+	 &quot;&amp;vpxor	(@x[$d3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d3])&quot;,
+	 &quot;&amp;vpshufb	(@x[$d3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d3],$t1)&quot;,
+
+	&quot;&amp;vpaddd	($xc,$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$d2])&quot;,
+	&quot;&amp;vpxor		(@x[$b2],$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$b2])&quot;,
+	&quot;&amp;vpslld	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t0, at x</A>[$b2],12)&quot;,
+	&quot;&amp;vpsrld	(@x[$b2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b2],20)&quot;,
+	&quot;&amp;vpor		(@x[$b2],$<A HREF="../../../mailman/listinfo/openssl-commits.html">t0, at x</A>[$b2])&quot;,
+	&quot;&amp;vbroadcasti128($t0,'(%r11)')&quot;,		# .Lrot24(%rip)
+	 &quot;&amp;vpaddd	($xc_,$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$d3])&quot;,
+	 &quot;&amp;vpxor	(@x[$b3],$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$b3])&quot;,
+	 &quot;&amp;vpslld	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t1, at x</A>[$b3],12)&quot;,
+	 &quot;&amp;vpsrld	(@x[$b3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b3],20)&quot;,
+	 &quot;&amp;vpor		(@x[$b3],$<A HREF="../../../mailman/listinfo/openssl-commits.html">t1, at x</A>[$b3])&quot;,
+
+	&quot;&amp;vpaddd	(@x[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b2])&quot;,
+	&quot;&amp;vpxor		(@x[$d2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d2])&quot;,
+	&quot;&amp;vpshufb	(@x[$d2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d2],$t0)&quot;,
+	 &quot;&amp;vpaddd	(@x[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b3])&quot;,
+	 &quot;&amp;vpxor	(@x[$d3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$a3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d3])&quot;,
+	 &quot;&amp;vpshufb	(@x[$d3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$d3],$t0)&quot;,
+
+	&quot;&amp;vpaddd	($xc,$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$d2])&quot;,
+	&quot;&amp;vpxor		(@x[$b2],$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc, at x</A>[$b2])&quot;,
+	&quot;&amp;vpslld	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t1, at x</A>[$b2],7)&quot;,
+	&quot;&amp;vpsrld	(@x[$b2]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b2],25)&quot;,
+	&quot;&amp;vpor		(@x[$b2],$<A HREF="../../../mailman/listinfo/openssl-commits.html">t1, at x</A>[$b2])&quot;,
+	&quot;&amp;vbroadcasti128($t1,'(%r10)')&quot;,		# .Lrot16(%rip)
+	 &quot;&amp;vpaddd	($xc_,$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$d3])&quot;,
+	 &quot;&amp;vpxor	(@x[$b3],$<A HREF="../../../mailman/listinfo/openssl-commits.html">xc_, at x</A>[$b3])&quot;,
+	 &quot;&amp;vpslld	($<A HREF="../../../mailman/listinfo/openssl-commits.html">t0, at x</A>[$b3],7)&quot;,
+	 &quot;&amp;vpsrld	(@x[$b3]<A HREF="../../../mailman/listinfo/openssl-commits.html">, at x</A>[$b3],25)&quot;,
+	 &quot;&amp;vpor		(@x[$b3],$<A HREF="../../../mailman/listinfo/openssl-commits.html">t0, at x</A>[$b3])&quot;
+	);
+}
+
+my $xframe = $win64 ? 0xb0 : 8;
+
+$code.=&lt;&lt;___;
+.type	ChaCha20_8x,\@function,5
+.align	32
+ChaCha20_8x:
+.LChaCha20_8x:
+	mov		%rsp,%r10
+	sub		\$0x280+$xframe,%rsp
+	and		\$-32,%rsp
+___
+$code.=&lt;&lt;___	if ($win64);
+	lea		0x290+0x30(%rsp),%r11
+	movaps		%xmm6,-0x30(%r11)
+	movaps		%xmm7,-0x20(%r11)
+	movaps		%xmm8,-0x10(%r11)
+	movaps		%xmm9,0x00(%r11)
+	movaps		%xmm10,0x10(%r11)
+	movaps		%xmm11,0x20(%r11)
+	movaps		%xmm12,0x30(%r11)
+	movaps		%xmm13,0x40(%r11)
+	movaps		%xmm14,0x50(%r11)
+	movaps		%xmm15,0x60(%r11)
+___
+$code.=&lt;&lt;___;
+	vzeroupper
+	mov		%r10,0x280(%rsp)
+
+	################ stack layout
+	# +0x00		SIMD equivalent of @x[8-12]
+	# ...
+	# +0x80		constant copy of key[0-2] smashed by lanes
+	# ...
+	# +0x200	SIMD counters (with nonce smashed by lanes)
+	# ...
+	# +0x280	saved %rsp
+
+	vbroadcasti128	.Lsigma(%rip),$xa3	# key[0]
+	vbroadcasti128	($key),$xb3		# key[1]
+	vbroadcasti128	16($key),$xt3		# key[2]
+	vbroadcasti128	($counter),$xd3		# key[3]
+	lea		0x100(%rsp),%rcx	# size optimization
+	lea		0x200(%rsp),%rax	# size optimization
+	lea		.Lrot16(%rip),%r10
+	lea		.Lrot24(%rip),%r11
+
+	vpshufd		\$0x00,$xa3,$xa0	# smash key by lanes...
+	vpshufd		\$0x55,$xa3,$xa1
+	vmovdqa		$xa0,0x80-0x100(%rcx)	# ... and offload
+	vpshufd		\$0xaa,$xa3,$xa2
+	vmovdqa		$xa1,0xa0-0x100(%rcx)
+	vpshufd		\$0xff,$xa3,$xa3
+	vmovdqa		$xa2,0xc0-0x100(%rcx)
+	vmovdqa		$xa3,0xe0-0x100(%rcx)
+
+	vpshufd		\$0x00,$xb3,$xb0
+	vpshufd		\$0x55,$xb3,$xb1
+	vmovdqa		$xb0,0x100-0x100(%rcx)
+	vpshufd		\$0xaa,$xb3,$xb2
+	vmovdqa		$xb1,0x120-0x100(%rcx)
+	vpshufd		\$0xff,$xb3,$xb3
+	vmovdqa		$xb2,0x140-0x100(%rcx)
+	vmovdqa		$xb3,0x160-0x100(%rcx)
+
+	vpshufd		\$0x00,$xt3,$xt0	# &quot;xc0&quot;
+	vpshufd		\$0x55,$xt3,$xt1	# &quot;xc1&quot;
+	vmovdqa		$xt0,0x180-0x200(%rax)
+	vpshufd		\$0xaa,$xt3,$xt2	# &quot;xc2&quot;
+	vmovdqa		$xt1,0x1a0-0x200(%rax)
+	vpshufd		\$0xff,$xt3,$xt3	# &quot;xc3&quot;
+	vmovdqa		$xt2,0x1c0-0x200(%rax)
+	vmovdqa		$xt3,0x1e0-0x200(%rax)
+
+	vpshufd		\$0x00,$xd3,$xd0
+	vpshufd		\$0x55,$xd3,$xd1
+	vpaddd		.Lincy(%rip),$xd0,$xd0	# don't save counters yet
+	vpshufd		\$0xaa,$xd3,$xd2
+	vmovdqa		$xd1,0x220-0x200(%rax)
+	vpshufd		\$0xff,$xd3,$xd3
+	vmovdqa		$xd2,0x240-0x200(%rax)
+	vmovdqa		$xd3,0x260-0x200(%rax)
+
+	jmp		.Loop_enter8x
+
+.align	32
+.Loop_outer8x:
+	vmovdqa		0x80-0x100(%rcx),$xa0	# re-load smashed key
+	vmovdqa		0xa0-0x100(%rcx),$xa1
+	vmovdqa		0xc0-0x100(%rcx),$xa2
+	vmovdqa		0xe0-0x100(%rcx),$xa3
+	vmovdqa		0x100-0x100(%rcx),$xb0
+	vmovdqa		0x120-0x100(%rcx),$xb1
+	vmovdqa		0x140-0x100(%rcx),$xb2
+	vmovdqa		0x160-0x100(%rcx),$xb3
+	vmovdqa		0x180-0x200(%rax),$xt0	# &quot;xc0&quot;
+	vmovdqa		0x1a0-0x200(%rax),$xt1	# &quot;xc1&quot;
+	vmovdqa		0x1c0-0x200(%rax),$xt2	# &quot;xc2&quot;
+	vmovdqa		0x1e0-0x200(%rax),$xt3	# &quot;xc3&quot;
+	vmovdqa		0x200-0x200(%rax),$xd0
+	vmovdqa		0x220-0x200(%rax),$xd1
+	vmovdqa		0x240-0x200(%rax),$xd2
+	vmovdqa		0x260-0x200(%rax),$xd3
+	vpaddd		.Leight(%rip),$xd0,$xd0	# next SIMD counters
+
+.Loop_enter8x:
+	vmovdqa		$xt2,0x40(%rsp)		# SIMD equivalent of &quot;@x[10]&quot;
+	vmovdqa		$xt3,0x60(%rsp)		# SIMD equivalent of &quot;@x[11]&quot;
+	vbroadcasti128	(%r10),$xt3
+	vmovdqa		$xd0,0x200-0x200(%rax)	# save SIMD counters
+	mov		\$10,%eax
+	jmp		.Loop8x
+
+.align	32
+.Loop8x:
+___
+	foreach (&amp;AVX2_lane_ROUND(0, 4, 8,12)) { eval; }
+	foreach (&amp;AVX2_lane_ROUND(0, 5,10,15)) { eval; }
+$code.=&lt;&lt;___;
+	dec		%eax
+	jnz		.Loop8x
+
+	lea		0x200(%rsp),%rax	# size optimization
+	vpaddd		0x80-0x100(%rcx),$xa0,$xa0	# accumulate key
+	vpaddd		0xa0-0x100(%rcx),$xa1,$xa1
+	vpaddd		0xc0-0x100(%rcx),$xa2,$xa2
+	vpaddd		0xe0-0x100(%rcx),$xa3,$xa3
+
+	vpunpckldq	$xa1,$xa0,$xt2		# &quot;de-interlace&quot; data
+	vpunpckldq	$xa3,$xa2,$xt3
+	vpunpckhdq	$xa1,$xa0,$xa0
+	vpunpckhdq	$xa3,$xa2,$xa2
+	vpunpcklqdq	$xt3,$xt2,$xa1		# &quot;a0&quot;
+	vpunpckhqdq	$xt3,$xt2,$xt2		# &quot;a1&quot;
+	vpunpcklqdq	$xa2,$xa0,$xa3		# &quot;a2&quot;
+	vpunpckhqdq	$xa2,$xa0,$xa0		# &quot;a3&quot;
+___
+	($xa0,$xa1,$xa2,$xa3,$xt2)=($xa1,$xt2,$xa3,$xa0,$xa2);
+$code.=&lt;&lt;___;
+	vpaddd		0x100-0x100(%rcx),$xb0,$xb0
+	vpaddd		0x120-0x100(%rcx),$xb1,$xb1
+	vpaddd		0x140-0x100(%rcx),$xb2,$xb2
+	vpaddd		0x160-0x100(%rcx),$xb3,$xb3
+
+	vpunpckldq	$xb1,$xb0,$xt2
+	vpunpckldq	$xb3,$xb2,$xt3
+	vpunpckhdq	$xb1,$xb0,$xb0
+	vpunpckhdq	$xb3,$xb2,$xb2
+	vpunpcklqdq	$xt3,$xt2,$xb1		# &quot;b0&quot;
+	vpunpckhqdq	$xt3,$xt2,$xt2		# &quot;b1&quot;
+	vpunpcklqdq	$xb2,$xb0,$xb3		# &quot;b2&quot;
+	vpunpckhqdq	$xb2,$xb0,$xb0		# &quot;b3&quot;
+___
+	($xb0,$xb1,$xb2,$xb3,$xt2)=($xb1,$xt2,$xb3,$xb0,$xb2);
+$code.=&lt;&lt;___;
+	vperm2i128	\$0x20,$xb0,$xa0,$xt3	# &quot;de-interlace&quot; further
+	vperm2i128	\$0x31,$xb0,$xa0,$xb0
+	vperm2i128	\$0x20,$xb1,$xa1,$xa0
+	vperm2i128	\$0x31,$xb1,$xa1,$xb1
+	vperm2i128	\$0x20,$xb2,$xa2,$xa1
+	vperm2i128	\$0x31,$xb2,$xa2,$xb2
+	vperm2i128	\$0x20,$xb3,$xa3,$xa2
+	vperm2i128	\$0x31,$xb3,$xa3,$xb3
+___
+	($xa0,$xa1,$xa2,$xa3,$xt3)=($xt3,$xa0,$xa1,$xa2,$xa3);
+	my ($xc0,$xc1,$xc2,$xc3)=($xt0,$xt1,$xa0,$xa1);
+$code.=&lt;&lt;___;
+	vmovdqa		$xa0,0x00(%rsp)		# offload $xaN
+	vmovdqa		$xa1,0x20(%rsp)
+	vmovdqa		0x40(%rsp),$xc2		# $xa0
+	vmovdqa		0x60(%rsp),$xc3		# $xa1
+
+	vpaddd		0x180-0x200(%rax),$xc0,$xc0
+	vpaddd		0x1a0-0x200(%rax),$xc1,$xc1
+	vpaddd		0x1c0-0x200(%rax),$xc2,$xc2
+	vpaddd		0x1e0-0x200(%rax),$xc3,$xc3
+
+	vpunpckldq	$xc1,$xc0,$xt2
+	vpunpckldq	$xc3,$xc2,$xt3
+	vpunpckhdq	$xc1,$xc0,$xc0
+	vpunpckhdq	$xc3,$xc2,$xc2
+	vpunpcklqdq	$xt3,$xt2,$xc1		# &quot;c0&quot;
+	vpunpckhqdq	$xt3,$xt2,$xt2		# &quot;c1&quot;
+	vpunpcklqdq	$xc2,$xc0,$xc3		# &quot;c2&quot;
+	vpunpckhqdq	$xc2,$xc0,$xc0		# &quot;c3&quot;
+___
+	($xc0,$xc1,$xc2,$xc3,$xt2)=($xc1,$xt2,$xc3,$xc0,$xc2);
+$code.=&lt;&lt;___;
+	vpaddd		0x200-0x200(%rax),$xd0,$xd0
+	vpaddd		0x220-0x200(%rax),$xd1,$xd1
+	vpaddd		0x240-0x200(%rax),$xd2,$xd2
+	vpaddd		0x260-0x200(%rax),$xd3,$xd3
+
+	vpunpckldq	$xd1,$xd0,$xt2
+	vpunpckldq	$xd3,$xd2,$xt3
+	vpunpckhdq	$xd1,$xd0,$xd0
+	vpunpckhdq	$xd3,$xd2,$xd2
+	vpunpcklqdq	$xt3,$xt2,$xd1		# &quot;d0&quot;
+	vpunpckhqdq	$xt3,$xt2,$xt2		# &quot;d1&quot;
+	vpunpcklqdq	$xd2,$xd0,$xd3		# &quot;d2&quot;
+	vpunpckhqdq	$xd2,$xd0,$xd0		# &quot;d3&quot;
+___
+	($xd0,$xd1,$xd2,$xd3,$xt2)=($xd1,$xt2,$xd3,$xd0,$xd2);
+$code.=&lt;&lt;___;
+	vperm2i128	\$0x20,$xd0,$xc0,$xt3	# &quot;de-interlace&quot; further
+	vperm2i128	\$0x31,$xd0,$xc0,$xd0
+	vperm2i128	\$0x20,$xd1,$xc1,$xc0
+	vperm2i128	\$0x31,$xd1,$xc1,$xd1
+	vperm2i128	\$0x20,$xd2,$xc2,$xc1
+	vperm2i128	\$0x31,$xd2,$xc2,$xd2
+	vperm2i128	\$0x20,$xd3,$xc3,$xc2
+	vperm2i128	\$0x31,$xd3,$xc3,$xd3
+___
+	($xc0,$xc1,$xc2,$xc3,$xt3)=($xt3,$xc0,$xc1,$xc2,$xc3);
+	($xb0,$xb1,$xb2,$xb3,$xc0,$xc1,$xc2,$xc3)=
+	($xc0,$xc1,$xc2,$xc3,$xb0,$xb1,$xb2,$xb3);
+	($xa0,$xa1)=($xt2,$xt3);
+$code.=&lt;&lt;___;
+	vmovdqa		0x00(%rsp),$xa0		# $xaN was offloaded, remember?
+	vmovdqa		0x20(%rsp),$xa1
+
+	cmp		\$64*8,$len
+	jb		.Ltail8x
+
+	vpxor		0x00($inp),$xa0,$xa0	# xor with input
+	vpxor		0x20($inp),$xb0,$xb0
+	vpxor		0x40($inp),$xc0,$xc0
+	vpxor		0x60($inp),$xd0,$xd0
+	lea		0x80($inp),$inp		# size optimization
+	vmovdqu		$xa0,0x00($out)
+	vmovdqu		$xb0,0x20($out)
+	vmovdqu		$xc0,0x40($out)
+	vmovdqu		$xd0,0x60($out)
+	lea		0x80($out),$out		# size optimization
+
+	vpxor		0x00($inp),$xa1,$xa1
+	vpxor		0x20($inp),$xb1,$xb1
+	vpxor		0x40($inp),$xc1,$xc1
+	vpxor		0x60($inp),$xd1,$xd1
+	lea		0x80($inp),$inp		# size optimization
+	vmovdqu		$xa1,0x00($out)
+	vmovdqu		$xb1,0x20($out)
+	vmovdqu		$xc1,0x40($out)
+	vmovdqu		$xd1,0x60($out)
+	lea		0x80($out),$out		# size optimization
+
+	vpxor		0x00($inp),$xa2,$xa2
+	vpxor		0x20($inp),$xb2,$xb2
+	vpxor		0x40($inp),$xc2,$xc2
+	vpxor		0x60($inp),$xd2,$xd2
+	lea		0x80($inp),$inp		# size optimization
+	vmovdqu		$xa2,0x00($out)
+	vmovdqu		$xb2,0x20($out)
+	vmovdqu		$xc2,0x40($out)
+	vmovdqu		$xd2,0x60($out)
+	lea		0x80($out),$out		# size optimization
+
+	vpxor		0x00($inp),$xa3,$xa3
+	vpxor		0x20($inp),$xb3,$xb3
+	vpxor		0x40($inp),$xc3,$xc3
+	vpxor		0x60($inp),$xd3,$xd3
+	lea		0x80($inp),$inp		# size optimization
+	vmovdqu		$xa3,0x00($out)
+	vmovdqu		$xb3,0x20($out)
+	vmovdqu		$xc3,0x40($out)
+	vmovdqu		$xd3,0x60($out)
+	lea		0x80($out),$out		# size optimization
+
+	sub		\$64*8,$len
+	jnz		.Loop_outer8x
+
+	jmp		.Ldone8x
+
+.Ltail8x:
+	cmp		\$448,$len
+	jae		.L448_or_more8x
+	cmp		\$384,$len
+	jae		.L384_or_more8x
+	cmp		\$320,$len
+	jae		.L320_or_more8x
+	cmp		\$256,$len
+	jae		.L256_or_more8x
+	cmp		\$192,$len
+	jae		.L192_or_more8x
+	cmp		\$128,$len
+	jae		.L128_or_more8x
+	cmp		\$64,$len
+	jae		.L64_or_more8x
+
+	xor		%r10,%r10
+	vmovdqa		$xa0,0x00(%rsp)
+	vmovdqa		$xb0,0x20(%rsp)
+	jmp		.Loop_tail8x
+
+.align	32
+.L64_or_more8x:
+	vpxor		0x00($inp),$xa0,$xa0	# xor with input
+	vpxor		0x20($inp),$xb0,$xb0
+	vmovdqu		$xa0,0x00($out)
+	vmovdqu		$xb0,0x20($out)
+	je		.Ldone8x
+
+	lea		0x40($inp),$inp		# inp+=64*1
+	xor		%r10,%r10
+	vmovdqa		$xc0,0x00(%rsp)
+	lea		0x40($out),$out		# out+=64*1
+	sub		\$64,$len		# len-=64*1
+	vmovdqa		$xd0,0x20(%rsp)
+	jmp		.Loop_tail8x
+
+.align	32
+.L128_or_more8x:
+	vpxor		0x00($inp),$xa0,$xa0	# xor with input
+	vpxor		0x20($inp),$xb0,$xb0
+	vpxor		0x40($inp),$xc0,$xc0
+	vpxor		0x60($inp),$xd0,$xd0
+	vmovdqu		$xa0,0x00($out)
+	vmovdqu		$xb0,0x20($out)
+	vmovdqu		$xc0,0x40($out)
+	vmovdqu		$xd0,0x60($out)
+	je		.Ldone8x
+
+	lea		0x80($inp),$inp		# inp+=64*2
+	xor		%r10,%r10
+	vmovdqa		$xa1,0x00(%rsp)
+	lea		0x80($out),$out		# out+=64*2
+	sub		\$128,$len		# len-=64*2
+	vmovdqa		$xb1,0x20(%rsp)
+	jmp		.Loop_tail8x
+
+.align	32
+.L192_or_more8x:
+	vpxor		0x00($inp),$xa0,$xa0	# xor with input
+	vpxor		0x20($inp),$xb0,$xb0
+	vpxor		0x40($inp),$xc0,$xc0
+	vpxor		0x60($inp),$xd0,$xd0
+	vpxor		0x80($inp),$xa1,$xa1
+	vpxor		0xa0($inp),$xb1,$xb1
+	vmovdqu		$xa0,0x00($out)
+	vmovdqu		$xb0,0x20($out)
+	vmovdqu		$xc0,0x40($out)
+	vmovdqu		$xd0,0x60($out)
+	vmovdqu		$xa1,0x80($out)
+	vmovdqu		$xb1,0xa0($out)
+	je		.Ldone8x
+
+	lea		0xc0($inp),$inp		# inp+=64*3
+	xor		%r10,%r10
+	vmovdqa		$xc1,0x00(%rsp)
+	lea		0xc0($out),$out		# out+=64*3
+	sub		\$192,$len		# len-=64*3
+	vmovdqa		$xd1,0x20(%rsp)
+	jmp		.Loop_tail8x
+
+.align	32
+.L256_or_more8x:
+	vpxor		0x00($inp),$xa0,$xa0	# xor with input
+	vpxor		0x20($inp),$xb0,$xb0
+	vpxor		0x40($inp),$xc0,$xc0
+	vpxor		0x60($inp),$xd0,$xd0
+	vpxor		0x80($inp),$xa1,$xa1
+	vpxor		0xa0($inp),$xb1,$xb1
+	vpxor		0xc0($inp),$xc1,$xc1
+	vpxor		0xe0($inp),$xd1,$xd1
+	vmovdqu		$xa0,0x00($out)
+	vmovdqu		$xb0,0x20($out)
+	vmovdqu		$xc0,0x40($out)
+	vmovdqu		$xd0,0x60($out)
+	vmovdqu		$xa1,0x80($out)
+	vmovdqu		$xb1,0xa0($out)
+	vmovdqu		$xc1,0xc0($out)
+	vmovdqu		$xd1,0xe0($out)
+	je		.Ldone8x
+
+	lea		0x100($inp),$inp	# inp+=64*4
+	xor		%r10,%r10
+	vmovdqa		$xa2,0x00(%rsp)
+	lea		0x100($out),$out	# out+=64*4
+	sub		\$256,$len		# len-=64*4
+	vmovdqa		$xb2,0x20(%rsp)
+	jmp		.Loop_tail8x
+
+.align	32
+.L320_or_more8x:
+	vpxor		0x00($inp),$xa0,$xa0	# xor with input
+	vpxor		0x20($inp),$xb0,$xb0
+	vpxor		0x40($inp),$xc0,$xc0
+	vpxor		0x60($inp),$xd0,$xd0
+	vpxor		0x80($inp),$xa1,$xa1
+	vpxor		0xa0($inp),$xb1,$xb1
+	vpxor		0xc0($inp),$xc1,$xc1
+	vpxor		0xe0($inp),$xd1,$xd1
+	vpxor		0x100($inp),$xa2,$xa2
+	vpxor		0x120($inp),$xb2,$xb2
+	vmovdqu		$xa0,0x00($out)
+	vmovdqu		$xb0,0x20($out)
+	vmovdqu		$xc0,0x40($out)
+	vmovdqu		$xd0,0x60($out)
+	vmovdqu		$xa1,0x80($out)
+	vmovdqu		$xb1,0xa0($out)
+	vmovdqu		$xc1,0xc0($out)
+	vmovdqu		$xd1,0xe0($out)
+	vmovdqu		$xa2,0x100($out)
+	vmovdqu		$xb2,0x120($out)
+	je		.Ldone8x
+
+	lea		0x140($inp),$inp	# inp+=64*5
+	xor		%r10,%r10
+	vmovdqa		$xc2,0x00(%rsp)
+	lea		0x140($out),$out	# out+=64*5
+	sub		\$320,$len		# len-=64*5
+	vmovdqa		$xd2,0x20(%rsp)
+	jmp		.Loop_tail8x
+
+.align	32
+.L384_or_more8x:
+	vpxor		0x00($inp),$xa0,$xa0	# xor with input
+	vpxor		0x20($inp),$xb0,$xb0
+	vpxor		0x40($inp),$xc0,$xc0
+	vpxor		0x60($inp),$xd0,$xd0
+	vpxor		0x80($inp),$xa1,$xa1
+	vpxor		0xa0($inp),$xb1,$xb1
+	vpxor		0xc0($inp),$xc1,$xc1
+	vpxor		0xe0($inp),$xd1,$xd1
+	vpxor		0x100($inp),$xa2,$xa2
+	vpxor		0x120($inp),$xb2,$xb2
+	vpxor		0x140($inp),$xc2,$xc2
+	vpxor		0x160($inp),$xd2,$xd2
+	vmovdqu		$xa0,0x00($out)
+	vmovdqu		$xb0,0x20($out)
+	vmovdqu		$xc0,0x40($out)
+	vmovdqu		$xd0,0x60($out)
+	vmovdqu		$xa1,0x80($out)
+	vmovdqu		$xb1,0xa0($out)
+	vmovdqu		$xc1,0xc0($out)
+	vmovdqu		$xd1,0xe0($out)
+	vmovdqu		$xa2,0x100($out)
+	vmovdqu		$xb2,0x120($out)
+	vmovdqu		$xc2,0x140($out)
+	vmovdqu		$xd2,0x160($out)
+	je		.Ldone8x
+
+	lea		0x180($inp),$inp	# inp+=64*6
+	xor		%r10,%r10
+	vmovdqa		$xa3,0x00(%rsp)
+	lea		0x180($out),$out	# out+=64*6
+	sub		\$384,$len		# len-=64*6
+	vmovdqa		$xb3,0x20(%rsp)
+	jmp		.Loop_tail8x
+
+.align	32
+.L448_or_more8x:
+	vpxor		0x00($inp),$xa0,$xa0	# xor with input
+	vpxor		0x20($inp),$xb0,$xb0
+	vpxor		0x40($inp),$xc0,$xc0
+	vpxor		0x60($inp),$xd0,$xd0
+	vpxor		0x80($inp),$xa1,$xa1
+	vpxor		0xa0($inp),$xb1,$xb1
+	vpxor		0xc0($inp),$xc1,$xc1
+	vpxor		0xe0($inp),$xd1,$xd1
+	vpxor		0x100($inp),$xa2,$xa2
+	vpxor		0x120($inp),$xb2,$xb2
+	vpxor		0x140($inp),$xc2,$xc2
+	vpxor		0x160($inp),$xd2,$xd2
+	vpxor		0x180($inp),$xa3,$xa3
+	vpxor		0x1a0($inp),$xb3,$xb3
+	vmovdqu		$xa0,0x00($out)
+	vmovdqu		$xb0,0x20($out)
+	vmovdqu		$xc0,0x40($out)
+	vmovdqu		$xd0,0x60($out)
+	vmovdqu		$xa1,0x80($out)
+	vmovdqu		$xb1,0xa0($out)
+	vmovdqu		$xc1,0xc0($out)
+	vmovdqu		$xd1,0xe0($out)
+	vmovdqu		$xa2,0x100($out)
+	vmovdqu		$xb2,0x120($out)
+	vmovdqu		$xc2,0x140($out)
+	vmovdqu		$xd2,0x160($out)
+	vmovdqu		$xa3,0x180($out)
+	vmovdqu		$xb3,0x1a0($out)
+	je		.Ldone8x
+
+	lea		0x1c0($inp),$inp	# inp+=64*7
+	xor		%r10,%r10
+	vmovdqa		$xc3,0x00(%rsp)
+	lea		0x1c0($out),$out	# out+=64*7
+	sub		\$448,$len		# len-=64*7
+	vmovdqa		$xd3,0x20(%rsp)
+
+.Loop_tail8x:
+	movzb		($inp,%r10),%eax
+	movzb		(%rsp,%r10),%ecx
+	lea		1(%r10),%r10
+	xor		%ecx,%eax
+	mov		%al,-1($out,%r10)
+	dec		$len
+	jnz		.Loop_tail8x
+
+.Ldone8x:
+	vzeroall
+___
+$code.=&lt;&lt;___	if ($win64);
+	lea		0x290+0x30(%rsp),%r11
+	movaps		-0x30(%r11),%xmm6
+	movaps		-0x20(%r11),%xmm7
+	movaps		-0x10(%r11),%xmm8
+	movaps		0x00(%r11),%xmm9
+	movaps		0x10(%r11),%xmm10
+	movaps		0x20(%r11),%xmm11
+	movaps		0x30(%r11),%xmm12
+	movaps		0x40(%r11),%xmm13
+	movaps		0x50(%r11),%xmm14
+	movaps		0x60(%r11),%xmm15
+___
+$code.=&lt;&lt;___;
+	mov		0x280(%rsp),%rsp
+	ret
+.size	ChaCha20_8x,.-ChaCha20_8x
+___
+}
+
+foreach (split(&quot;\n&quot;,$code)) {
+	s/\`([^\`]*)\`/eval $1/geo;
+
+	s/%x#%y/%x/go;
+
+	print $_,&quot;\n&quot;;
+}
+
+close STDOUT;
diff --git a/crypto/perlasm/x86gas.pl b/crypto/perlasm/x86gas.pl
index 63b2301..9ee6fa3 100644
--- a/crypto/perlasm/x86gas.pl
+++ b/crypto/perlasm/x86gas.pl
@@ -17,7 +17,7 @@ sub opsize()
 { my $reg=shift;
     if    ($reg =~ m/^%e/o)		{ &quot;l&quot;; }
     elsif ($reg =~ m/^%[a-d][hl]$/o)	{ &quot;b&quot;; }
-    elsif ($reg =~ m/^%[xm]/o)		{ undef; }
+    elsif ($reg =~ m/^%[yxm]/o)		{ undef; }
     else				{ &quot;w&quot;; }
 }
 
diff --git a/crypto/poly1305/Makefile.in b/crypto/poly1305/Makefile.in
index 9d74865..c848843 100644
--- a/crypto/poly1305/Makefile.in
+++ b/crypto/poly1305/Makefile.in
@@ -38,6 +38,10 @@ lib:	$(LIBOBJ)
 
 poly1305-sparcv9.S:	asm/poly1305-sparcv9.pl
 	$(PERL) asm/poly1305-sparcv9.pl &gt; $@
+poly1305-x86.s:		asm/poly1305-x86.pl
+	$(PERL) asm/poly1305-x86.pl $(PERLASM_SCHEME) $(CFLAGS) $(PROCESSOR) &gt; $@
+poly1305-x86_64.s:	asm/poly1305-x86_64.pl
+	$(PERL) asm/poly1305-x86_64.pl $(PERLASM_SCHEME) &gt; $@
 
 poly1305-%.S:	asm/poly1305-%.pl;	$(PERL) $&lt; $(PERLASM_SCHEME) $@
 
diff --git a/crypto/poly1305/asm/poly1305-x86.pl b/crypto/poly1305/asm/poly1305-x86.pl
new file mode 100755
index 0000000..7c1aee5
--- /dev/null
+++ b/crypto/poly1305/asm/poly1305-x86.pl
@@ -0,0 +1,1794 @@
+#!/usr/bin/env perl
+#
+# ====================================================================
+# Written by Andy Polyakov &lt;<A HREF="../../../mailman/listinfo/openssl-commits.html">appro at openssl.org</A>&gt; for the OpenSSL
+# project. The module is, however, dual licensed under OpenSSL and
+# CRYPTOGAMS licenses depending on where you obtain it. For further
+# details see <A HREF="http://www.openssl.org/~appro/cryptogams/.">http://www.openssl.org/~appro/cryptogams/.</A>
+# ====================================================================
+#
+# This module implements Poly1305 hash for x86.
+#
+# April 2015
+#
+# Numbers are cycles per processed byte with poly1305_blocks alone,
+# measured with rdtsc at fixed clock frequency.
+#
+#		IALU/gcc-3.4(*)	SSE2(**)	AVX2
+# Pentium	15.7/+80%	-
+# PIII		6.21/+90%	-
+# P4		19.8/+40%	3.24
+# Core 2	4.85/+90%	1.80
+# Westmere	4.58/+100%	1.43
+# Sandy Bridge	3.90/+100%	1.36
+# Haswell	3.88/+70%	1.18		0.72
+# Silvermont	11.0/+40%	4.80
+# VIA Nano	6.71/+90%	2.47
+# Sledgehammer	3.51/+180%	4.27
+# Bulldozer	4.53/+140%	1.31
+#
+# (*)	gcc 4.8 for some reason generated worse code;
+# (**)	besides SSE2 there are floating-point and AVX options; FP
+#	is deemed unnecessary, because pre-SSE2 processor are too
+#	old to care about, while it's not the fastest option on
+#	SSE2-capable ones; AVX is omitted, because it doesn't give
+#	a lot of improvement, 5-10% depending on processor;
+
+$0 =~ m/(.*[\/\\])[^\/\\]+$/; $dir=$1;
+push(@INC,&quot;${dir}&quot;,&quot;${dir}../../perlasm&quot;);
+require &quot;x86asm.pl&quot;;
+
+&amp;asm_init($ARGV[0],&quot;poly1305-x86.pl&quot;,$ARGV[$#ARGV] eq &quot;386&quot;);
+
+$sse2=$avx=0;
+for (@ARGV) { $sse2=1 if (/-DOPENSSL_IA32_SSE2/); }
+
+if ($sse2) {
+	&amp;static_label(&quot;const_sse2&quot;);
+	&amp;static_label(&quot;enter_blocks&quot;);
+	&amp;static_label(&quot;enter_emit&quot;);
+	&amp;external_label(&quot;OPENSSL_ia32cap_P&quot;);
+
+	if (`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2&gt;&amp;1`
+			=~ /GNU assembler version ([2-9]\.[0-9]+)/) {
+		$avx = ($1&gt;=2.19) + ($1&gt;=2.22);
+	}
+
+	if (!$avx &amp;&amp; $ARGV[0] eq &quot;win32n&quot; &amp;&amp;
+	   `nasm -v 2&gt;&amp;1` =~ /NASM version ([2-9]\.[0-9]+)/) {
+	$avx = ($1&gt;=2.09) + ($1&gt;=2.10);
+	}
+
+	if (!$avx &amp;&amp; `$ENV{CC} -v 2&gt;&amp;1` =~ /(^clang version|based on LLVM) ([3-9]\.[0-9]+)/) {
+		$avx = ($2&gt;=3.0) + ($2&gt;3.0);
+	}
+}
+
+########################################################################
+# Layout of opaque area is following.
+#
+#	unsigned __int32 h[5];		# current hash value base 2^32
+#	unsigned __int32 pad;		# is_base2_26 in vector context
+#	unsigned __int32 r[4];		# key value base 2^32
+
+&amp;align(64);
+&amp;function_begin(&quot;poly1305_init&quot;);
+	&amp;mov	(&quot;edi&quot;,&amp;wparam(0));		# context
+	&amp;mov	(&quot;esi&quot;,&amp;wparam(1));		# key
+	&amp;mov	(&quot;ebp&quot;,&amp;wparam(2));		# function table
+
+	&amp;xor	(&quot;eax&quot;,&quot;eax&quot;);
+	&amp;mov	(&amp;DWP(4*0,&quot;edi&quot;),&quot;eax&quot;);	# zero hash value
+	&amp;mov	(&amp;DWP(4*1,&quot;edi&quot;),&quot;eax&quot;);
+	&amp;mov	(&amp;DWP(4*2,&quot;edi&quot;),&quot;eax&quot;);
+	&amp;mov	(&amp;DWP(4*3,&quot;edi&quot;),&quot;eax&quot;);
+	&amp;mov	(&amp;DWP(4*4,&quot;edi&quot;),&quot;eax&quot;);
+	&amp;mov	(&amp;DWP(4*5,&quot;edi&quot;),&quot;eax&quot;);	# is_base2_26
+
+	&amp;cmp	(&quot;esi&quot;,0);
+	&amp;je	(&amp;label(&quot;nokey&quot;));
+
+    if ($sse2) {
+	&amp;call	(&amp;label(&quot;pic_point&quot;));
+    &amp;set_label(&quot;pic_point&quot;);
+	&amp;blindpop(&quot;ebx&quot;);
+
+	&amp;lea	(&quot;eax&quot;,&amp;DWP(&quot;poly1305_blocks-&quot;.&amp;label(&quot;pic_point&quot;),&quot;ebx&quot;));
+	&amp;lea	(&quot;edx&quot;,&amp;DWP(&quot;poly1305_emit-&quot;.&amp;label(&quot;pic_point&quot;),&quot;ebx&quot;));
+
+	&amp;picmeup(&quot;edi&quot;,&quot;OPENSSL_ia32cap_P&quot;,&quot;ebx&quot;,&amp;label(&quot;pic_point&quot;));
+	&amp;mov	(&quot;ecx&quot;,&amp;DWP(0,&quot;edi&quot;));
+	&amp;and	(&quot;ecx&quot;,1&lt;&lt;26|1&lt;&lt;24);
+	&amp;cmp	(&quot;ecx&quot;,1&lt;&lt;26|1&lt;&lt;24);		# SSE2 and XMM?
+	&amp;jne	(&amp;label(&quot;no_sse2&quot;));
+
+	&amp;lea	(&quot;eax&quot;,&amp;DWP(&quot;_poly1305_blocks_sse2-&quot;.&amp;label(&quot;pic_point&quot;),&quot;ebx&quot;));
+	&amp;lea	(&quot;edx&quot;,&amp;DWP(&quot;_poly1305_emit_sse2-&quot;.&amp;label(&quot;pic_point&quot;),&quot;ebx&quot;));
+
+      if ($avx&gt;1) {
+	&amp;mov	(&quot;ecx&quot;,&amp;DWP(8,&quot;edi&quot;));
+	&amp;test	(&quot;ecx&quot;,1&lt;&lt;5);			# AVX2?
+	&amp;jz	(&amp;label(&quot;no_sse2&quot;));
+
+	&amp;lea	(&quot;eax&quot;,&amp;DWP(&quot;_poly1305_blocks_avx2-&quot;.&amp;label(&quot;pic_point&quot;),&quot;ebx&quot;));
+      }
+    &amp;set_label(&quot;no_sse2&quot;);
+	&amp;mov	(&quot;edi&quot;,&amp;wparam(0));		# reload context
+	&amp;mov	(&amp;DWP(0,&quot;ebp&quot;),&quot;eax&quot;);		# fill function table
+	&amp;mov	(&amp;DWP(4,&quot;ebp&quot;),&quot;edx&quot;);
+    }
+
+	&amp;mov	(&quot;eax&quot;,&amp;DWP(4*0,&quot;esi&quot;));	# load input key
+	&amp;mov	(&quot;ebx&quot;,&amp;DWP(4*1,&quot;esi&quot;));
+	&amp;mov	(&quot;ecx&quot;,&amp;DWP(4*2,&quot;esi&quot;));
+	&amp;mov	(&quot;edx&quot;,&amp;DWP(4*3,&quot;esi&quot;));
+	&amp;and	(&quot;eax&quot;,0x0fffffff);
+	&amp;and	(&quot;ebx&quot;,0x0ffffffc);
+	&amp;and	(&quot;ecx&quot;,0x0ffffffc);
+	&amp;and	(&quot;edx&quot;,0x0ffffffc);
+	&amp;mov	(&amp;DWP(4*6,&quot;edi&quot;),&quot;eax&quot;);
+	&amp;mov	(&amp;DWP(4*7,&quot;edi&quot;),&quot;ebx&quot;);
+	&amp;mov	(&amp;DWP(4*8,&quot;edi&quot;),&quot;ecx&quot;);
+	&amp;mov	(&amp;DWP(4*9,&quot;edi&quot;),&quot;edx&quot;);
+
+	&amp;mov	(&quot;eax&quot;,$sse2);
+&amp;set_label(&quot;nokey&quot;);
+&amp;function_end(&quot;poly1305_init&quot;);
+
+($h0,$h1,$h2,$h3,$h4,
+ $d0,$d1,$d2,$d3,
+ $r0,$r1,$r2,$r3,
+     $s1,$s2,$s3)=map(4*$_,(0..15));
+
+&amp;function_begin(&quot;poly1305_blocks&quot;);
+	&amp;mov	(&quot;edi&quot;,&amp;wparam(0));		# ctx
+	&amp;mov	(&quot;esi&quot;,&amp;wparam(1));		# inp
+	&amp;mov	(&quot;ecx&quot;,&amp;wparam(2));		# len
+&amp;set_label(&quot;enter_blocks&quot;);
+	&amp;and	(&quot;ecx&quot;,-15);
+	&amp;jz	(&amp;label(&quot;nodata&quot;));
+
+	&amp;stack_push(16);
+	&amp;mov	(&quot;eax&quot;,&amp;DWP(4*6,&quot;edi&quot;));	# r0
+	&amp;mov	(&quot;ebx&quot;,&amp;DWP(4*7,&quot;edi&quot;));	# r1
+	 &amp;lea	(&quot;ebp&quot;,&amp;DWP(0,&quot;esi&quot;,&quot;ecx&quot;));	# end of input
+	&amp;mov	(&quot;ecx&quot;,&amp;DWP(4*8,&quot;edi&quot;));	# r2
+	&amp;mov	(&quot;edx&quot;,&amp;DWP(4*9,&quot;edi&quot;));	# r3
+
+	&amp;mov	(&amp;wparam(2),&quot;ebp&quot;);
+	&amp;mov	(&quot;ebp&quot;,&quot;esi&quot;);
+
+	&amp;mov	(&amp;DWP($r0,&quot;esp&quot;),&quot;eax&quot;);	# r0
+	&amp;mov	(&quot;eax&quot;,&quot;ebx&quot;);
+	&amp;shr	(&quot;eax&quot;,2);
+	&amp;mov	(&amp;DWP($r1,&quot;esp&quot;),&quot;ebx&quot;);	# r1
+	&amp;add	(&quot;eax&quot;,&quot;ebx&quot;);			# s1
+	&amp;mov	(&quot;ebx&quot;,&quot;ecx&quot;);
+	&amp;shr	(&quot;ebx&quot;,2);
+	&amp;mov	(&amp;DWP($r2,&quot;esp&quot;),&quot;ecx&quot;);	# r2
+	&amp;add	(&quot;ebx&quot;,&quot;ecx&quot;);			# s2
+	&amp;mov	(&quot;ecx&quot;,&quot;edx&quot;);
+	&amp;shr	(&quot;ecx&quot;,2);
+	&amp;mov	(&amp;DWP($r3,&quot;esp&quot;),&quot;edx&quot;);	# r3
+	&amp;add	(&quot;ecx&quot;,&quot;edx&quot;);			# s3
+	&amp;mov	(&amp;DWP($s1,&quot;esp&quot;),&quot;eax&quot;);	# s1
+	&amp;mov	(&amp;DWP($s2,&quot;esp&quot;),&quot;ebx&quot;);	# s2
+	&amp;mov	(&amp;DWP($s3,&quot;esp&quot;),&quot;ecx&quot;);	# s3
+
+	&amp;mov	(&quot;eax&quot;,&amp;DWP(4*0,&quot;edi&quot;));	# load hash value
+	&amp;mov	(&quot;ebx&quot;,&amp;DWP(4*1,&quot;edi&quot;));
+	&amp;mov	(&quot;ecx&quot;,&amp;DWP(4*2,&quot;edi&quot;));
+	&amp;mov	(&quot;esi&quot;,&amp;DWP(4*3,&quot;edi&quot;));
+	&amp;mov	(&quot;edi&quot;,&amp;DWP(4*4,&quot;edi&quot;));
+	&amp;jmp	(&amp;label(&quot;loop&quot;));
+
+&amp;set_label(&quot;loop&quot;,32);
+	&amp;add	(&quot;eax&quot;,&amp;DWP(4*0,&quot;ebp&quot;));	# accumulate input
+	&amp;adc	(&quot;ebx&quot;,&amp;DWP(4*1,&quot;ebp&quot;));
+	&amp;adc	(&quot;ecx&quot;,&amp;DWP(4*2,&quot;ebp&quot;));
+	&amp;adc	(&quot;esi&quot;,&amp;DWP(4*3,&quot;ebp&quot;));
+	&amp;lea	(&quot;ebp&quot;,&amp;DWP(4*4,&quot;ebp&quot;));
+	&amp;adc	(&quot;edi&quot;,&amp;wparam(3));		# padbit
+
+	&amp;mov	(&amp;DWP($h0,&quot;esp&quot;),&quot;eax&quot;);	# put aside hash[+inp]
+	&amp;mov	(&amp;DWP($h3,&quot;esp&quot;),&quot;esi&quot;);
+
+	&amp;mul	(&amp;DWP($r0,&quot;esp&quot;));		# h0*r0
+	 &amp;mov	(&amp;DWP($h4,&quot;esp&quot;),&quot;edi&quot;);
+	&amp;mov	(&quot;edi&quot;,&quot;eax&quot;);
+	&amp;mov	(&quot;eax&quot;,&quot;ebx&quot;);			# h1
+	&amp;mov	(&quot;esi&quot;,&quot;edx&quot;);
+	&amp;mul	(&amp;DWP($s3,&quot;esp&quot;));		# h1*s3
+	&amp;add	(&quot;edi&quot;,&quot;eax&quot;);
+	&amp;mov	(&quot;eax&quot;,&quot;ecx&quot;);			# h2
+	&amp;adc	(&quot;esi&quot;,&quot;edx&quot;);
+	&amp;mul	(&amp;DWP($s2,&quot;esp&quot;));		# h2*s2
+	&amp;add	(&quot;edi&quot;,&quot;eax&quot;);
+	&amp;mov	(&quot;eax&quot;,&amp;DWP($h3,&quot;esp&quot;));
+	&amp;adc	(&quot;esi&quot;,&quot;edx&quot;);
+	&amp;mul	(&amp;DWP($s1,&quot;esp&quot;));		# h3*s1
+	&amp;add	(&quot;edi&quot;,&quot;eax&quot;);
+	 &amp;mov	(&quot;eax&quot;,&amp;DWP($h0,&quot;esp&quot;));
+	&amp;adc	(&quot;esi&quot;,&quot;edx&quot;);
+
+	&amp;mul	(&amp;DWP($r1,&quot;esp&quot;));		# h0*r1
+	 &amp;mov	(&amp;DWP($d0,&quot;esp&quot;),&quot;edi&quot;);
+	&amp;xor	(&quot;edi&quot;,&quot;edi&quot;);
+	&amp;add	(&quot;esi&quot;,&quot;eax&quot;);
+	&amp;mov	(&quot;eax&quot;,&quot;ebx&quot;);			# h1
+	&amp;adc	(&quot;edi&quot;,&quot;edx&quot;);
+	&amp;mul	(&amp;DWP($r0,&quot;esp&quot;));		# h1*r0
+	&amp;add	(&quot;esi&quot;,&quot;eax&quot;);
+	&amp;mov	(&quot;eax&quot;,&quot;ecx&quot;);			# h2
+	&amp;adc	(&quot;edi&quot;,&quot;edx&quot;);
+	&amp;mul	(&amp;DWP($s3,&quot;esp&quot;));		# h2*s3
+	&amp;add	(&quot;esi&quot;,&quot;eax&quot;);
+	&amp;mov	(&quot;eax&quot;,&amp;DWP($h3,&quot;esp&quot;));
+	&amp;adc	(&quot;edi&quot;,&quot;edx&quot;);
+	&amp;mul	(&amp;DWP($s2,&quot;esp&quot;));		# h3*s2
+	&amp;add	(&quot;esi&quot;,&quot;eax&quot;);
+	&amp;mov	(&quot;eax&quot;,&amp;DWP($h4,&quot;esp&quot;));
+	&amp;adc	(&quot;edi&quot;,&quot;edx&quot;);
+	&amp;imul	(&quot;eax&quot;,&amp;DWP($s1,&quot;esp&quot;));	# h4*s1
+	&amp;add	(&quot;esi&quot;,&quot;eax&quot;);
+	 &amp;mov	(&quot;eax&quot;,&amp;DWP($h0,&quot;esp&quot;));
+	&amp;adc	(&quot;edi&quot;,0);
+
+	&amp;mul	(&amp;DWP($r2,&quot;esp&quot;));		# h0*r2
+	 &amp;mov	(&amp;DWP($d1,&quot;esp&quot;),&quot;esi&quot;);
+	&amp;xor	(&quot;esi&quot;,&quot;esi&quot;);
+	&amp;add	(&quot;edi&quot;,&quot;eax&quot;);
+	&amp;mov	(&quot;eax&quot;,&quot;ebx&quot;);			# h1
+	&amp;adc	(&quot;esi&quot;,&quot;edx&quot;);
+	&amp;mul	(&amp;DWP($r1,&quot;esp&quot;));		# h1*r1
+	&amp;add	(&quot;edi&quot;,&quot;eax&quot;);
+	&amp;mov	(&quot;eax&quot;,&quot;ecx&quot;);			# h2
+	&amp;adc	(&quot;esi&quot;,&quot;edx&quot;);
+	&amp;mul	(&amp;DWP($r0,&quot;esp&quot;));		# h2*r0
+	&amp;add	(&quot;edi&quot;,&quot;eax&quot;);
+	&amp;mov	(&quot;eax&quot;,&amp;DWP($h3,&quot;esp&quot;));
+	&amp;adc	(&quot;esi&quot;,&quot;edx&quot;);
+	&amp;mul	(&amp;DWP($s3,&quot;esp&quot;));		# h3*s3
+	&amp;add	(&quot;edi&quot;,&quot;eax&quot;);
+	&amp;mov	(&quot;eax&quot;,&amp;DWP($h4,&quot;esp&quot;));
+	&amp;adc	(&quot;esi&quot;,&quot;edx&quot;);
+	&amp;imul	(&quot;eax&quot;,&amp;DWP($s2,&quot;esp&quot;));	# h4*s2
+	&amp;add	(&quot;edi&quot;,&quot;eax&quot;);
+	 &amp;mov	(&quot;eax&quot;,&amp;DWP($h0,&quot;esp&quot;));
+	&amp;adc	(&quot;esi&quot;,0);
+
+	&amp;mul	(&amp;DWP($r3,&quot;esp&quot;));		# h0*r3
+	 &amp;mov	(&amp;DWP($d2,&quot;esp&quot;),&quot;edi&quot;);
+	&amp;xor	(&quot;edi&quot;,&quot;edi&quot;);
+	&amp;add	(&quot;esi&quot;,&quot;eax&quot;);
+	&amp;mov	(&quot;eax&quot;,&quot;ebx&quot;);			# h1
+	&amp;adc	(&quot;edi&quot;,&quot;edx&quot;);
+	&amp;mul	(&amp;DWP($r2,&quot;esp&quot;));		# h1*r2
+	&amp;add	(&quot;esi&quot;,&quot;eax&quot;);
+	&amp;mov	(&quot;eax&quot;,&quot;ecx&quot;);			# h2
+	&amp;adc	(&quot;edi&quot;,&quot;edx&quot;);
+	&amp;mul	(&amp;DWP($r1,&quot;esp&quot;));		# h2*r1
+	&amp;add	(&quot;esi&quot;,&quot;eax&quot;);
+	&amp;mov	(&quot;eax&quot;,&amp;DWP($h3,&quot;esp&quot;));
+	&amp;adc	(&quot;edi&quot;,&quot;edx&quot;);
+	&amp;mul	(&amp;DWP($r0,&quot;esp&quot;));		# h3*r0
+	&amp;add	(&quot;esi&quot;,&quot;eax&quot;);
+	 &amp;mov	(&quot;ecx&quot;,&amp;DWP($h4,&quot;esp&quot;));
+	&amp;adc	(&quot;edi&quot;,&quot;edx&quot;);
+
+	&amp;mov	(&quot;edx&quot;,&quot;ecx&quot;);
+	&amp;imul	(&quot;ecx&quot;,&amp;DWP($s3,&quot;esp&quot;));	# h4*s3
+	&amp;add	(&quot;esi&quot;,&quot;ecx&quot;);
+	 &amp;mov	(&quot;eax&quot;,&amp;DWP($d0,&quot;esp&quot;));
+	&amp;adc	(&quot;edi&quot;,0);
+
+	&amp;imul	(&quot;edx&quot;,&amp;DWP($r0,&quot;esp&quot;));	# h4*r0
+	&amp;add	(&quot;edx&quot;,&quot;edi&quot;);
+
+	&amp;mov	(&quot;ebx&quot;,&amp;DWP($d1,&quot;esp&quot;));
+	&amp;mov	(&quot;ecx&quot;,&amp;DWP($d2,&quot;esp&quot;));
+
+	&amp;mov	(&quot;edi&quot;,&quot;edx&quot;);			# last reduction step
+	&amp;shr	(&quot;edx&quot;,2);
+	&amp;and	(&quot;edi&quot;,3);
+	&amp;lea	(&quot;edx&quot;,&amp;DWP(0,&quot;edx&quot;,&quot;edx&quot;,4));	# *5
+	&amp;add	(&quot;eax&quot;,&quot;edx&quot;);
+	&amp;adc	(&quot;ebx&quot;,0);
+	&amp;adc	(&quot;ecx&quot;,0);
+	&amp;adc	(&quot;esi&quot;,0);
+
+	&amp;cmp	(&quot;ebp&quot;,&amp;wparam(2));		# done yet?
+	&amp;jne	(&amp;label(&quot;loop&quot;));
+
+	&amp;mov	(&quot;edx&quot;,&amp;wparam(0));		# ctx
+	&amp;stack_pop(16);
+	&amp;mov	(&amp;DWP(4*0,&quot;edx&quot;),&quot;eax&quot;);	# store hash value
+	&amp;mov	(&amp;DWP(4*1,&quot;edx&quot;),&quot;ebx&quot;);
+	&amp;mov	(&amp;DWP(4*2,&quot;edx&quot;),&quot;ecx&quot;);
+	&amp;mov	(&amp;DWP(4*3,&quot;edx&quot;),&quot;esi&quot;);
+	&amp;mov	(&amp;DWP(4*4,&quot;edx&quot;),&quot;edi&quot;);
+&amp;set_label(&quot;nodata&quot;);
+&amp;function_end(&quot;poly1305_blocks&quot;);
+
+&amp;function_begin(&quot;poly1305_emit&quot;);
+	&amp;mov	(&quot;ebp&quot;,&amp;wparam(0));		# context
+&amp;set_label(&quot;enter_emit&quot;);
+	&amp;mov	(&quot;edi&quot;,&amp;wparam(1));		# output
+	&amp;mov	(&quot;eax&quot;,&amp;DWP(4*0,&quot;ebp&quot;));	# load hash value
+	&amp;mov	(&quot;ebx&quot;,&amp;DWP(4*1,&quot;ebp&quot;));
+	&amp;mov	(&quot;ecx&quot;,&amp;DWP(4*2,&quot;ebp&quot;));
+	&amp;mov	(&quot;edx&quot;,&amp;DWP(4*3,&quot;ebp&quot;));
+	&amp;mov	(&quot;esi&quot;,&amp;DWP(4*4,&quot;ebp&quot;));
+
+	&amp;add	(&quot;eax&quot;,5);			# compare to modulus
+	&amp;adc	(&quot;ebx&quot;,0);
+	&amp;adc	(&quot;ecx&quot;,0);
+	&amp;adc	(&quot;edx&quot;,0);
+	&amp;adc	(&quot;esi&quot;,0);
+	&amp;shr	(&quot;esi&quot;,2);			# did it carry/borrow?
+	&amp;neg	(&quot;esi&quot;);			# do we choose hash-modulus?
+
+	&amp;and	(&quot;eax&quot;,&quot;esi&quot;);
+	&amp;and	(&quot;ebx&quot;,&quot;esi&quot;);
+	&amp;and	(&quot;ecx&quot;,&quot;esi&quot;);
+	&amp;and	(&quot;edx&quot;,&quot;esi&quot;);
+	&amp;mov	(&amp;DWP(4*0,&quot;edi&quot;),&quot;eax&quot;);
+	&amp;mov	(&amp;DWP(4*1,&quot;edi&quot;),&quot;ebx&quot;);
+	&amp;mov	(&amp;DWP(4*2,&quot;edi&quot;),&quot;ecx&quot;);
+	&amp;mov	(&amp;DWP(4*3,&quot;edi&quot;),&quot;edx&quot;);
+
+	&amp;not	(&quot;esi&quot;);			# or original hash value?
+	&amp;mov	(&quot;eax&quot;,&amp;DWP(4*0,&quot;ebp&quot;));
+	&amp;mov	(&quot;ebx&quot;,&amp;DWP(4*1,&quot;ebp&quot;));
+	&amp;mov	(&quot;ecx&quot;,&amp;DWP(4*2,&quot;ebp&quot;));
+	&amp;mov	(&quot;edx&quot;,&amp;DWP(4*3,&quot;ebp&quot;));
+	&amp;mov	(&quot;ebp&quot;,&amp;wparam(2));
+	&amp;and	(&quot;eax&quot;,&quot;esi&quot;);
+	&amp;and	(&quot;ebx&quot;,&quot;esi&quot;);
+	&amp;and	(&quot;ecx&quot;,&quot;esi&quot;);
+	&amp;and	(&quot;edx&quot;,&quot;esi&quot;);
+	&amp;or	(&quot;eax&quot;,&amp;DWP(4*0,&quot;edi&quot;));
+	&amp;or	(&quot;ebx&quot;,&amp;DWP(4*1,&quot;edi&quot;));
+	&amp;or	(&quot;ecx&quot;,&amp;DWP(4*2,&quot;edi&quot;));
+	&amp;or	(&quot;edx&quot;,&amp;DWP(4*3,&quot;edi&quot;));
+
+	&amp;add	(&quot;eax&quot;,&amp;DWP(4*0,&quot;ebp&quot;));	# accumulate key
+	&amp;adc	(&quot;ebx&quot;,&amp;DWP(4*1,&quot;ebp&quot;));
+	&amp;adc	(&quot;ecx&quot;,&amp;DWP(4*2,&quot;ebp&quot;));
+	&amp;adc	(&quot;edx&quot;,&amp;DWP(4*3,&quot;ebp&quot;));
+
+	&amp;mov	(&amp;DWP(4*0,&quot;edi&quot;),&quot;eax&quot;);
+	&amp;mov	(&amp;DWP(4*1,&quot;edi&quot;),&quot;ebx&quot;);
+	&amp;mov	(&amp;DWP(4*2,&quot;edi&quot;),&quot;ecx&quot;);
+	&amp;mov	(&amp;DWP(4*3,&quot;edi&quot;),&quot;edx&quot;);
+&amp;function_end(&quot;poly1305_emit&quot;);
+
+if ($sse2) {
+########################################################################
+# Layout of opaque area is following.
+#
+#	unsigned __int32 h[5];		# current hash value base 2^26
+#	unsigned __int32 is_base2_26;
+#	unsigned __int32 r[4];		# key value base 2^32
+#	unsigned __int32 pad[2];
+#	struct { unsigned __int32 r^4, r^3, r^2, r^1; } r[9];
+#
+# where r^n are base 2^26 digits of degrees of multiplier key. There are
+# 5 digits, but last four are interleaved with multiples of 5, totalling
+# in 9 elements: r0, r1, 5*r1, r2, 5*r2, r3, 5*r3, r4, 5*r4.
+
+my ($D0,$D1,$D2,$D3,$D4,$T0,$T1,$T2)=map(&quot;xmm$_&quot;,(0..7));
+my $MASK=$T2;	# borrow and keep in mind
+
+&amp;align	(32);
+&amp;function_begin_B(&quot;_poly1305_init_sse2&quot;);
+	&amp;movdqu		($D4,&amp;QWP(4*6,&quot;edi&quot;));		# key base 2^32
+	&amp;lea		(&quot;edi&quot;,&amp;DWP(16*3,&quot;edi&quot;));	# size optimization
+	&amp;mov		(&quot;ebp&quot;,&quot;esp&quot;);
+	&amp;sub		(&quot;esp&quot;,16*(9+5));
+	&amp;and		(&quot;esp&quot;,-16);
+
+	#&amp;pand		($D4,&amp;QWP(96,&quot;ebx&quot;));		# magic mask
+	&amp;movq		($MASK,&amp;QWP(64,&quot;ebx&quot;));
+
+	&amp;movdqa		($D0,$D4);
+	&amp;movdqa		($D1,$D4);
+	&amp;movdqa		($D2,$D4);
+
+	&amp;pand		($D0,$MASK);			# -&gt; base 2^26
+	&amp;psrlq		($D1,26);
+	&amp;psrldq		($D2,6);
+	&amp;pand		($D1,$MASK);
+	&amp;movdqa		($D3,$D2);
+	&amp;psrlq		($D2,4)
+	&amp;psrlq		($D3,30);
+	&amp;pand		($D2,$MASK);
+	&amp;pand		($D3,$MASK);
+	&amp;psrldq		($D4,13);
+
+	&amp;lea		(&quot;edx&quot;,&amp;DWP(16*9,&quot;esp&quot;));	# size optimization
+	&amp;mov		(&quot;ecx&quot;,2);
+&amp;set_label(&quot;square&quot;);
+	&amp;movdqa		(&amp;QWP(16*0,&quot;esp&quot;),$D0);
+	&amp;movdqa		(&amp;QWP(16*1,&quot;esp&quot;),$D1);
+	&amp;movdqa		(&amp;QWP(16*2,&quot;esp&quot;),$D2);
+	&amp;movdqa		(&amp;QWP(16*3,&quot;esp&quot;),$D3);
+	&amp;movdqa		(&amp;QWP(16*4,&quot;esp&quot;),$D4);
+
+	&amp;movdqa		($T1,$D1);
+	&amp;movdqa		($T0,$D2);
+	&amp;pslld		($T1,2);
+	&amp;pslld		($T0,2);
+	&amp;paddd		($T1,$D1);			# *5
+	&amp;paddd		($T0,$D2);			# *5
+	&amp;movdqa		(&amp;QWP(16*5,&quot;esp&quot;),$T1);
+	&amp;movdqa		(&amp;QWP(16*6,&quot;esp&quot;),$T0);
+	&amp;movdqa		($T1,$D3);
+	&amp;movdqa		($T0,$D4);
+	&amp;pslld		($T1,2);
+	&amp;pslld		($T0,2);
+	&amp;paddd		($T1,$D3);			# *5
+	&amp;paddd		($T0,$D4);			# *5
+	&amp;movdqa		(&amp;QWP(16*7,&quot;esp&quot;),$T1);
+	&amp;movdqa		(&amp;QWP(16*8,&quot;esp&quot;),$T0);
+
+	&amp;pshufd		($T1,$D0,0b01000100);
+	&amp;movdqa		($T0,$D1);
+	&amp;pshufd		($D1,$D1,0b01000100);
+	&amp;pshufd		($D2,$D2,0b01000100);
+	&amp;pshufd		($D3,$D3,0b01000100);
+	&amp;pshufd		($D4,$D4,0b01000100);
+	&amp;movdqa		(&amp;QWP(16*0,&quot;edx&quot;),$T1);
+	&amp;movdqa		(&amp;QWP(16*1,&quot;edx&quot;),$D1);
+	&amp;movdqa		(&amp;QWP(16*2,&quot;edx&quot;),$D2);
+	&amp;movdqa		(&amp;QWP(16*3,&quot;edx&quot;),$D3);
+	&amp;movdqa		(&amp;QWP(16*4,&quot;edx&quot;),$D4);
+
+	################################################################
+	# d4 = h4*r0 + h3*r1   + h2*r2   + h1*r3   + h0*r4
+	# d3 = h3*r0 + h2*r1   + h1*r2   + h0*r3   + h4*5*r4
+	# d2 = h2*r0 + h1*r1   + h0*r2   + h4*5*r3 + h3*5*r4
+	# d1 = h1*r0 + h0*r1   + h4*5*r2 + h3*5*r3 + h2*5*r4
+	# d0 = h0*r0 + h4*5*r1 + h3*5*r2 + h2*5*r3 + h1*5*r4
+
+	&amp;pmuludq	($D4,$D0);			# h4*r0
+	&amp;pmuludq	($D3,$D0);			# h3*r0
+	&amp;pmuludq	($D2,$D0);			# h2*r0
+	&amp;pmuludq	($D1,$D0);			# h1*r0
+	&amp;pmuludq	($D0,$T1);			# h0*r0
+
+sub pmuladd {
+my $load = shift;
+my $base = shift; $base = &quot;esp&quot; if (!defined($base));
+
+	################################################################
+	# As for choice to &quot;rotate&quot; $T0-$T2 in order to move paddq
+	# past next multiplication. While it makes code harder to read
+	# and doesn't have significant effect on most processors, it
+	# makes a lot of difference on Atom, up to 30% improvement.
+
+	&amp;movdqa		($T1,$T0);
+	&amp;pmuludq	($T0,&amp;QWP(16*3,$base));		# r1*h3
+	&amp;movdqa		($T2,$T1);
+	&amp;pmuludq	($T1,&amp;QWP(16*2,$base));		# r1*h2
+	&amp;paddq		($D4,$T0);
+	&amp;movdqa		($T0,$T2);
+	&amp;pmuludq	($T2,&amp;QWP(16*1,$base));		# r1*h1
+	&amp;paddq		($D3,$T1);
+	&amp;$load		($T1,5);			# s1
+	&amp;pmuludq	($T0,&amp;QWP(16*0,$base));		# r1*h0
+	&amp;paddq		($D2,$T2);
+	&amp;pmuludq	($T1,&amp;QWP(16*4,$base));		# s1*h4
+	 &amp;$load		($T2,2);			# r2^n
+	&amp;paddq		($D1,$T0);
+
+	&amp;movdqa		($T0,$T2);
+	&amp;pmuludq	($T2,&amp;QWP(16*2,$base));		# r2*h2
+	 &amp;paddq		($D0,$T1);
+	&amp;movdqa		($T1,$T0);
+	&amp;pmuludq	($T0,&amp;QWP(16*1,$base));		# r2*h1
+	&amp;paddq		($D4,$T2);
+	&amp;$load		($T2,6);			# s2^n
+	&amp;pmuludq	($T1,&amp;QWP(16*0,$base));		# r2*h0
+	&amp;paddq		($D3,$T0);
+	&amp;movdqa		($T0,$T2);
+	&amp;pmuludq	($T2,&amp;QWP(16*4,$base));		# s2*h4
+	&amp;paddq		($D2,$T1);
+	&amp;pmuludq	($T0,&amp;QWP(16*3,$base));		# s2*h3
+	 &amp;$load		($T1,3);			# r3^n
+	&amp;paddq		($D1,$T2);
+
+	&amp;movdqa		($T2,$T1);
+	&amp;pmuludq	($T1,&amp;QWP(16*1,$base));		# r3*h1
+	 &amp;paddq		($D0,$T0);
+	&amp;$load		($T0,7);			# s3^n
+	&amp;pmuludq	($T2,&amp;QWP(16*0,$base));		# r3*h0
+	&amp;paddq		($D4,$T1);
+	&amp;movdqa		($T1,$T0);
+	&amp;pmuludq	($T0,&amp;QWP(16*4,$base));		# s3*h4
+	&amp;paddq		($D3,$T2);
+	&amp;movdqa		($T2,$T1);
+	&amp;pmuludq	($T1,&amp;QWP(16*3,$base));		# s3*h3
+	&amp;paddq		($D2,$T0);
+	&amp;pmuludq	($T2,&amp;QWP(16*2,$base));		# s3*h2
+	 &amp;$load		($T0,4);			# r4^n
+	&amp;paddq		($D1,$T1);
+
+	&amp;$load		($T1,8);			# s4^n
+	&amp;pmuludq	($T0,&amp;QWP(16*0,$base));		# r4*h0
+	 &amp;paddq		($D0,$T2);
+	&amp;movdqa		($T2,$T1);
+	&amp;pmuludq	($T1,&amp;QWP(16*4,$base));		# s4*h4
+	&amp;paddq		($D4,$T0);
+	&amp;movdqa		($T0,$T2);
+	&amp;pmuludq	($T2,&amp;QWP(16*1,$base));		# s4*h1
+	&amp;paddq		($D3,$T1);
+	&amp;movdqa		($T1,$T0);
+	&amp;pmuludq	($T0,&amp;QWP(16*2,$base));		# s4*h2
+	&amp;paddq		($D0,$T2);
+	&amp;pmuludq	($T1,&amp;QWP(16*3,$base));		# s4*h3
+	 &amp;movdqa	($MASK,&amp;QWP(64,&quot;ebx&quot;));
+	&amp;paddq		($D1,$T0);
+	&amp;paddq		($D2,$T1);
+}
+	&amp;pmuladd	(sub {	my ($reg,$i)=@_;
+				&amp;movdqa ($reg,&amp;QWP(16*$i,&quot;esp&quot;));
+			     },&quot;edx&quot;);
+
+sub lazy_reduction {
+	################################################################
+	# lazy reduction as discussed in &quot;NEON crypto&quot; by D.J. Bernstein
+	# and P. Schwabe
+
+	 &amp;movdqa	($T0,$D3);
+	 &amp;pand		($D3,$MASK);
+	 &amp;psrlq		($T0,26);
+	 &amp;paddq		($T0,$D4);			# h3 -&gt; h4
+	&amp;movdqa		($T1,$D0);
+	&amp;pand		($D0,$MASK);
+	&amp;psrlq		($T1,26);
+	 &amp;movdqa	($D4,$T0);
+	&amp;paddq		($T1,$D1);			# h0 -&gt; h1
+	 &amp;psrlq		($T0,26);
+	 &amp;pand		($D4,$MASK);
+	&amp;movdqa		($D1,$T1);
+	&amp;psrlq		($T1,26);
+	 &amp;paddd		($D0,$T0);			# favour paddd when
+							# possible, because
+							# paddq is &quot;broken&quot;
+							# on Atom
+	&amp;pand		($D1,$MASK);
+	&amp;paddq		($T1,$D2);			# h1 -&gt; h2
+	 &amp;psllq		($T0,2);
+	&amp;movdqa		($D2,$T1);
+	&amp;psrlq		($T1,26);
+	 &amp;paddd		($T0,$D0);			# h4 -&gt; h0
+	&amp;pand		($D2,$MASK);
+	&amp;paddd		($T1,$D3);			# h2 -&gt; h3
+	 &amp;movdqa	($D0,$T0);
+	 &amp;psrlq		($T0,26);
+	&amp;movdqa		($D3,$T1);
+	&amp;psrlq		($T1,26);
+	 &amp;pand		($D0,$MASK);
+	 &amp;paddd		($D1,$T0);			# h0 -&gt; h1
+	&amp;pand		($D3,$MASK);
+	&amp;paddd		($D4,$T1);			# h3 -&gt; h4
+}
+	&amp;lazy_reduction	();
+
+	&amp;dec		(&quot;ecx&quot;);
+	&amp;jz		(&amp;label(&quot;square_break&quot;));
+
+	&amp;punpcklqdq	($D0,&amp;QWP(16*0,&quot;esp&quot;));		# 0:r^1:0:r^2
+	&amp;punpcklqdq	($D1,&amp;QWP(16*1,&quot;esp&quot;));
+	&amp;punpcklqdq	($D2,&amp;QWP(16*2,&quot;esp&quot;));
+	&amp;punpcklqdq	($D3,&amp;QWP(16*3,&quot;esp&quot;));
+	&amp;punpcklqdq	($D4,&amp;QWP(16*4,&quot;esp&quot;));
+	&amp;jmp		(&amp;label(&quot;square&quot;));
+
+&amp;set_label(&quot;square_break&quot;);
+	&amp;psllq		($D0,32);			# -&gt; r^3:0:r^4:0
+	&amp;psllq		($D1,32);
+	&amp;psllq		($D2,32);
+	&amp;psllq		($D3,32);
+	&amp;psllq		($D4,32);
+	&amp;por		($D0,&amp;QWP(16*0,&quot;esp&quot;));		# r^3:r^1:r^4:r^2
+	&amp;por		($D1,&amp;QWP(16*1,&quot;esp&quot;));
+	&amp;por		($D2,&amp;QWP(16*2,&quot;esp&quot;));
+	&amp;por		($D3,&amp;QWP(16*3,&quot;esp&quot;));
+	&amp;por		($D4,&amp;QWP(16*4,&quot;esp&quot;));
+
+	&amp;pshufd		($D0,$D0,0b10001101);		# -&gt; r^1:r^2:r^3:r^4
+	&amp;pshufd		($D1,$D1,0b10001101);
+	&amp;pshufd		($D2,$D2,0b10001101);
+	&amp;pshufd		($D3,$D3,0b10001101);
+	&amp;pshufd		($D4,$D4,0b10001101);
+
+	&amp;movdqu		(&amp;QWP(16*0,&quot;edi&quot;),$D0);		# save the table
+	&amp;movdqu		(&amp;QWP(16*1,&quot;edi&quot;),$D1);
+	&amp;movdqu		(&amp;QWP(16*2,&quot;edi&quot;),$D2);
+	&amp;movdqu		(&amp;QWP(16*3,&quot;edi&quot;),$D3);
+	&amp;movdqu		(&amp;QWP(16*4,&quot;edi&quot;),$D4);
+
+	&amp;movdqa		($T1,$D1);
+	&amp;movdqa		($T0,$D2);
+	&amp;pslld		($T1,2);
+	&amp;pslld		($T0,2);
+	&amp;paddd		($T1,$D1);			# *5
+	&amp;paddd		($T0,$D2);			# *5
+	&amp;movdqu		(&amp;QWP(16*5,&quot;edi&quot;),$T1);
+	&amp;movdqu		(&amp;QWP(16*6,&quot;edi&quot;),$T0);
+	&amp;movdqa		($T1,$D3);
+	&amp;movdqa		($T0,$D4);
+	&amp;pslld		($T1,2);
+	&amp;pslld		($T0,2);
+	&amp;paddd		($T1,$D3);			# *5
+	&amp;paddd		($T0,$D4);			# *5
+	&amp;movdqu		(&amp;QWP(16*7,&quot;edi&quot;),$T1);
+	&amp;movdqu		(&amp;QWP(16*8,&quot;edi&quot;),$T0);
+
+	&amp;mov		(&quot;esp&quot;,&quot;ebp&quot;);
+	&amp;lea		(&quot;edi&quot;,&amp;DWP(-16*3,&quot;edi&quot;));	# size de-optimization
+	&amp;ret		();
+&amp;function_end_B(&quot;_poly1305_init_sse2&quot;);
+
+&amp;align	(32);
+&amp;function_begin(&quot;_poly1305_blocks_sse2&quot;);
+	&amp;mov	(&quot;edi&quot;,&amp;wparam(0));			# ctx
+	&amp;mov	(&quot;esi&quot;,&amp;wparam(1));			# inp
+	&amp;mov	(&quot;ecx&quot;,&amp;wparam(2));			# len
+
+	&amp;mov	(&quot;eax&quot;,&amp;DWP(4*5,&quot;edi&quot;));		# is_base2_26
+	&amp;and	(&quot;ecx&quot;,-16);
+	&amp;jz	(&amp;label(&quot;nodata&quot;));
+	&amp;cmp	(&quot;ecx&quot;,64);
+	&amp;jae	(&amp;label(&quot;enter_sse2&quot;));
+	&amp;test	(&quot;eax&quot;,&quot;eax&quot;);				# is_base2_26?
+	&amp;jz	(&amp;label(&quot;enter_blocks&quot;));
+
+&amp;set_label(&quot;enter_sse2&quot;,16);
+	&amp;call	(&amp;label(&quot;pic_point&quot;));
+&amp;set_label(&quot;pic_point&quot;);
+	&amp;blindpop(&quot;ebx&quot;);
+	&amp;lea	(&quot;ebx&quot;,&amp;DWP(&amp;label(&quot;const_sse2&quot;).&quot;-&quot;.&amp;label(&quot;pic_point&quot;),&quot;ebx&quot;));
+
+	&amp;test	(&quot;eax&quot;,&quot;eax&quot;);				# is_base2_26?
+	&amp;jnz	(&amp;label(&quot;base2_26&quot;));
+
+	&amp;call	(&quot;_poly1305_init_sse2&quot;);
+
+	################################################# base 2^32 -&gt; base 2^26
+	&amp;mov	(&quot;eax&quot;,&amp;DWP(0,&quot;edi&quot;));
+	&amp;mov	(&quot;ecx&quot;,&amp;DWP(3,&quot;edi&quot;));
+	&amp;mov	(&quot;edx&quot;,&amp;DWP(6,&quot;edi&quot;));
+	&amp;mov	(&quot;esi&quot;,&amp;DWP(9,&quot;edi&quot;));
+	&amp;mov	(&quot;ebp&quot;,&amp;DWP(13,&quot;edi&quot;));
+	&amp;mov	(&amp;DWP(4*5,&quot;edi&quot;),1);			# is_base2_26
+
+	&amp;shr	(&quot;ecx&quot;,2);
+	&amp;and	(&quot;eax&quot;,0x3ffffff);
+	&amp;shr	(&quot;edx&quot;,4);
+	&amp;and	(&quot;ecx&quot;,0x3ffffff);
+	&amp;shr	(&quot;esi&quot;,6);
+	&amp;and	(&quot;edx&quot;,0x3ffffff);
+
+	&amp;movd	($D0,&quot;eax&quot;);
+	&amp;movd	($D1,&quot;ecx&quot;);
+	&amp;movd	($D2,&quot;edx&quot;);
+	&amp;movd	($D3,&quot;esi&quot;);
+	&amp;movd	($D4,&quot;ebp&quot;);
+
+	&amp;mov	(&quot;esi&quot;,&amp;wparam(1));			# [reload] inp
+	&amp;mov	(&quot;ecx&quot;,&amp;wparam(2));			# [reload] len
+	&amp;jmp	(&amp;label(&quot;base2_32&quot;));
+
+&amp;set_label(&quot;base2_26&quot;,16);
+	&amp;movd	($D0,&amp;DWP(4*0,&quot;edi&quot;));			# load hash value
+	&amp;movd	($D1,&amp;DWP(4*1,&quot;edi&quot;));
+	&amp;movd	($D2,&amp;DWP(4*2,&quot;edi&quot;));
+	&amp;movd	($D3,&amp;DWP(4*3,&quot;edi&quot;));
+	&amp;movd	($D4,&amp;DWP(4*4,&quot;edi&quot;));
+	&amp;movdqa	($MASK,&amp;QWP(64,&quot;ebx&quot;));
+
+&amp;set_label(&quot;base2_32&quot;);
+	&amp;mov	(&quot;eax&quot;,&amp;wparam(3));			# padbit
+	&amp;mov	(&quot;ebp&quot;,&quot;esp&quot;);
+
+	&amp;sub	(&quot;esp&quot;,16*(5+5+5+9+9));
+	&amp;and	(&quot;esp&quot;,-16);
+
+	&amp;lea	(&quot;edi&quot;,&amp;DWP(16*3,&quot;edi&quot;));		# size optimization
+	&amp;shl	(&quot;eax&quot;,24);				# padbit
+
+	&amp;test	(&quot;ecx&quot;,31);
+	&amp;jz	(&amp;label(&quot;even&quot;));
+
+	################################################################
+	# process single block, with SSE2, because it's still faster
+	# even though half of result is discarded
+
+	&amp;movdqu		($T1,&amp;QWP(0,&quot;esi&quot;));		# input
+	&amp;lea		(&quot;esi&quot;,&amp;DWP(16,&quot;esi&quot;));
+
+	&amp;movdqa		($T0,$T1);			# -&gt; base 2^26 ...
+	&amp;pand		($T1,$MASK);
+	&amp;paddd		($D0,$T1);			# ... and accumuate
+
+	&amp;movdqa		($T1,$T0);
+	&amp;psrlq		($T0,26);
+	&amp;psrldq		($T1,6);
+	&amp;pand		($T0,$MASK);
+	&amp;paddd		($D1,$T0);
+
+	&amp;movdqa		($T0,$T1);
+	&amp;psrlq		($T1,4);
+	&amp;pand		($T1,$MASK);
+	&amp;paddd		($D2,$T1);
+
+	&amp;movdqa		($T1,$T0);
+	&amp;psrlq		($T0,30);
+	&amp;pand		($T0,$MASK);
+	&amp;psrldq		($T1,7);
+	&amp;paddd		($D3,$T0);
+
+	&amp;movd		($T0,&quot;eax&quot;);			# padbit
+	&amp;paddd		($D4,$T1);
+	 &amp;movd		($T1,&amp;DWP(16*0+12,&quot;edi&quot;));	# r0
+	&amp;paddd		($D4,$T0);
+
+	&amp;movdqa		(&amp;QWP(16*0,&quot;esp&quot;),$D0);
+	&amp;movdqa		(&amp;QWP(16*1,&quot;esp&quot;),$D1);
+	&amp;movdqa		(&amp;QWP(16*2,&quot;esp&quot;),$D2);
+	&amp;movdqa		(&amp;QWP(16*3,&quot;esp&quot;),$D3);
+	&amp;movdqa		(&amp;QWP(16*4,&quot;esp&quot;),$D4);
+
+	################################################################
+	# d4 = h4*r0 + h3*r1   + h2*r2   + h1*r3   + h0*r4
+	# d3 = h3*r0 + h2*r1   + h1*r2   + h0*r3   + h4*5*r4
+	# d2 = h2*r0 + h1*r1   + h0*r2   + h4*5*r3 + h3*5*r4
+	# d1 = h1*r0 + h0*r1   + h4*5*r2 + h3*5*r3 + h2*5*r4
+	# d0 = h0*r0 + h4*5*r1 + h3*5*r2 + h2*5*r3 + h1*5*r4
+
+	&amp;pmuludq	($D0,$T1);			# h4*r0
+	&amp;pmuludq	($D1,$T1);			# h3*r0
+	&amp;pmuludq	($D2,$T1);			# h2*r0
+	 &amp;movd		($T0,&amp;DWP(16*1+12,&quot;edi&quot;));	# r1
+	&amp;pmuludq	($D3,$T1);			# h1*r0
+	&amp;pmuludq	($D4,$T1);			# h0*r0
+
+	&amp;pmuladd	(sub {	my ($reg,$i)=@_;
+				&amp;movd ($reg,&amp;DWP(16*$i+12,&quot;edi&quot;));
+			     });
+
+	&amp;lazy_reduction	();
+
+	&amp;sub		(&quot;ecx&quot;,16);
+	&amp;jz		(&amp;label(&quot;done&quot;));
+
+&amp;set_label(&quot;even&quot;);
+	&amp;lea		(&quot;edx&quot;,&amp;DWP(16*(5+5+5+9),&quot;esp&quot;));# size optimization
+	&amp;lea		(&quot;eax&quot;,&amp;DWP(-16*2,&quot;esi&quot;));
+	&amp;sub		(&quot;ecx&quot;,64);
+
+	################################################################
+	# expand and copy pre-calculated table to stack
+
+	&amp;movdqu		($T0,&amp;QWP(16*0,&quot;edi&quot;));		# r^1:r^2:r^3:r^4
+	&amp;pshufd		($T1,$T0,0b01000100);		# duplicate r^3:r^4
+	&amp;cmovb		(&quot;esi&quot;,&quot;eax&quot;);
+	&amp;pshufd		($T0,$T0,0b11101110);		# duplicate r^1:r^2
+	&amp;movdqa		(&amp;QWP(16*0,&quot;edx&quot;),$T1);
+	&amp;lea		(&quot;eax&quot;,&amp;DWP(16*10,&quot;esp&quot;));
+	&amp;movdqu		($T1,&amp;QWP(16*1,&quot;edi&quot;));
+	&amp;movdqa		(&amp;QWP(16*(0-9),&quot;edx&quot;),$T0);
+	&amp;pshufd		($T0,$T1,0b01000100);
+	&amp;pshufd		($T1,$T1,0b11101110);
+	&amp;movdqa		(&amp;QWP(16*1,&quot;edx&quot;),$T0);
+	&amp;movdqu		($T0,&amp;QWP(16*2,&quot;edi&quot;));
+	&amp;movdqa		(&amp;QWP(16*(1-9),&quot;edx&quot;),$T1);
+	&amp;pshufd		($T1,$T0,0b01000100);
+	&amp;pshufd		($T0,$T0,0b11101110);
+	&amp;movdqa		(&amp;QWP(16*2,&quot;edx&quot;),$T1);
+	&amp;movdqu		($T1,&amp;QWP(16*3,&quot;edi&quot;));
+	&amp;movdqa		(&amp;QWP(16*(2-9),&quot;edx&quot;),$T0);
+	&amp;pshufd		($T0,$T1,0b01000100);
+	&amp;pshufd		($T1,$T1,0b11101110);
+	&amp;movdqa		(&amp;QWP(16*3,&quot;edx&quot;),$T0);
+	&amp;movdqu		($T0,&amp;QWP(16*4,&quot;edi&quot;));
+	&amp;movdqa		(&amp;QWP(16*(3-9),&quot;edx&quot;),$T1);
+	&amp;pshufd		($T1,$T0,0b01000100);
+	&amp;pshufd		($T0,$T0,0b11101110);
+	&amp;movdqa		(&amp;QWP(16*4,&quot;edx&quot;),$T1);
+	&amp;movdqu		($T1,&amp;QWP(16*5,&quot;edi&quot;));
+	&amp;movdqa		(&amp;QWP(16*(4-9),&quot;edx&quot;),$T0);
+	&amp;pshufd		($T0,$T1,0b01000100);
+	&amp;pshufd		($T1,$T1,0b11101110);
+	&amp;movdqa		(&amp;QWP(16*5,&quot;edx&quot;),$T0);
+	&amp;movdqu		($T0,&amp;QWP(16*6,&quot;edi&quot;));
+	&amp;movdqa		(&amp;QWP(16*(5-9),&quot;edx&quot;),$T1);
+	&amp;pshufd		($T1,$T0,0b01000100);
+	&amp;pshufd		($T0,$T0,0b11101110);
+	&amp;movdqa		(&amp;QWP(16*6,&quot;edx&quot;),$T1);
+	&amp;movdqu		($T1,&amp;QWP(16*7,&quot;edi&quot;));
+	&amp;movdqa		(&amp;QWP(16*(6-9),&quot;edx&quot;),$T0);
+	&amp;pshufd		($T0,$T1,0b01000100);
+	&amp;pshufd		($T1,$T1,0b11101110);
+	&amp;movdqa		(&amp;QWP(16*7,&quot;edx&quot;),$T0);
+	&amp;movdqu		($T0,&amp;QWP(16*8,&quot;edi&quot;));
+	&amp;movdqa		(&amp;QWP(16*(7-9),&quot;edx&quot;),$T1);
+	&amp;pshufd		($T1,$T0,0b01000100);
+	&amp;pshufd		($T0,$T0,0b11101110);
+	&amp;movdqa		(&amp;QWP(16*8,&quot;edx&quot;),$T1);
+	&amp;movdqa		(&amp;QWP(16*(8-9),&quot;edx&quot;),$T0);
+
+sub load_input {
+my ($inpbase,$offbase)=@_;
+
+	&amp;movdqu		($T0,&amp;QWP($inpbase+0,&quot;esi&quot;));	# load input
+	&amp;movdqu		($T1,&amp;QWP($inpbase+16,&quot;esi&quot;));
+	&amp;lea		(&quot;esi&quot;,&amp;DWP(16*2,&quot;esi&quot;));
+
+	&amp;movdqa		(&amp;QWP($offbase+16*2,&quot;esp&quot;),$D2);
+	&amp;movdqa		(&amp;QWP($offbase+16*3,&quot;esp&quot;),$D3);
+	&amp;movdqa		(&amp;QWP($offbase+16*4,&quot;esp&quot;),$D4);
+
+	&amp;movdqa		($D2,$T0);			# splat input
+	&amp;movdqa		($D3,$T1);
+	&amp;psrldq		($D2,6);
+	&amp;psrldq		($D3,6);
+	&amp;movdqa		($D4,$T0);
+	&amp;punpcklqdq	($D2,$D3);			# 2:3
+	&amp;punpckhqdq	($D4,$T1);			# 4
+	&amp;punpcklqdq	($T0,$T1);			# 0:1
+
+	&amp;movdqa		($D3,$D2);
+	&amp;psrlq		($D2,4);
+	&amp;psrlq		($D3,30);
+	&amp;movdqa		($T1,$T0);
+	&amp;psrlq		($D4,40);			# 4
+	&amp;psrlq		($T1,26);
+	&amp;pand		($T0,$MASK);			# 0
+	&amp;pand		($T1,$MASK);			# 1
+	&amp;pand		($D2,$MASK);			# 2
+	&amp;pand		($D3,$MASK);			# 3
+	&amp;por		($D4,&amp;QWP(0,&quot;ebx&quot;));		# padbit, yes, always
+
+	&amp;movdqa		(&amp;QWP($offbase+16*0,&quot;esp&quot;),$D0)	if ($offbase);
+	&amp;movdqa		(&amp;QWP($offbase+16*1,&quot;esp&quot;),$D1)	if ($offbase);
+}
+	&amp;load_input	(16*2,16*5);
+
+	&amp;jbe		(&amp;label(&quot;skip_loop&quot;));
+	&amp;jmp		(&amp;label(&quot;loop&quot;));
+
+&amp;set_label(&quot;loop&quot;,32);
+	################################################################
+	# ((inp[0]*r^4+inp[2]*r^2+inp[4])*r^4+inp[6]*r^2
+	# ((inp[1]*r^4+inp[3]*r^2+inp[5])*r^3+inp[7]*r
+	#   \___________________/
+	# ((inp[0]*r^4+inp[2]*r^2+inp[4])*r^4+inp[6]*r^2+inp[8])*r^2
+	# ((inp[1]*r^4+inp[3]*r^2+inp[5])*r^4+inp[7]*r^2+inp[9])*r
+	#   \___________________/ \____________________/
+	################################################################
+
+	&amp;movdqa		($T2,&amp;QWP(16*(0-9),&quot;edx&quot;));	# r0^2
+	&amp;movdqa		(&amp;QWP(16*1,&quot;eax&quot;),$T1);
+	&amp;movdqa		(&amp;QWP(16*2,&quot;eax&quot;),$D2);
+	&amp;movdqa		(&amp;QWP(16*3,&quot;eax&quot;),$D3);
+	&amp;movdqa		(&amp;QWP(16*4,&quot;eax&quot;),$D4);
+
+	################################################################
+	# d4 = h4*r0 + h0*r4   + h1*r3   + h2*r2   + h3*r1
+	# d3 = h3*r0 + h0*r3   + h1*r2   + h2*r1   + h4*5*r4
+	# d2 = h2*r0 + h0*r2   + h1*r1   + h3*5*r4 + h4*5*r3
+	# d1 = h1*r0 + h0*r1   + h2*5*r4 + h3*5*r3 + h4*5*r2
+	# d0 = h0*r0 + h1*5*r4 + h2*5*r3 + h3*5*r2 + h4*5*r1
+
+	&amp;movdqa		($D1,$T0);
+	&amp;pmuludq	($T0,$T2);			# h0*r0
+	&amp;movdqa		($D0,$T1);
+	&amp;pmuludq	($T1,$T2);			# h1*r0
+	&amp;pmuludq	($D2,$T2);			# h2*r0
+	&amp;pmuludq	($D3,$T2);			# h3*r0
+	&amp;pmuludq	($D4,$T2);			# h4*r0
+
+sub pmuladd_alt {
+my $addr = shift;
+
+	&amp;pmuludq	($D0,&amp;$addr(8));		# h1*s4
+	&amp;movdqa		($T2,$D1);
+	&amp;pmuludq	($D1,&amp;$addr(1));		# h0*r1
+	&amp;paddq		($D0,$T0);
+	&amp;movdqa		($T0,$T2);
+	&amp;pmuludq	($T2,&amp;$addr(2));		# h0*r2
+	&amp;paddq		($D1,$T1);
+	&amp;movdqa		($T1,$T0);
+	&amp;pmuludq	($T0,&amp;$addr(3));		# h0*r3
+	&amp;paddq		($D2,$T2);
+	 &amp;movdqa	($T2,&amp;QWP(16*1,&quot;eax&quot;));		# pull h1
+	&amp;pmuludq	($T1,&amp;$addr(4));		# h0*r4
+	&amp;paddq		($D3,$T0);
+
+	&amp;movdqa		($T0,$T2);
+	&amp;pmuludq	($T2,&amp;$addr(1));		# h1*r1
+	 &amp;paddq		($D4,$T1);
+	&amp;movdqa		($T1,$T0);
+	&amp;pmuludq	($T0,&amp;$addr(2));		# h1*r2
+	&amp;paddq		($D2,$T2);
+	&amp;movdqa		($T2,&amp;QWP(16*2,&quot;eax&quot;));		# pull h2
+	&amp;pmuludq	($T1,&amp;$addr(3));		# h1*r3
+	&amp;paddq		($D3,$T0);
+	&amp;movdqa		($T0,$T2);
+	&amp;pmuludq	($T2,&amp;$addr(7));		# h2*s3
+	&amp;paddq		($D4,$T1);
+	&amp;movdqa		($T1,$T0);
+	&amp;pmuludq	($T0,&amp;$addr(8));		# h2*s4
+	&amp;paddq		($D0,$T2);
+
+	&amp;movdqa		($T2,$T1);
+	&amp;pmuludq	($T1,&amp;$addr(1));		# h2*r1
+	 &amp;paddq		($D1,$T0);
+	&amp;movdqa		($T0,&amp;QWP(16*3,&quot;eax&quot;));		# pull h3
+	&amp;pmuludq	($T2,&amp;$addr(2));		# h2*r2
+	&amp;paddq		($D3,$T1);
+	&amp;movdqa		($T1,$T0);
+	&amp;pmuludq	($T0,&amp;$addr(6));		# h3*s2
+	&amp;paddq		($D4,$T2);
+	&amp;movdqa		($T2,$T1);
+	&amp;pmuludq	($T1,&amp;$addr(7));		# h3*s3
+	&amp;paddq		($D0,$T0);
+	&amp;movdqa		($T0,$T2);
+	&amp;pmuludq	($T2,&amp;$addr(8));		# h3*s4
+	&amp;paddq		($D1,$T1);
+
+	&amp;movdqa		($T1,&amp;QWP(16*4,&quot;eax&quot;));		# pull h4
+	&amp;pmuludq	($T0,&amp;$addr(1));		# h3*r1
+	 &amp;paddq		($D2,$T2);
+	&amp;movdqa		($T2,$T1);
+	&amp;pmuludq	($T1,&amp;$addr(8));		# h4*s4
+	&amp;paddq		($D4,$T0);
+	&amp;movdqa		($T0,$T2);
+	&amp;pmuludq	($T2,&amp;$addr(5));		# h4*s1
+	&amp;paddq		($D3,$T1);
+	&amp;movdqa		($T1,$T0);
+	&amp;pmuludq	($T0,&amp;$addr(6));		# h4*s2
+	&amp;paddq		($D0,$T2);
+	 &amp;movdqa	($MASK,&amp;QWP(64,&quot;ebx&quot;));
+	&amp;pmuludq	($T1,&amp;$addr(7));		# h4*s3
+	&amp;paddq		($D1,$T0);
+	&amp;paddq		($D2,$T1);
+}
+	&amp;pmuladd_alt	(sub {	my $i=shift; &amp;QWP(16*($i-9),&quot;edx&quot;);	});
+
+	&amp;load_input	(-16*2,0);
+	&amp;lea		(&quot;eax&quot;,&amp;DWP(-16*2,&quot;esi&quot;));
+	&amp;sub		(&quot;ecx&quot;,64);
+
+	&amp;paddd		($T0,&amp;QWP(16*(5+0),&quot;esp&quot;));	# add hash value
+	&amp;paddd		($T1,&amp;QWP(16*(5+1),&quot;esp&quot;));
+	&amp;paddd		($D2,&amp;QWP(16*(5+2),&quot;esp&quot;));
+	&amp;paddd		($D3,&amp;QWP(16*(5+3),&quot;esp&quot;));
+	&amp;paddd		($D4,&amp;QWP(16*(5+4),&quot;esp&quot;));
+
+	&amp;cmovb		(&quot;esi&quot;,&quot;eax&quot;);
+	&amp;lea		(&quot;eax&quot;,&amp;DWP(16*10,&quot;esp&quot;));
+
+	&amp;movdqa		($T2,&amp;QWP(16*0,&quot;edx&quot;));		# r0^4
+	&amp;movdqa		(&amp;QWP(16*1,&quot;esp&quot;),$D1);
+	&amp;movdqa		(&amp;QWP(16*1,&quot;eax&quot;),$T1);
+	&amp;movdqa		(&amp;QWP(16*2,&quot;eax&quot;),$D2);
+	&amp;movdqa		(&amp;QWP(16*3,&quot;eax&quot;),$D3);
+	&amp;movdqa		(&amp;QWP(16*4,&quot;eax&quot;),$D4);
+
+	################################################################
+	# d4 += h4*r0 + h0*r4   + h1*r3   + h2*r2   + h3*r1
+	# d3 += h3*r0 + h0*r3   + h1*r2   + h2*r1   + h4*5*r4
+	# d2 += h2*r0 + h0*r2   + h1*r1   + h3*5*r4 + h4*5*r3
+	# d1 += h1*r0 + h0*r1   + h2*5*r4 + h3*5*r3 + h4*5*r2
+	# d0 += h0*r0 + h1*5*r4 + h2*5*r3 + h3*5*r2 + h4*5*r1
+
+	&amp;movdqa		($D1,$T0);
+	&amp;pmuludq	($T0,$T2);			# h0*r0
+	&amp;paddq		($T0,$D0);
+	&amp;movdqa		($D0,$T1);
+	&amp;pmuludq	($T1,$T2);			# h1*r0
+	&amp;pmuludq	($D2,$T2);			# h2*r0
+	&amp;pmuludq	($D3,$T2);			# h3*r0
+	&amp;pmuludq	($D4,$T2);			# h4*r0
+
+	&amp;paddq		($T1,&amp;QWP(16*1,&quot;esp&quot;));
+	&amp;paddq		($D2,&amp;QWP(16*2,&quot;esp&quot;));
+	&amp;paddq		($D3,&amp;QWP(16*3,&quot;esp&quot;));
+	&amp;paddq		($D4,&amp;QWP(16*4,&quot;esp&quot;));
+
+	&amp;pmuladd_alt	(sub {	my $i=shift; &amp;QWP(16*$i,&quot;edx&quot;);	});
+
+	&amp;lazy_reduction	();
+
+	&amp;load_input	(16*2,16*5);
+
+	&amp;ja		(&amp;label(&quot;loop&quot;));
+
+&amp;set_label(&quot;skip_loop&quot;);
+	################################################################
+	# multiply (inp[0:1]+hash) or inp[2:3] by r^2:r^1
+
+	 &amp;pshufd	($T2,&amp;QWP(16*(0-9),&quot;edx&quot;),0x10);# r0^n
+	&amp;add		(&quot;ecx&quot;,32);
+	&amp;jnz		(&amp;label(&quot;long_tail&quot;));
+
+	&amp;paddd		($T0,$D0);			# add hash value
+	&amp;paddd		($T1,$D1);
+	&amp;paddd		($D2,&amp;QWP(16*7,&quot;esp&quot;));
+	&amp;paddd		($D3,&amp;QWP(16*8,&quot;esp&quot;));
+	&amp;paddd		($D4,&amp;QWP(16*9,&quot;esp&quot;));
+
+&amp;set_label(&quot;long_tail&quot;);
+
+	&amp;movdqa		(&amp;QWP(16*0,&quot;eax&quot;),$T0);
+	&amp;movdqa		(&amp;QWP(16*1,&quot;eax&quot;),$T1);
+	&amp;movdqa		(&amp;QWP(16*2,&quot;eax&quot;),$D2);
+	&amp;movdqa		(&amp;QWP(16*3,&quot;eax&quot;),$D3);
+	&amp;movdqa		(&amp;QWP(16*4,&quot;eax&quot;),$D4);
+
+	################################################################
+	# d4 = h4*r0 + h3*r1   + h2*r2   + h1*r3   + h0*r4
+	# d3 = h3*r0 + h2*r1   + h1*r2   + h0*r3   + h4*5*r4
+	# d2 = h2*r0 + h1*r1   + h0*r2   + h4*5*r3 + h3*5*r4
+	# d1 = h1*r0 + h0*r1   + h4*5*r2 + h3*5*r3 + h2*5*r4
+	# d0 = h0*r0 + h4*5*r1 + h3*5*r2 + h2*5*r3 + h1*5*r4
+
+	&amp;pmuludq	($T0,$T2);			# h0*r0
+	&amp;pmuludq	($T1,$T2);			# h1*r0
+	&amp;pmuludq	($D2,$T2);			# h2*r0
+	&amp;movdqa		($D0,$T0);
+	 &amp;pshufd	($T0,&amp;QWP(16*(1-9),&quot;edx&quot;),0x10);# r1^n
+	&amp;pmuludq	($D3,$T2);			# h3*r0
+	&amp;movdqa		($D1,$T1);
+	&amp;pmuludq	($D4,$T2);			# h4*r0
+
+	&amp;pmuladd	(sub {	my ($reg,$i)=@_;
+				&amp;pshufd ($reg,&amp;QWP(16*($i-9),&quot;edx&quot;),0x10);
+			     },&quot;eax&quot;);
+
+	&amp;jz		(&amp;label(&quot;short_tail&quot;));
+
+	&amp;load_input	(-16*2,0);
+
+	 &amp;pshufd	($T2,&amp;QWP(16*0,&quot;edx&quot;),0x10);	# r0^n
+	&amp;paddd		($T0,&amp;QWP(16*5,&quot;esp&quot;));		# add hash value
+	&amp;paddd		($T1,&amp;QWP(16*6,&quot;esp&quot;));
+	&amp;paddd		($D2,&amp;QWP(16*7,&quot;esp&quot;));
+	&amp;paddd		($D3,&amp;QWP(16*8,&quot;esp&quot;));
+	&amp;paddd		($D4,&amp;QWP(16*9,&quot;esp&quot;));
+
+	################################################################
+	# multiply inp[0:1] by r^4:r^3 and accumulate
+
+	&amp;movdqa		(&amp;QWP(16*0,&quot;esp&quot;),$T0);
+	&amp;pmuludq	($T0,$T2);			# h0*r0
+	&amp;movdqa		(&amp;QWP(16*1,&quot;esp&quot;),$T1);
+	&amp;pmuludq	($T1,$T2);			# h1*r0
+	&amp;paddq		($D0,$T0);
+	&amp;movdqa		($T0,$D2);
+	&amp;pmuludq	($D2,$T2);			# h2*r0
+	&amp;paddq		($D1,$T1);
+	&amp;movdqa		($T1,$D3);
+	&amp;pmuludq	($D3,$T2);			# h3*r0
+	&amp;paddq		($D2,&amp;QWP(16*2,&quot;esp&quot;));
+	&amp;movdqa		(&amp;QWP(16*2,&quot;esp&quot;),$T0);
+	 &amp;pshufd	($T0,&amp;QWP(16*1,&quot;edx&quot;),0x10);	# r1^n
+	&amp;paddq		($D3,&amp;QWP(16*3,&quot;esp&quot;));
+	&amp;movdqa		(&amp;QWP(16*3,&quot;esp&quot;),$T1);
+	&amp;movdqa		($T1,$D4);
+	&amp;pmuludq	($D4,$T2);			# h4*r0
+	&amp;paddq		($D4,&amp;QWP(16*4,&quot;esp&quot;));
+	&amp;movdqa		(&amp;QWP(16*4,&quot;esp&quot;),$T1);
+
+	&amp;pmuladd	(sub {	my ($reg,$i)=@_;
+				&amp;pshufd ($reg,&amp;QWP(16*$i,&quot;edx&quot;),0x10);
+			     });
+
+&amp;set_label(&quot;short_tail&quot;);
+
+	&amp;lazy_reduction	();
+
+	################################################################
+	# horizontal addition
+
+	&amp;pshufd		($T1,$D0,0b01001110);
+	&amp;pshufd		($T0,$D1,0b01001110);
+	&amp;paddd		($D0,$T1);
+	&amp;pshufd		($T1,$D2,0b01001110);
+	&amp;paddd		($D1,$T0);
+	&amp;pshufd		($T0,$D3,0b01001110);
+	&amp;paddd		($D2,$T1);
+	&amp;pshufd		($T1,$D4,0b01001110);
+	&amp;paddd		($D3,$T0);
+	&amp;paddd		($D4,$T1);
+
+&amp;set_label(&quot;done&quot;);
+	&amp;movd		(&amp;DWP(-16*3+4*0,&quot;edi&quot;),$D0);	# store hash value
+	&amp;movd		(&amp;DWP(-16*3+4*1,&quot;edi&quot;),$D1);
+	&amp;movd		(&amp;DWP(-16*3+4*2,&quot;edi&quot;),$D2);
+	&amp;movd		(&amp;DWP(-16*3+4*3,&quot;edi&quot;),$D3);
+	&amp;movd		(&amp;DWP(-16*3+4*4,&quot;edi&quot;),$D4);
+&amp;set_label(&quot;nodata&quot;);
+	&amp;mov	(&quot;esp&quot;,&quot;ebp&quot;);
+&amp;function_end(&quot;_poly1305_blocks_sse2&quot;);
+
+&amp;align	(32);
+&amp;function_begin(&quot;_poly1305_emit_sse2&quot;);
+	&amp;mov	(&quot;ebp&quot;,&amp;wparam(0));		# context
+
+	&amp;cmp	(&amp;DWP(4*5,&quot;ebp&quot;),0);		# is_base2_26?
+	&amp;je	(&amp;label(&quot;enter_emit&quot;));
+
+	&amp;mov	(&quot;eax&quot;,&amp;DWP(4*0,&quot;ebp&quot;));	# load hash value
+	&amp;mov	(&quot;edi&quot;,&amp;DWP(4*1,&quot;ebp&quot;));
+	&amp;mov	(&quot;ecx&quot;,&amp;DWP(4*2,&quot;ebp&quot;));
+	&amp;mov	(&quot;edx&quot;,&amp;DWP(4*3,&quot;ebp&quot;));
+	&amp;mov	(&quot;esi&quot;,&amp;DWP(4*4,&quot;ebp&quot;));
+
+	&amp;mov	(&quot;ebx&quot;,&quot;edi&quot;);			# base 2^26 -&gt; base 2^32
+	&amp;shl	(&quot;edi&quot;,26);
+	&amp;shr	(&quot;ebx&quot;,6);
+	&amp;add	(&quot;eax&quot;,&quot;edi&quot;);
+	&amp;mov	(&quot;edi&quot;,&quot;ecx&quot;);
+	&amp;adc	(&quot;ebx&quot;,0);
+
+	&amp;shl	(&quot;edi&quot;,20);
+	&amp;shr	(&quot;ecx&quot;,12);
+	&amp;add	(&quot;ebx&quot;,&quot;edi&quot;);
+	&amp;mov	(&quot;edi&quot;,&quot;edx&quot;);
+	&amp;adc	(&quot;ecx&quot;,0);
+
+	&amp;shl	(&quot;edi&quot;,14);
+	&amp;shr	(&quot;edx&quot;,18);
+	&amp;add	(&quot;ecx&quot;,&quot;edi&quot;);
+	&amp;mov	(&quot;edi&quot;,&quot;esi&quot;);
+	&amp;adc	(&quot;edx&quot;,0);
+
+	&amp;shl	(&quot;edi&quot;,8);
+	&amp;shr	(&quot;esi&quot;,24);
+	&amp;add	(&quot;edx&quot;,&quot;edi&quot;);
+	&amp;adc	(&quot;esi&quot;,0);			# can be partially reduced
+
+	&amp;mov	(&quot;edi&quot;,&quot;esi&quot;);			# final reduction
+	&amp;and	(&quot;esi&quot;,3);
+	&amp;shr	(&quot;edi&quot;,2);
+	&amp;lea	(&quot;ebp&quot;,&amp;DWP(0,&quot;edi&quot;,&quot;edi&quot;,4));	# *5
+	 &amp;mov	(&quot;edi&quot;,&amp;wparam(1));		# output
+	add	(&quot;eax&quot;,&quot;ebp&quot;);
+	 &amp;mov	(&quot;ebp&quot;,&amp;wparam(2));		# key
+	adc	(&quot;ebx&quot;,0);
+	adc	(&quot;ecx&quot;,0);
+	adc	(&quot;edx&quot;,0);
+
+	&amp;movd	($D0,&quot;eax&quot;);			# offload original hash value
+	&amp;add	(&quot;eax&quot;,5);			# compare to modulus
+	&amp;movd	($D1,&quot;ebx&quot;);
+	&amp;adc	(&quot;ebx&quot;,0);
+	&amp;movd	($D2,&quot;ecx&quot;);
+	&amp;adc	(&quot;ecx&quot;,0);
+	&amp;movd	($D3,&quot;edx&quot;);
+	&amp;adc	(&quot;edx&quot;,0);
+	&amp;adc	(&quot;esi&quot;,0);
+	&amp;shr	(&quot;esi&quot;,2);			# did it carry/borrow?
+
+	&amp;neg	(&quot;esi&quot;);			# do we choose (hash-modulus) ...
+	&amp;and	(&quot;eax&quot;,&quot;esi&quot;);
+	&amp;and	(&quot;ebx&quot;,&quot;esi&quot;);
+	&amp;and	(&quot;ecx&quot;,&quot;esi&quot;);
+	&amp;and	(&quot;edx&quot;,&quot;esi&quot;);
+	&amp;mov	(&amp;DWP(4*0,&quot;edi&quot;),&quot;eax&quot;);
+	&amp;movd	(&quot;eax&quot;,$D0);
+	&amp;mov	(&amp;DWP(4*1,&quot;edi&quot;),&quot;ebx&quot;);
+	&amp;movd	(&quot;ebx&quot;,$D1);
+	&amp;mov	(&amp;DWP(4*2,&quot;edi&quot;),&quot;ecx&quot;);
+	&amp;movd	(&quot;ecx&quot;,$D2);
+	&amp;mov	(&amp;DWP(4*3,&quot;edi&quot;),&quot;edx&quot;);
+	&amp;movd	(&quot;edx&quot;,$D3);
+
+	&amp;not	(&quot;esi&quot;);			# ... or original hash value?
+	&amp;and	(&quot;eax&quot;,&quot;esi&quot;);
+	&amp;and	(&quot;ebx&quot;,&quot;esi&quot;);
+	&amp;or	(&quot;eax&quot;,&amp;DWP(4*0,&quot;edi&quot;));
+	&amp;and	(&quot;ecx&quot;,&quot;esi&quot;);
+	&amp;or	(&quot;ebx&quot;,&amp;DWP(4*1,&quot;edi&quot;));
+	&amp;and	(&quot;edx&quot;,&quot;esi&quot;);
+	&amp;or	(&quot;ecx&quot;,&amp;DWP(4*2,&quot;edi&quot;));
+	&amp;or	(&quot;edx&quot;,&amp;DWP(4*3,&quot;edi&quot;));
+
+	&amp;add	(&quot;eax&quot;,&amp;DWP(4*0,&quot;ebp&quot;));	# accumulate key
+	&amp;adc	(&quot;ebx&quot;,&amp;DWP(4*1,&quot;ebp&quot;));
+	&amp;mov	(&amp;DWP(4*0,&quot;edi&quot;),&quot;eax&quot;);
+	&amp;adc	(&quot;ecx&quot;,&amp;DWP(4*2,&quot;ebp&quot;));
+	&amp;mov	(&amp;DWP(4*1,&quot;edi&quot;),&quot;ebx&quot;);
+	&amp;adc	(&quot;edx&quot;,&amp;DWP(4*3,&quot;ebp&quot;));
+	&amp;mov	(&amp;DWP(4*2,&quot;edi&quot;),&quot;ecx&quot;);
+	&amp;mov	(&amp;DWP(4*3,&quot;edi&quot;),&quot;edx&quot;);
+&amp;function_end(&quot;_poly1305_emit_sse2&quot;);
+
+if ($avx&gt;1) {
+########################################################################
+# Note that poly1305_init_avx2 operates on %xmm, I could have used
+# poly1305_init_sse2...
+
+&amp;align	(32);
+&amp;function_begin_B(&quot;_poly1305_init_avx2&quot;);
+	&amp;vmovdqu	($D4,&amp;QWP(4*6,&quot;edi&quot;));		# key base 2^32
+	&amp;lea		(&quot;edi&quot;,&amp;DWP(16*3,&quot;edi&quot;));	# size optimization
+	&amp;mov		(&quot;ebp&quot;,&quot;esp&quot;);
+	&amp;sub		(&quot;esp&quot;,16*(9+5));
+	&amp;and		(&quot;esp&quot;,-16);
+
+	#&amp;vpand		($D4,$D4,&amp;QWP(96,&quot;ebx&quot;));	# magic mask
+	&amp;vmovdqa	($MASK,&amp;QWP(64,&quot;ebx&quot;));
+
+	&amp;vpand		($D0,$D4,$MASK);		# -&gt; base 2^26
+	&amp;vpsrlq		($D1,$D4,26);
+	&amp;vpsrldq	($D3,$D4,6);
+	&amp;vpand		($D1,$D1,$MASK);
+	&amp;vpsrlq		($D2,$D3,4)
+	&amp;vpsrlq		($D3,$D3,30);
+	&amp;vpand		($D2,$D2,$MASK);
+	&amp;vpand		($D3,$D3,$MASK);
+	&amp;vpsrldq	($D4,$D4,13);
+
+	&amp;lea		(&quot;edx&quot;,&amp;DWP(16*9,&quot;esp&quot;));	# size optimization
+	&amp;mov		(&quot;ecx&quot;,2);
+&amp;set_label(&quot;square&quot;);
+	&amp;vmovdqa	(&amp;QWP(16*0,&quot;esp&quot;),$D0);
+	&amp;vmovdqa	(&amp;QWP(16*1,&quot;esp&quot;),$D1);
+	&amp;vmovdqa	(&amp;QWP(16*2,&quot;esp&quot;),$D2);
+	&amp;vmovdqa	(&amp;QWP(16*3,&quot;esp&quot;),$D3);
+	&amp;vmovdqa	(&amp;QWP(16*4,&quot;esp&quot;),$D4);
+
+	&amp;vpslld		($T1,$D1,2);
+	&amp;vpslld		($T0,$D2,2);
+	&amp;vpaddd		($T1,$T1,$D1);			# *5
+	&amp;vpaddd		($T0,$T0,$D2);			# *5
+	&amp;vmovdqa	(&amp;QWP(16*5,&quot;esp&quot;),$T1);
+	&amp;vmovdqa	(&amp;QWP(16*6,&quot;esp&quot;),$T0);
+	&amp;vpslld		($T1,$D3,2);
+	&amp;vpslld		($T0,$D4,2);
+	&amp;vpaddd		($T1,$T1,$D3);			# *5
+	&amp;vpaddd		($T0,$T0,$D4);			# *5
+	&amp;vmovdqa	(&amp;QWP(16*7,&quot;esp&quot;),$T1);
+	&amp;vmovdqa	(&amp;QWP(16*8,&quot;esp&quot;),$T0);
+
+	&amp;vpshufd	($T0,$D0,0b01000100);
+	&amp;vmovdqa	($T1,$D1);
+	&amp;vpshufd	($D1,$D1,0b01000100);
+	&amp;vpshufd	($D2,$D2,0b01000100);
+	&amp;vpshufd	($D3,$D3,0b01000100);
+	&amp;vpshufd	($D4,$D4,0b01000100);
+	&amp;vmovdqa	(&amp;QWP(16*0,&quot;edx&quot;),$T0);
+	&amp;vmovdqa	(&amp;QWP(16*1,&quot;edx&quot;),$D1);
+	&amp;vmovdqa	(&amp;QWP(16*2,&quot;edx&quot;),$D2);
+	&amp;vmovdqa	(&amp;QWP(16*3,&quot;edx&quot;),$D3);
+	&amp;vmovdqa	(&amp;QWP(16*4,&quot;edx&quot;),$D4);
+
+	################################################################
+	# d4 = h4*r0 + h3*r1   + h2*r2   + h1*r3   + h0*r4
+	# d3 = h3*r0 + h2*r1   + h1*r2   + h0*r3   + h4*5*r4
+	# d2 = h2*r0 + h1*r1   + h0*r2   + h4*5*r3 + h3*5*r4
+	# d1 = h1*r0 + h0*r1   + h4*5*r2 + h3*5*r3 + h2*5*r4
+	# d0 = h0*r0 + h4*5*r1 + h3*5*r2 + h2*5*r3 + h1*5*r4
+
+	&amp;vpmuludq	($D4,$D4,$D0);			# h4*r0
+	&amp;vpmuludq	($D3,$D3,$D0);			# h3*r0
+	&amp;vpmuludq	($D2,$D2,$D0);			# h2*r0
+	&amp;vpmuludq	($D1,$D1,$D0);			# h1*r0
+	&amp;vpmuludq	($D0,$T0,$D0);			# h0*r0
+
+	&amp;vpmuludq	($T0,$T1,&amp;QWP(16*3,&quot;edx&quot;));	# r1*h3
+	&amp;vpaddq		($D4,$D4,$T0);
+	&amp;vpmuludq	($T2,$T1,&amp;QWP(16*2,&quot;edx&quot;));	# r1*h2
+	&amp;vpaddq		($D3,$D3,$T2);
+	&amp;vpmuludq	($T0,$T1,&amp;QWP(16*1,&quot;edx&quot;));	# r1*h1
+	&amp;vpaddq		($D2,$D2,$T0);
+	&amp;vmovdqa	($T2,&amp;QWP(16*5,&quot;esp&quot;));		# s1
+	&amp;vpmuludq	($T1,$T1,&amp;QWP(16*0,&quot;edx&quot;));	# r1*h0
+	&amp;vpaddq		($D1,$D1,$T1);
+	 &amp;vmovdqa	($T0,&amp;QWP(16*2,&quot;esp&quot;));		# r2
+	&amp;vpmuludq	($T2,$T2,&amp;QWP(16*4,&quot;edx&quot;));	# s1*h4
+	&amp;vpaddq		($D0,$D0,$T2);
+
+	&amp;vpmuludq	($T1,$T0,&amp;QWP(16*2,&quot;edx&quot;));	# r2*h2
+	&amp;vpaddq		($D4,$D4,$T1);
+	&amp;vpmuludq	($T2,$T0,&amp;QWP(16*1,&quot;edx&quot;));	# r2*h1
+	&amp;vpaddq		($D3,$D3,$T2);
+	&amp;vmovdqa	($T1,&amp;QWP(16*6,&quot;esp&quot;));		# s2
+	&amp;vpmuludq	($T0,$T0,&amp;QWP(16*0,&quot;edx&quot;));	# r2*h0
+	&amp;vpaddq		($D2,$D2,$T0);
+	&amp;vpmuludq	($T2,$T1,&amp;QWP(16*4,&quot;edx&quot;));	# s2*h4
+	&amp;vpaddq		($D1,$D1,$T2);
+	 &amp;vmovdqa	($T0,&amp;QWP(16*3,&quot;esp&quot;));		# r3
+	&amp;vpmuludq	($T1,$T1,&amp;QWP(16*3,&quot;edx&quot;));	# s2*h3
+	&amp;vpaddq		($D0,$D0,$T1);
+
+	&amp;vpmuludq	($T2,$T0,&amp;QWP(16*1,&quot;edx&quot;));	# r3*h1
+	&amp;vpaddq		($D4,$D4,$T2);
+	&amp;vmovdqa	($T1,&amp;QWP(16*7,&quot;esp&quot;));		# s3
+	&amp;vpmuludq	($T0,$T0,&amp;QWP(16*0,&quot;edx&quot;));	# r3*h0
+	&amp;vpaddq		($D3,$D3,$T0);
+	&amp;vpmuludq	($T2,$T1,&amp;QWP(16*4,&quot;edx&quot;));	# s3*h4
+	&amp;vpaddq		($D2,$D2,$T2);
+	&amp;vpmuludq	($T0,$T1,&amp;QWP(16*3,&quot;edx&quot;));	# s3*h3
+	&amp;vpaddq		($D1,$D1,$T0);
+	 &amp;vmovdqa	($T2,&amp;QWP(16*4,&quot;esp&quot;));		# r4
+	&amp;vpmuludq	($T1,$T1,&amp;QWP(16*2,&quot;edx&quot;));	# s3*h2
+	&amp;vpaddq		($D0,$D0,$T1);
+
+	&amp;vmovdqa	($T0,&amp;QWP(16*8,&quot;esp&quot;));		# s4
+	&amp;vpmuludq	($T2,$T2,&amp;QWP(16*0,&quot;edx&quot;));	# r4*h0
+	&amp;vpaddq		($D4,$D4,$T2);
+	&amp;vpmuludq	($T1,$T0,&amp;QWP(16*4,&quot;edx&quot;));	# s4*h4
+	&amp;vpaddq		($D3,$D3,$T1);
+	&amp;vpmuludq	($T2,$T0,&amp;QWP(16*1,&quot;edx&quot;));	# s4*h1
+	&amp;vpaddq		($D0,$D0,$T2);
+	&amp;vpmuludq	($T1,$T0,&amp;QWP(16*2,&quot;edx&quot;));	# s4*h2
+	&amp;vpaddq		($D1,$D1,$T1);
+	 &amp;vmovdqa	($MASK,&amp;QWP(64,&quot;ebx&quot;));
+	&amp;vpmuludq	($T0,$T0,&amp;QWP(16*3,&quot;edx&quot;));	# s4*h3
+	&amp;vpaddq		($D2,$D2,$T0);
+
+	################################################################
+	# lazy reduction
+	 &amp;vpsrlq	($T0,$D3,26);
+	 &amp;vpand		($D3,$D3,$MASK);
+	&amp;vpsrlq		($T1,$D0,26);
+	&amp;vpand		($D0,$D0,$MASK);
+	 &amp;vpaddq	($D4,$D4,$T0);			# h3 -&gt; h4
+	&amp;vpaddq		($D1,$D1,$T1);			# h0 -&gt; h1
+	 &amp;vpsrlq	($T0,$D4,26);
+	 &amp;vpand		($D4,$D4,$MASK);
+	&amp;vpsrlq		($T1,$D1,26);
+	&amp;vpand		($D1,$D1,$MASK);
+	&amp;vpaddq		($D2,$D2,$T1);			# h1 -&gt; h2
+	 &amp;vpaddd	($D0,$D0,$T0);
+	 &amp;vpsllq	($T0,$T0,2);
+	&amp;vpsrlq		($T1,$D2,26);
+	&amp;vpand		($D2,$D2,$MASK);
+	 &amp;vpaddd	($D0,$D0,$T0);			# h4 -&gt; h0
+	&amp;vpaddd		($D3,$D3,$T1);			# h2 -&gt; h3
+	&amp;vpsrlq		($T1,$D3,26);
+	 &amp;vpsrlq	($T0,$D0,26);
+	 &amp;vpand		($D0,$D0,$MASK);
+	&amp;vpand		($D3,$D3,$MASK);
+	 &amp;vpaddd	($D1,$D1,$T0);			# h0 -&gt; h1
+	&amp;vpaddd		($D4,$D4,$T1);			# h3 -&gt; h4
+
+	&amp;dec		(&quot;ecx&quot;);
+	&amp;jz		(&amp;label(&quot;square_break&quot;));
+
+	&amp;vpunpcklqdq	($D0,$D0,&amp;QWP(16*0,&quot;esp&quot;));	# 0:r^1:0:r^2
+	&amp;vpunpcklqdq	($D1,$D1,&amp;QWP(16*1,&quot;esp&quot;));
+	&amp;vpunpcklqdq	($D2,$D2,&amp;QWP(16*2,&quot;esp&quot;));
+	&amp;vpunpcklqdq	($D3,$D3,&amp;QWP(16*3,&quot;esp&quot;));
+	&amp;vpunpcklqdq	($D4,$D4,&amp;QWP(16*4,&quot;esp&quot;));
+	&amp;jmp		(&amp;label(&quot;square&quot;));
+
+&amp;set_label(&quot;square_break&quot;);
+	&amp;vpsllq		($D0,$D0,32);			# -&gt; r^3:0:r^4:0
+	&amp;vpsllq		($D1,$D1,32);
+	&amp;vpsllq		($D2,$D2,32);
+	&amp;vpsllq		($D3,$D3,32);
+	&amp;vpsllq		($D4,$D4,32);
+	&amp;vpor		($D0,$D0,&amp;QWP(16*0,&quot;esp&quot;));	# r^3:r^1:r^4:r^2
+	&amp;vpor		($D1,$D1,&amp;QWP(16*1,&quot;esp&quot;));
+	&amp;vpor		($D2,$D2,&amp;QWP(16*2,&quot;esp&quot;));
+	&amp;vpor		($D3,$D3,&amp;QWP(16*3,&quot;esp&quot;));
+	&amp;vpor		($D4,$D4,&amp;QWP(16*4,&quot;esp&quot;));
+
+	&amp;vpshufd	($D0,$D0,0b10001101);		# -&gt; r^1:r^2:r^3:r^4
+	&amp;vpshufd	($D1,$D1,0b10001101);
+	&amp;vpshufd	($D2,$D2,0b10001101);
+	&amp;vpshufd	($D3,$D3,0b10001101);
+	&amp;vpshufd	($D4,$D4,0b10001101);
+
+	&amp;vmovdqu	(&amp;QWP(16*0,&quot;edi&quot;),$D0);		# save the table
+	&amp;vmovdqu	(&amp;QWP(16*1,&quot;edi&quot;),$D1);
+	&amp;vmovdqu	(&amp;QWP(16*2,&quot;edi&quot;),$D2);
+	&amp;vmovdqu	(&amp;QWP(16*3,&quot;edi&quot;),$D3);
+	&amp;vmovdqu	(&amp;QWP(16*4,&quot;edi&quot;),$D4);
+
+	&amp;vpslld		($T1,$D1,2);
+	&amp;vpslld		($T0,$D2,2);
+	&amp;vpaddd		($T1,$T1,$D1);			# *5
+	&amp;vpaddd		($T0,$T0,$D2);			# *5
+	&amp;vmovdqu	(&amp;QWP(16*5,&quot;edi&quot;),$T1);
+	&amp;vmovdqu	(&amp;QWP(16*6,&quot;edi&quot;),$T0);
+	&amp;vpslld		($T1,$D3,2);
+	&amp;vpslld		($T0,$D4,2);
+	&amp;vpaddd		($T1,$T1,$D3);			# *5
+	&amp;vpaddd		($T0,$T0,$D4);			# *5
+	&amp;vmovdqu	(&amp;QWP(16*7,&quot;edi&quot;),$T1);
+	&amp;vmovdqu	(&amp;QWP(16*8,&quot;edi&quot;),$T0);
+
+	&amp;mov		(&quot;esp&quot;,&quot;ebp&quot;);
+	&amp;lea		(&quot;edi&quot;,&amp;DWP(-16*3,&quot;edi&quot;));	# size de-optimization
+	&amp;ret		();
+&amp;function_end_B(&quot;_poly1305_init_avx2&quot;);
+
+########################################################################
+# now it's time to switch to %ymm
+
+my ($D0,$D1,$D2,$D3,$D4,$T0,$T1,$T2)=map(&quot;ymm$_&quot;,(0..7));
+my $MASK=$T2;
+
+sub X { my $reg=shift; $reg=~s/^ymm/xmm/; $reg; }
+
+&amp;align	(32);
+&amp;function_begin(&quot;_poly1305_blocks_avx2&quot;);
+	&amp;mov	(&quot;edi&quot;,&amp;wparam(0));			# ctx
+	&amp;mov	(&quot;esi&quot;,&amp;wparam(1));			# inp
+	&amp;mov	(&quot;ecx&quot;,&amp;wparam(2));			# len
+
+	&amp;mov	(&quot;eax&quot;,&amp;DWP(4*5,&quot;edi&quot;));		# is_base2_26
+	&amp;and	(&quot;ecx&quot;,-16);
+	&amp;jz	(&amp;label(&quot;nodata&quot;));
+	&amp;cmp	(&quot;ecx&quot;,64);
+	&amp;jae	(&amp;label(&quot;enter_avx2&quot;));
+	&amp;test	(&quot;eax&quot;,&quot;eax&quot;);				# is_base2_26?
+	&amp;jz	(&amp;label(&quot;enter_blocks&quot;));
+
+&amp;set_label(&quot;enter_avx2&quot;,16);
+	&amp;vzeroupper	();
+
+	&amp;call	(&amp;label(&quot;pic_point&quot;));
+&amp;set_label(&quot;pic_point&quot;);
+	&amp;blindpop(&quot;ebx&quot;);
+	&amp;lea	(&quot;ebx&quot;,&amp;DWP(&amp;label(&quot;const_sse2&quot;).&quot;-&quot;.&amp;label(&quot;pic_point&quot;),&quot;ebx&quot;));
+
+	&amp;test	(&quot;eax&quot;,&quot;eax&quot;);				# is_base2_26?
+	&amp;jnz	(&amp;label(&quot;base2_26&quot;));
+
+	&amp;call	(&quot;_poly1305_init_avx2&quot;);
+
+	################################################# base 2^32 -&gt; base 2^26
+	&amp;mov	(&quot;eax&quot;,&amp;DWP(0,&quot;edi&quot;));
+	&amp;mov	(&quot;ecx&quot;,&amp;DWP(3,&quot;edi&quot;));
+	&amp;mov	(&quot;edx&quot;,&amp;DWP(6,&quot;edi&quot;));
+	&amp;mov	(&quot;esi&quot;,&amp;DWP(9,&quot;edi&quot;));
+	&amp;mov	(&quot;ebp&quot;,&amp;DWP(13,&quot;edi&quot;));
+
+	&amp;shr	(&quot;ecx&quot;,2);
+	&amp;and	(&quot;eax&quot;,0x3ffffff);
+	&amp;shr	(&quot;edx&quot;,4);
+	&amp;and	(&quot;ecx&quot;,0x3ffffff);
+	&amp;shr	(&quot;esi&quot;,6);
+	&amp;and	(&quot;edx&quot;,0x3ffffff);
+
+	&amp;mov	(&amp;DWP(4*0,&quot;edi&quot;),&quot;eax&quot;);
+	&amp;mov	(&amp;DWP(4*1,&quot;edi&quot;),&quot;ecx&quot;);
+	&amp;mov	(&amp;DWP(4*2,&quot;edi&quot;),&quot;edx&quot;);
+	&amp;mov	(&amp;DWP(4*3,&quot;edi&quot;),&quot;esi&quot;);
+	&amp;mov	(&amp;DWP(4*4,&quot;edi&quot;),&quot;ebp&quot;);
+	&amp;mov	(&amp;DWP(4*5,&quot;edi&quot;),1);			# is_base2_26
+
+	&amp;mov	(&quot;esi&quot;,&amp;wparam(1));			# [reload] inp
+	&amp;mov	(&quot;ecx&quot;,&amp;wparam(2));			# [reload] len
+
+&amp;set_label(&quot;base2_26&quot;);
+	&amp;mov	(&quot;eax&quot;,&amp;wparam(3));			# padbit
+	&amp;mov	(&quot;ebp&quot;,&quot;esp&quot;);
+
+	&amp;sub	(&quot;esp&quot;,32*(5+9));
+	&amp;and	(&quot;esp&quot;,-512);				# ensure that frame
+							# doesn't cross page
+							# boundary, which is
+							# essential for
+							# misaligned 32-byte
+							# loads
+
+	################################################################
+        # expand and copy pre-calculated table to stack
+
+	&amp;vmovdqu	(&amp;X($D0),&amp;QWP(16*(3+0),&quot;edi&quot;));
+	&amp;lea		(&quot;edx&quot;,&amp;DWP(32*5+128,&quot;esp&quot;));	# +128 size optimization
+	&amp;vmovdqu	(&amp;X($D1),&amp;QWP(16*(3+1),&quot;edi&quot;));
+	&amp;vmovdqu	(&amp;X($D2),&amp;QWP(16*(3+2),&quot;edi&quot;));
+	&amp;vmovdqu	(&amp;X($D3),&amp;QWP(16*(3+3),&quot;edi&quot;));
+	&amp;vmovdqu	(&amp;X($D4),&amp;QWP(16*(3+4),&quot;edi&quot;));
+	&amp;lea		(&quot;edi&quot;,&amp;DWP(16*3,&quot;edi&quot;));	# size optimization
+	&amp;vpermq		($D0,$D0,0b01000000);		# 00001234 -&gt; 12343434
+	&amp;vpermq		($D1,$D1,0b01000000);
+	&amp;vpermq		($D2,$D2,0b01000000);
+	&amp;vpermq		($D3,$D3,0b01000000);
+	&amp;vpermq		($D4,$D4,0b01000000);
+	&amp;vpshufd	($D0,$D0,0b11001000);		# 12343434 -&gt; 14243444
+	&amp;vpshufd	($D1,$D1,0b11001000);
+	&amp;vpshufd	($D2,$D2,0b11001000);
+	&amp;vpshufd	($D3,$D3,0b11001000);
+	&amp;vpshufd	($D4,$D4,0b11001000);
+	&amp;vmovdqa	(&amp;QWP(32*0-128,&quot;edx&quot;),$D0);
+	&amp;vmovdqu	(&amp;X($D0),&amp;QWP(16*5,&quot;edi&quot;));
+	&amp;vmovdqa	(&amp;QWP(32*1-128,&quot;edx&quot;),$D1);
+	&amp;vmovdqu	(&amp;X($D1),&amp;QWP(16*6,&quot;edi&quot;));
+	&amp;vmovdqa	(&amp;QWP(32*2-128,&quot;edx&quot;),$D2);
+	&amp;vmovdqu	(&amp;X($D2),&amp;QWP(16*7,&quot;edi&quot;));
+	&amp;vmovdqa	(&amp;QWP(32*3-128,&quot;edx&quot;),$D3);
+	&amp;vmovdqu	(&amp;X($D3),&amp;QWP(16*8,&quot;edi&quot;));
+	&amp;vmovdqa	(&amp;QWP(32*4-128,&quot;edx&quot;),$D4);
+	&amp;vpermq		($D0,$D0,0b01000000);
+	&amp;vpermq		($D1,$D1,0b01000000);
+	&amp;vpermq		($D2,$D2,0b01000000);
+	&amp;vpermq		($D3,$D3,0b01000000);
+	&amp;vpshufd	($D0,$D0,0b11001000);
+	&amp;vpshufd	($D1,$D1,0b11001000);
+	&amp;vpshufd	($D2,$D2,0b11001000);
+	&amp;vpshufd	($D3,$D3,0b11001000);
+	&amp;vmovdqa	(&amp;QWP(32*5-128,&quot;edx&quot;),$D0);
+	&amp;vmovd		(&amp;X($D0),&amp;DWP(-16*3+4*0,&quot;edi&quot;));# load hash value
+	&amp;vmovdqa	(&amp;QWP(32*6-128,&quot;edx&quot;),$D1);
+	&amp;vmovd		(&amp;X($D1),&amp;DWP(-16*3+4*1,&quot;edi&quot;));
+	&amp;vmovdqa	(&amp;QWP(32*7-128,&quot;edx&quot;),$D2);
+	&amp;vmovd		(&amp;X($D2),&amp;DWP(-16*3+4*2,&quot;edi&quot;));
+	&amp;vmovdqa	(&amp;QWP(32*8-128,&quot;edx&quot;),$D3);
+	&amp;vmovd		(&amp;X($D3),&amp;DWP(-16*3+4*3,&quot;edi&quot;));
+	&amp;vmovd		(&amp;X($D4),&amp;DWP(-16*3+4*4,&quot;edi&quot;));
+	&amp;vmovdqa	($MASK,&amp;QWP(64,&quot;ebx&quot;));
+	&amp;neg		(&quot;eax&quot;);			# padbit
+
+	&amp;test		(&quot;ecx&quot;,63);
+	&amp;jz		(&amp;label(&quot;even&quot;));
+
+	&amp;mov		(&quot;edx&quot;,&quot;ecx&quot;);
+	&amp;and		(&quot;ecx&quot;,-64);
+	&amp;and		(&quot;edx&quot;,63);
+
+	&amp;vmovdqu	(&amp;X($T0),&amp;QWP(16*0,&quot;esi&quot;));
+	&amp;cmp		(&quot;edx&quot;,32);
+	&amp;jb		(&amp;label(&quot;one&quot;));
+
+	&amp;vmovdqu	(&amp;X($T1),&amp;QWP(16*1,&quot;esi&quot;));
+	&amp;je		(&amp;label(&quot;two&quot;));
+
+	&amp;vinserti128	($T0,$T0,&amp;QWP(16*2,&quot;esi&quot;),1);
+	&amp;lea		(&quot;esi&quot;,&amp;DWP(16*3,&quot;esi&quot;));
+	&amp;lea		(&quot;ebx&quot;,&amp;DWP(8,&quot;ebx&quot;));		# three padbits
+	&amp;lea		(&quot;edx&quot;,&amp;DWP(32*5+128+8,&quot;esp&quot;));	# --:r^1:r^2:r^3 (*)
+	&amp;jmp		(&amp;label(&quot;tail&quot;));
+
+&amp;set_label(&quot;two&quot;);
+	&amp;lea		(&quot;esi&quot;,&amp;DWP(16*2,&quot;esi&quot;));
+	&amp;lea		(&quot;ebx&quot;,&amp;DWP(16,&quot;ebx&quot;));		# two padbits
+	&amp;lea		(&quot;edx&quot;,&amp;DWP(32*5+128+16,&quot;esp&quot;));# --:--:r^1:r^2 (*)
+	&amp;jmp		(&amp;label(&quot;tail&quot;));
+
+&amp;set_label(&quot;one&quot;);
+	&amp;lea		(&quot;esi&quot;,&amp;DWP(16*1,&quot;esi&quot;));
+	&amp;vpxor		($T1,$T1,$T1);
+	&amp;lea		(&quot;ebx&quot;,&amp;DWP(32,&quot;ebx&quot;,&quot;eax&quot;,8));	# one or no padbits
+	&amp;lea		(&quot;edx&quot;,&amp;DWP(32*5+128+24,&quot;esp&quot;));# --:--:--:r^1 (*)
+	&amp;jmp		(&amp;label(&quot;tail&quot;));
+
+# (*)	spots marked with '--' are data from next table entry, but they
+#	are multiplied by 0 and therefore rendered insignificant
+
+&amp;set_label(&quot;even&quot;,32);
+	&amp;vmovdqu	(&amp;X($T0),&amp;QWP(16*0,&quot;esi&quot;));	# load input
+	&amp;vmovdqu	(&amp;X($T1),&amp;QWP(16*1,&quot;esi&quot;));
+	&amp;vinserti128	($T0,$T0,&amp;QWP(16*2,&quot;esi&quot;),1);
+	&amp;vinserti128	($T1,$T1,&amp;QWP(16*3,&quot;esi&quot;),1);
+	&amp;lea		(&quot;esi&quot;,&amp;DWP(16*4,&quot;esi&quot;));
+	&amp;sub		(&quot;ecx&quot;,64);
+	&amp;jz		(&amp;label(&quot;tail&quot;));
+
+&amp;set_label(&quot;loop&quot;);
+	################################################################
+	# ((inp[0]*r^4+r[4])*r^4+r[8])*r^4
+	# ((inp[1]*r^4+r[5])*r^4+r[9])*r^3
+	# ((inp[2]*r^4+r[6])*r^4+r[10])*r^2
+	# ((inp[3]*r^4+r[7])*r^4+r[11])*r^1
+	#   \________/ \_______/
+	################################################################
+
+sub vsplat_input {
+	&amp;vmovdqa	(&amp;QWP(32*2,&quot;esp&quot;),$D2);
+	&amp;vpsrldq	($D2,$T0,6);			# splat input
+	&amp;vmovdqa	(&amp;QWP(32*0,&quot;esp&quot;),$D0);
+	&amp;vpsrldq	($D0,$T1,6);
+	&amp;vmovdqa	(&amp;QWP(32*1,&quot;esp&quot;),$D1);
+	&amp;vpunpckhqdq	($D1,$T0,$T1);			# 4
+	&amp;vpunpcklqdq	($T0,$T0,$T1);			# 0:1
+	&amp;vpunpcklqdq	($D2,$D2,$D0);			# 2:3
+
+	&amp;vpsrlq		($D0,$D2,30);
+	&amp;vpsrlq		($D2,$D2,4);
+	&amp;vpsrlq		($T1,$T0,26);
+	&amp;vpsrlq		($D1,$D1,40);			# 4
+	&amp;vpand		($D2,$D2,$MASK);		# 2
+	&amp;vpand		($T0,$T0,$MASK);		# 0
+	&amp;vpand		($T1,$T1,$MASK);		# 1
+	&amp;vpand		($D0,$D0,$MASK);		# 3 (*)
+	&amp;vpor		($D1,$D1,&amp;QWP(0,&quot;ebx&quot;));	# padbit, yes, always
+
+	# (*)	note that output is counterintuitive, inp[3:4] is
+	#	returned in $D1-2, while $D3-4 are preserved;
+}
+	&amp;vsplat_input	();
+
+sub vpmuladd {
+my $addr = shift;
+
+	&amp;vpaddq		($D2,$D2,&amp;QWP(32*2,&quot;esp&quot;));	# add hash value
+	&amp;vpaddq		($T0,$T0,&amp;QWP(32*0,&quot;esp&quot;));
+	&amp;vpaddq		($T1,$T1,&amp;QWP(32*1,&quot;esp&quot;));
+	&amp;vpaddq		($D0,$D0,$D3);
+	&amp;vpaddq		($D1,$D1,$D4);
+
+	################################################################
+	# d3 = h2*r1   + h0*r3 + h1*r2   + h3*r0   + h4*5*r4
+	# d4 = h2*r2   + h0*r4 + h1*r3   + h3*r1   + h4*r0
+	# d0 = h2*5*r3 + h0*r0 + h1*5*r4 + h3*5*r2 + h4*5*r1
+	# d1 = h2*5*r4 + h0*r1 + h1*r0   + h3*5*r3 + h4*5*r2
+	# d2 = h2*r0   + h0*r2 + h1*r1   + h3*5*r4 + h4*5*r3
+
+	&amp;vpmuludq	($D3,$D2,&amp;$addr(1));		# d3 = h2*r1
+	 &amp;vmovdqa	(QWP(32*1,&quot;esp&quot;),$T1);
+	&amp;vpmuludq	($D4,$D2,&amp;$addr(2));		# d4 = h2*r2
+	 &amp;vmovdqa	(QWP(32*3,&quot;esp&quot;),$D0);
+	&amp;vpmuludq	($D0,$D2,&amp;$addr(7));		# d0 = h2*s3
+	 &amp;vmovdqa	(QWP(32*4,&quot;esp&quot;),$D1);
+	&amp;vpmuludq	($D1,$D2,&amp;$addr(8));		# d1 = h2*s4
+	&amp;vpmuludq	($D2,$D2,&amp;$addr(0));		# d2 = h2*r0
+
+	&amp;vpmuludq	($T2,$T0,&amp;$addr(3));		# h0*r3
+	&amp;vpaddq		($D3,$D3,$T2);			# d3 += h0*r3
+	&amp;vpmuludq	($T1,$T0,&amp;$addr(4));		# h0*r4
+	&amp;vpaddq		($D4,$D4,$T1);			# d4 + h0*r4
+	&amp;vpmuludq	($T2,$T0,&amp;$addr(0));		# h0*r0
+	&amp;vpaddq		($D0,$D0,$T2);			# d0 + h0*r0
+	 &amp;vmovdqa	($T2,&amp;QWP(32*1,&quot;esp&quot;));		# h1
+	&amp;vpmuludq	($T1,$T0,&amp;$addr(1));		# h0*r1
+	&amp;vpaddq		($D1,$D1,$T1);			# d1 += h0*r1
+	&amp;vpmuludq	($T0,$T0,&amp;$addr(2));		# h0*r2
+	&amp;vpaddq		($D2,$D2,$T0);			# d2 += h0*r2
+
+	&amp;vpmuludq	($T1,$T2,&amp;$addr(2));		# h1*r2
+	&amp;vpaddq		($D3,$D3,$T1);			# d3 += h1*r2
+	&amp;vpmuludq	($T0,$T2,&amp;$addr(3));		# h1*r3
+	&amp;vpaddq		($D4,$D4,$T0);			# d4 += h1*r3
+	&amp;vpmuludq	($T1,$T2,&amp;$addr(8));		# h1*s4
+	&amp;vpaddq		($D0,$D0,$T1);			# d0 += h1*s4
+	 &amp;vmovdqa	($T1,&amp;QWP(32*3,&quot;esp&quot;));		# h3
+	&amp;vpmuludq	($T0,$T2,&amp;$addr(0));		# h1*r0
+	&amp;vpaddq		($D1,$D1,$T0);			# d1 += h1*r0
+	&amp;vpmuludq	($T2,$T2,&amp;$addr(1));		# h1*r1
+	&amp;vpaddq		($D2,$D2,$T2);			# d2 += h1*r1
+
+	&amp;vpmuludq	($T0,$T1,&amp;$addr(0));		# h3*r0
+	&amp;vpaddq		($D3,$D3,$T0);			# d3 += h3*r0
+	&amp;vpmuludq	($T2,$T1,&amp;$addr(1));		# h3*r1
+	&amp;vpaddq		($D4,$D4,$T2);			# d4 += h3*r1
+	&amp;vpmuludq	($T0,$T1,&amp;$addr(6));		# h3*s2
+	&amp;vpaddq		($D0,$D0,$T0);			# d0 += h3*s2
+	 &amp;vmovdqa	($T0,&amp;QWP(32*4,&quot;esp&quot;));		# h4
+	&amp;vpmuludq	($T2,$T1,&amp;$addr(7));		# h3*s3
+	&amp;vpaddq		($D1,$D1,$T2);			# d1+= h3*s3
+	&amp;vpmuludq	($T1,$T1,&amp;$addr(8));		# h3*s4
+	&amp;vpaddq		($D2,$D2,$T1);			# d2 += h3*s4
+
+	&amp;vpmuludq	($T2,$T0,&amp;$addr(8));		# h4*s4
+	&amp;vpaddq		($D3,$D3,$T2);			# d3 += h4*s4
+	&amp;vpmuludq	($T1,$T0,&amp;$addr(5));		# h4*s1
+	&amp;vpaddq		($D0,$D0,$T1);			# d0 += h4*s1
+	&amp;vpmuludq	($T2,$T0,&amp;$addr(0));		# h4*r0
+	&amp;vpaddq		($D4,$D4,$T2);			# d4 += h4*r0
+	 &amp;vmovdqa	($MASK,&amp;QWP(64,&quot;ebx&quot;));
+	&amp;vpmuludq	($T1,$T0,&amp;$addr(6));		# h4*s2
+	&amp;vpaddq		($D1,$D1,$T1);			# d1 += h4*s2
+	&amp;vpmuludq	($T0,$T0,&amp;$addr(7));		# h4*s3
+	&amp;vpaddq		($D2,$D2,$T0);			# d2 += h4*s3
+}
+	&amp;vpmuladd	(sub {	my $i=shift; &amp;QWP(32*$i-128,&quot;edx&quot;);	});
+
+sub vlazy_reduction {
+	################################################################
+	# lazy reduction
+
+	 &amp;vpsrlq	($T0,$D3,26);
+	 &amp;vpand		($D3,$D3,$MASK);
+	&amp;vpsrlq		($T1,$D0,26);
+	&amp;vpand		($D0,$D0,$MASK);
+	 &amp;vpaddq	($D4,$D4,$T0);			# h3 -&gt; h4
+	&amp;vpaddq		($D1,$D1,$T1);			# h0 -&gt; h1
+	 &amp;vpsrlq	($T0,$D4,26);
+	 &amp;vpand		($D4,$D4,$MASK);
+	&amp;vpsrlq		($T1,$D1,26);
+	&amp;vpand		($D1,$D1,$MASK);
+	&amp;vpaddq		($D2,$D2,$T1);			# h1 -&gt; h2
+	 &amp;vpaddd	($D0,$D0,$T0);
+	 &amp;vpsllq	($T0,$T0,2);
+	&amp;vpsrlq		($T1,$D2,26);
+	&amp;vpand		($D2,$D2,$MASK);
+	 &amp;vpaddd	($D0,$D0,$T0);			# h4 -&gt; h0
+	&amp;vpaddd		($D3,$D3,$T1);			# h2 -&gt; h3
+	&amp;vpsrlq		($T1,$D3,26);
+	 &amp;vpsrlq	($T0,$D0,26);
+	 &amp;vpand		($D0,$D0,$MASK);
+	&amp;vpand		($D3,$D3,$MASK);
+	 &amp;vpaddd	($D1,$D1,$T0);			# h0 -&gt; h1
+	&amp;vpaddd		($D4,$D4,$T1);			# h3 -&gt; h4
+}
+	&amp;vlazy_reduction();
+
+	&amp;vmovdqu	(&amp;X($T0),&amp;QWP(16*0,&quot;esi&quot;));	# load input
+	&amp;vmovdqu	(&amp;X($T1),&amp;QWP(16*1,&quot;esi&quot;));
+	&amp;vinserti128	($T0,$T0,&amp;QWP(16*2,&quot;esi&quot;),1);
+	&amp;vinserti128	($T1,$T1,&amp;QWP(16*3,&quot;esi&quot;),1);
+	&amp;lea		(&quot;esi&quot;,&amp;DWP(16*4,&quot;esi&quot;));
+	&amp;sub		(&quot;ecx&quot;,64);
+	&amp;jnz		(&amp;label(&quot;loop&quot;));
+
+&amp;set_label(&quot;tail&quot;);
+	&amp;vsplat_input	();
+	&amp;and		(&quot;ebx&quot;,-64);			# restore pointer
+
+	&amp;vpmuladd	(sub {	my $i=shift; &amp;QWP(4+32*$i-128,&quot;edx&quot;);	});
+
+	&amp;vlazy_reduction();
+
+	################################################################
+	# horizontal addition
+
+	&amp;vpsrldq	($T0,$D0,8);
+	&amp;vpsrldq	($T1,$D1,8);
+	&amp;vpaddq		($D0,$D0,$T0);
+	&amp;vpsrldq	($T0,$D2,8);
+	&amp;vpaddq		($D1,$D1,$T1);
+	&amp;vpsrldq	($T1,$D3,8);
+	&amp;vpaddq		($D2,$D2,$T0);
+	&amp;vpsrldq	($T0,$D4,8);
+	&amp;vpaddq		($D3,$D3,$T1);
+	&amp;vpermq		($T1,$D0,2);			# keep folding
+	&amp;vpaddq		($D4,$D4,$T0);
+	&amp;vpermq		($T0,$D1,2);
+	&amp;vpaddq		($D0,$D0,$T1);
+	&amp;vpermq		($T1,$D2,2);
+	&amp;vpaddq		($D1,$D1,$T0);
+	&amp;vpermq		($T0,$D3,2);
+	&amp;vpaddq		($D2,$D2,$T1);
+	&amp;vpermq		($T1,$D4,2);
+	&amp;vpaddq		($D3,$D3,$T0);
+	&amp;vpaddq		($D4,$D4,$T1);
+
+	&amp;cmp		(&quot;ecx&quot;,0);
+	&amp;je		(&amp;label(&quot;done&quot;));
+
+	################################################################
+	# clear all but single word
+
+	&amp;vpshufd	(&amp;X($D0),&amp;X($D0),0b11111100);
+	&amp;lea		(&quot;edx&quot;,&amp;DWP(32*5+128,&quot;esp&quot;));	# restore pointer
+	&amp;vpshufd	(&amp;X($D1),&amp;X($D1),0b11111100);
+	&amp;vpshufd	(&amp;X($D2),&amp;X($D2),0b11111100);
+	&amp;vpshufd	(&amp;X($D3),&amp;X($D3),0b11111100);
+	&amp;vpshufd	(&amp;X($D4),&amp;X($D4),0b11111100);
+	&amp;jmp		(&amp;label(&quot;even&quot;));
+
+&amp;set_label(&quot;done&quot;,16);
+	&amp;vmovd		(&amp;DWP(-16*3+4*0,&quot;edi&quot;),&quot;xmm0&quot;);	# store hash value
+	&amp;vmovd		(&amp;DWP(-16*3+4*1,&quot;edi&quot;),&quot;xmm1&quot;);
+	&amp;vmovd		(&amp;DWP(-16*3+4*2,&quot;edi&quot;),&quot;xmm2&quot;);
+	&amp;vmovd		(&amp;DWP(-16*3+4*3,&quot;edi&quot;),&quot;xmm3&quot;);
+	&amp;vmovd		(&amp;DWP(-16*3+4*4,&quot;edi&quot;),&quot;xmm4&quot;);
+	&amp;vzeroupper	();
+&amp;set_label(&quot;nodata&quot;);
+	&amp;mov	(&quot;esp&quot;,&quot;ebp&quot;);
+&amp;function_end(&quot;_poly1305_blocks_avx2&quot;);
+}
+&amp;set_label(&quot;const_sse2&quot;,64);
+	&amp;data_word(1&lt;&lt;24,0,	1&lt;&lt;24,0,	1&lt;&lt;24,0,	1&lt;&lt;24,0);
+	&amp;data_word(0,0,		0,0,		0,0,		0,0);
+	&amp;data_word(0x03ffffff,0,0x03ffffff,0,	0x03ffffff,0,	0x03ffffff,0);
+	&amp;data_word(0x0fffffff,0x0ffffffc,0x0ffffffc,0x0ffffffc);
+}
+&amp;asciz	(&quot;Poly1305 for x86, CRYPTOGAMS by &lt;appro\@openssl.org&gt;&quot;);
+&amp;align	(4);
+
+&amp;asm_finish();
diff --git a/crypto/poly1305/asm/poly1305-x86_64.pl b/crypto/poly1305/asm/poly1305-x86_64.pl
new file mode 100755
index 0000000..d991365
--- /dev/null
+++ b/crypto/poly1305/asm/poly1305-x86_64.pl
@@ -0,0 +1,2244 @@
+#!/usr/bin/env perl
+#
+# ====================================================================
+# Written by Andy Polyakov &lt;<A HREF="../../../mailman/listinfo/openssl-commits.html">appro at openssl.org</A>&gt; for the OpenSSL
+# project. The module is, however, dual licensed under OpenSSL and
+# CRYPTOGAMS licenses depending on where you obtain it. For further
+# details see <A HREF="http://www.openssl.org/~appro/cryptogams/.">http://www.openssl.org/~appro/cryptogams/.</A>
+# ====================================================================
+#
+# This module implements Poly1305 hash for x86_64.
+#
+# March 2015
+#
+# Numbers are cycles per processed byte with poly1305_blocks alone,
+# measured with rdtsc at fixed clock frequency.
+#
+#		IALU/gcc-4.8(*)	AVX(**)		AVX2
+# P4		4.90/+120%      -
+# Core 2	2.39/+90%	-
+# Westmere	1.86/+120%	-
+# Sandy Bridge	1.39/+140%	1.10
+# Haswell	1.10/+175%	1.11		0.65
+# Skylake	1.12/+120%	0.96		0.51
+# Silvermont	2.83/+95%	-
+# VIA Nano	1.82/+150%	-
+# Sledgehammer	1.38/+160%	-
+# Bulldozer	2.21/+130%	0.97
+#
+# (*)	improvement coefficients relative to clang are more modest and
+#	are ~50% on most processors, in both cases we are comparing to
+#	__int128 code;
+# (**)	SSE2 implementation was attempted, but among non-AVX processors
+#	it was faster than integer-only code only on older Intel P4 and
+#	Core processors, 50-30%, less newer processor is, but slower on
+#	contemporary ones, for example almost 2x slower on Atom, and as
+#	former are naturally disappearing, SSE2 is deemed unnecessary;
+
+$flavour = shift;
+$output  = shift;
+if ($flavour =~ /\./) { $output = $flavour; undef $flavour; }
+
+$win64=0; $win64=1 if ($flavour =~ /[nm]asm|mingw64/ || $output =~ /\.asm$/);
+
+$0 =~ m/(.*[\/\\])[^\/\\]+$/; $dir=$1;
+( $xlate=&quot;${dir}x86_64-xlate.pl&quot; and -f $xlate ) or
+( $xlate=&quot;${dir}../../perlasm/x86_64-xlate.pl&quot; and -f $xlate) or
+die &quot;can't locate x86_64-xlate.pl&quot;;
+
+if (`$ENV{CC} -Wa,-v -c -o /dev/null -x assembler /dev/null 2&gt;&amp;1`
+		=~ /GNU assembler version ([2-9]\.[0-9]+)/) {
+	$avx = ($1&gt;=2.19) + ($1&gt;=2.22);
+}
+
+if (!$avx &amp;&amp; $win64 &amp;&amp; ($flavour =~ /nasm/ || $ENV{ASM} =~ /nasm/) &amp;&amp;
+	   `nasm -v 2&gt;&amp;1` =~ /NASM version ([2-9]\.[0-9]+)/) {
+	$avx = ($1&gt;=2.09) + ($1&gt;=2.10);
+}
+
+if (!$avx &amp;&amp; $win64 &amp;&amp; ($flavour =~ /masm/ || $ENV{ASM} =~ /ml64/) &amp;&amp;
+	   `ml64 2&gt;&amp;1` =~ /Version ([0-9]+)\./) {
+	$avx = ($1&gt;=10) + ($1&gt;=12);
+}
+
+if (!$avx &amp;&amp; `$ENV{CC} -v 2&gt;&amp;1` =~ /((?:^clang|LLVM) version|.*based on LLVM) ([3-9]\.[0-9]+)/) {
+	$avx = ($2&gt;=3.0) + ($2&gt;3.0);
+}
+
+open OUT,&quot;| \&quot;$^X\&quot; $xlate $flavour $output&quot;;
+*STDOUT=*OUT;
+
+my ($ctx,$inp,$len,$padbit)=(&quot;%rdi&quot;,&quot;%rsi&quot;,&quot;%rdx&quot;,&quot;%rcx&quot;);
+my ($mac,$nonce)=($inp,$len);	# *_emit arguments
+my ($d1,$d2,$d3, $r0,$r1,$s1)=map(&quot;%r$_&quot;,(8..13));
+my ($h0,$h1,$h2)=(&quot;%r14&quot;,&quot;%rbx&quot;,&quot;%rbp&quot;);
+
+sub poly1305_iteration {
+# input:	copy of $r1 in %rax, $h0-$h2, $r0-$r1
+# output:	$h0-$h2 *= $r0-$r1
+$code.=&lt;&lt;___;
+	mulq	$h0			# h0*r1
+	mov	%rax,$d2
+	 mov	$r0,%rax
+	mov	%rdx,$d3
+
+	mulq	$h0			# h0*r0
+	mov	%rax,$h0		# future $h0
+	 mov	$r0,%rax
+	mov	%rdx,$d1
+
+	mulq	$h1			# h1*r0
+	add	%rax,$d2
+	 mov	$s1,%rax
+	adc	%rdx,$d3
+
+	mulq	$h1			# h1*s1
+	 mov	$h2,$h1			# borrow $h1
+	add	%rax,$h0
+	adc	%rdx,$d1
+
+	imulq	$s1,$h1			# h2*s1
+	add	$h1,$d2
+	 mov	$d1,$h1
+	adc	\$0,$d3
+
+	imulq	$r0,$h2			# h2*r0
+	add	$d2,$h1
+	mov	\$-4,%rax		# mask value
+	adc	$h2,$d3
+
+	and	$d3,%rax		# last reduction step
+	mov	$d3,$h2
+	shr	\$2,$d3
+	and	\$3,$h2
+	add	$d3,%rax
+	add	%rax,$h0
+	adc	\$0,$h1
+___
+}
+
+########################################################################
+# Layout of opaque area is following.
+#
+#	unsigned __int64 h[3];		# current hash value base 2^64
+#	unsigned __int64 r[2];		# key value base 2^64
+
+$code.=&lt;&lt;___;
+.text
+
+.extern	OPENSSL_ia32cap_P
+
+.globl	poly1305_init
+.type	poly1305_init,\@function,2
+.align	32
+poly1305_init:
+	xor	%rax,%rax
+	mov	%rax,0($ctx)		# initialize hash value
+	mov	%rax,8($ctx)
+	mov	%rax,16($ctx)
+
+	cmp	\$0,$inp
+	je	.Lno_key
+
+	lea	poly1305_blocks(%rip),%r10
+	lea	poly1305_emit(%rip),%r11
+___
+$code.=&lt;&lt;___	if ($avx);
+	mov	OPENSSL_ia32cap_P+4(%rip),%r9
+	lea	poly1305_blocks_avx(%rip),%rax
+	lea	poly1305_emit_avx(%rip),%rcx
+	bt	\$`60-32`,%r9		# AVX?
+	cmovc	%rax,%r10
+	cmovc	%rcx,%r11
+___
+$code.=&lt;&lt;___	if ($avx&gt;1);
+	lea	poly1305_blocks_avx2(%rip),%rax
+	bt	\$`5+32`,%r9		# AVX2?
+	cmovc	%rax,%r10
+___
+$code.=&lt;&lt;___;
+	mov	\$0x0ffffffc0fffffff,%rax
+	mov	\$0x0ffffffc0ffffffc,%rcx
+	and	0($inp),%rax
+	and	8($inp),%rcx
+	mov	%rax,24($ctx)
+	mov	%rcx,32($ctx)
+
+	mov	%r10,0(%rdx)
+	mov	%r11,8(%rdx)
+
+	mov	\$1,%eax
+.Lno_key:
+	ret
+.size	poly1305_init,.-poly1305_init
+
+.globl	poly1305_blocks
+.type	poly1305_blocks,\@function,4
+.align	32
+poly1305_blocks:
+	sub	\$16,$len		# too short?
+	jc	.Lno_data
+
+	push	%rbx
+	push	%rbp
+	push	%r12
+	push	%r13
+	push	%r14
+	push	%r15
+.Lblocks_body:
+
+	mov	$len,%r15		# reassign $len
+
+	mov	24($ctx),$r0		# load r
+	mov	32($ctx),$s1
+
+	mov	0($ctx),$h0		# load hash value
+	mov	8($ctx),$h1
+	mov	16($ctx),$h2
+
+	mov	$s1,$r1
+	shr	\$2,$s1
+	mov	$r1,%rax
+	add	$r1,$s1			# s1 = r1 + (r1 &gt;&gt; 2)
+	jmp	.Loop
+
+.align	32
+.Loop:
+	add	0($inp),$h0		# accumulate input
+	adc	8($inp),$h1
+	lea	16($inp),$inp
+	adc	$padbit,$h2
+___
+	&amp;poly1305_iteration();
+$code.=&lt;&lt;___;
+	mov	$r1,%rax
+	sub	\$16,%r15		# len-=16
+	jnc	.Loop
+
+	mov	$h0,0($ctx)		# store hash value
+	mov	$h1,8($ctx)
+	mov	$h2,16($ctx)
+
+	mov	0(%rsp),%r15
+	mov	8(%rsp),%r14
+	mov	16(%rsp),%r13
+	mov	24(%rsp),%r12
+	mov	32(%rsp),%rbp
+	mov	40(%rsp),%rbx
+	lea	48(%rsp),%rsp
+.Lno_data:
+.Lblocks_epilogue:
+	ret
+.size	poly1305_blocks,.-poly1305_blocks
+
+.globl	poly1305_emit
+.type	poly1305_emit,\@function,3
+.align	32
+poly1305_emit:
+	mov	0($ctx),%r8	# load hash value
+	mov	8($ctx),%r9
+	mov	16($ctx),%r10
+
+	mov	%r8,%rax
+	add	\$5,%r8		# compare to modulus
+	mov	%r9,%rcx
+	adc	\$0,%r9
+	adc	\$0,%r10
+	shr	\$2,%r10	# did 130-bit value overfow?
+	cmovnz	%r8,%rax
+	cmovnz	%r9,%rcx
+
+	add	0($nonce),%rax	# accumulate nonce
+	adc	8($nonce),%rcx
+	mov	%rax,0($mac)	# write result
+	mov	%rcx,8($mac)
+
+	ret
+.size	poly1305_emit,.-poly1305_emit
+___
+if ($avx) {
+
+########################################################################
+# Layout of opaque area is following.
+#
+#	unsigned __int32 h[5];		# current hash value base 2^26
+#	unsigned __int32 is_base2_26;
+#	unsigned __int64 r[2];		# key value base 2^64
+#	unsigned __int64 pad;
+#	struct { unsigned __int32 r^2, r^1, r^4, r^3; } r[9];
+#
+# where r^n are base 2^26 digits of degrees of multiplier key. There are
+# 5 digits, but last four are interleaved with multiples of 5, totalling
+# in 9 elements: r0, r1, 5*r1, r2, 5*r2, r3, 5*r3, r4, 5*r4.
+
+my ($H0,$H1,$H2,$H3,$H4, $T0,$T1,$T2,$T3,$T4, $D0,$D1,$D2,$D3,$D4, $MASK) =
+    map(&quot;%xmm$_&quot;,(0..15));
+
+$code.=&lt;&lt;___;
+.type	__poly1305_block,\@abi-omnipotent
+.align	32
+__poly1305_block:
+___
+	&amp;poly1305_iteration();
+$code.=&lt;&lt;___;
+	ret
+.size	__poly1305_block,.-__poly1305_block
+
+.type	__poly1305_init_avx,\@abi-omnipotent
+.align	32
+__poly1305_init_avx:
+	mov	$r0,$h0
+	mov	$r1,$h1
+	xor	$h2,$h2
+
+	lea	48+64($ctx),$ctx	# size optimization
+
+	mov	$r1,%rax
+	call	__poly1305_block	# r^2
+
+	mov	\$0x3ffffff,%eax	# save interleaved r^2 and r base 2^26
+	mov	\$0x3ffffff,%edx
+	mov	$h0,$d1
+	and	$h0#d,%eax
+	mov	$r0,$d2
+	and	$r0#d,%edx
+	mov	%eax,`16*0+0-64`($ctx)
+	shr	\$26,$d1
+	mov	%edx,`16*0+4-64`($ctx)
+	shr	\$26,$d2
+
+	mov	\$0x3ffffff,%eax
+	mov	\$0x3ffffff,%edx
+	and	$d1#d,%eax
+	and	$d2#d,%edx
+	mov	%eax,`16*1+0-64`($ctx)
+	lea	(%rax,%rax,4),%eax	# *5
+	mov	%edx,`16*1+4-64`($ctx)
+	lea	(%rdx,%rdx,4),%edx	# *5
+	mov	%eax,`16*2+0-64`($ctx)
+	shr	\$26,$d1
+	mov	%edx,`16*2+4-64`($ctx)
+	shr	\$26,$d2
+
+	mov	$h1,%rax
+	mov	$r1,%rdx
+	shl	\$12,%rax
+	shl	\$12,%rdx
+	or	$d1,%rax
+	or	$d2,%rdx
+	and	\$0x3ffffff,%eax
+	and	\$0x3ffffff,%edx
+	mov	%eax,`16*3+0-64`($ctx)
+	lea	(%rax,%rax,4),%eax	# *5
+	mov	%edx,`16*3+4-64`($ctx)
+	lea	(%rdx,%rdx,4),%edx	# *5
+	mov	%eax,`16*4+0-64`($ctx)
+	mov	$h1,$d1
+	mov	%edx,`16*4+4-64`($ctx)
+	mov	$r1,$d2
+
+	mov	\$0x3ffffff,%eax
+	mov	\$0x3ffffff,%edx
+	shr	\$14,$d1
+	shr	\$14,$d2
+	and	$d1#d,%eax
+	and	$d2#d,%edx
+	mov	%eax,`16*5+0-64`($ctx)
+	lea	(%rax,%rax,4),%eax	# *5
+	mov	%edx,`16*5+4-64`($ctx)
+	lea	(%rdx,%rdx,4),%edx	# *5
+	mov	%eax,`16*6+0-64`($ctx)
+	shr	\$26,$d1
+	mov	%edx,`16*6+4-64`($ctx)
+	shr	\$26,$d2
+
+	mov	$h2,%rax
+	shl	\$24,%rax
+	or	%rax,$d1
+	mov	$d1#d,`16*7+0-64`($ctx)
+	lea	($d1,$d1,4),$d1		# *5
+	mov	$d2#d,`16*7+4-64`($ctx)
+	lea	($d2,$d2,4),$d2		# *5
+	mov	$d1#d,`16*8+0-64`($ctx)
+	mov	$d2#d,`16*8+4-64`($ctx)
+
+	mov	$r1,%rax
+	call	__poly1305_block	# r^3
+
+	mov	\$0x3ffffff,%eax	# save r^3 base 2^26
+	mov	$h0,$d1
+	and	$h0#d,%eax
+	shr	\$26,$d1
+	mov	%eax,`16*0+12-64`($ctx)
+
+	mov	\$0x3ffffff,%edx
+	and	$d1#d,%edx
+	mov	%edx,`16*1+12-64`($ctx)
+	lea	(%rdx,%rdx,4),%edx	# *5
+	shr	\$26,$d1
+	mov	%edx,`16*2+12-64`($ctx)
+
+	mov	$h1,%rax
+	shl	\$12,%rax
+	or	$d1,%rax
+	and	\$0x3ffffff,%eax
+	mov	%eax,`16*3+12-64`($ctx)
+	lea	(%rax,%rax,4),%eax	# *5
+	mov	$h1,$d1
+	mov	%eax,`16*4+12-64`($ctx)
+
+	mov	\$0x3ffffff,%edx
+	shr	\$14,$d1
+	and	$d1#d,%edx
+	mov	%edx,`16*5+12-64`($ctx)
+	lea	(%rdx,%rdx,4),%edx	# *5
+	shr	\$26,$d1
+	mov	%edx,`16*6+12-64`($ctx)
+
+	mov	$h2,%rax
+	shl	\$24,%rax
+	or	%rax,$d1
+	mov	$d1#d,`16*7+12-64`($ctx)
+	lea	($d1,$d1,4),$d1		# *5
+	mov	$d1#d,`16*8+12-64`($ctx)
+
+	mov	$r1,%rax
+	call	__poly1305_block	# r^4
+
+	mov	\$0x3ffffff,%eax	# save r^4 base 2^26
+	mov	$h0,$d1
+	and	$h0#d,%eax
+	shr	\$26,$d1
+	mov	%eax,`16*0+8-64`($ctx)
+
+	mov	\$0x3ffffff,%edx
+	and	$d1#d,%edx
+	mov	%edx,`16*1+8-64`($ctx)
+	lea	(%rdx,%rdx,4),%edx	# *5
+	shr	\$26,$d1
+	mov	%edx,`16*2+8-64`($ctx)
+
+	mov	$h1,%rax
+	shl	\$12,%rax
+	or	$d1,%rax
+	and	\$0x3ffffff,%eax
+	mov	%eax,`16*3+8-64`($ctx)
+	lea	(%rax,%rax,4),%eax	# *5
+	mov	$h1,$d1
+	mov	%eax,`16*4+8-64`($ctx)
+
+	mov	\$0x3ffffff,%edx
+	shr	\$14,$d1
+	and	$d1#d,%edx
+	mov	%edx,`16*5+8-64`($ctx)
+	lea	(%rdx,%rdx,4),%edx	# *5
+	shr	\$26,$d1
+	mov	%edx,`16*6+8-64`($ctx)
+
+	mov	$h2,%rax
+	shl	\$24,%rax
+	or	%rax,$d1
+	mov	$d1#d,`16*7+8-64`($ctx)
+	lea	($d1,$d1,4),$d1		# *5
+	mov	$d1#d,`16*8+8-64`($ctx)
+
+	lea	-48-64($ctx),$ctx	# size [de-]optimization
+	ret
+.size	__poly1305_init_avx,.-__poly1305_init_avx
+
+.type	poly1305_blocks_avx,\@function,4
+.align	32
+poly1305_blocks_avx:
+	mov	20($ctx),%r8d		# is_base2_26
+	cmp	\$128,$len
+	jae	.Lblocks_avx
+	test	%r8d,%r8d
+	jz	poly1305_blocks
+
+.Lblocks_avx:
+	and	\$-16,$len
+	jz	.Lno_data_avx
+
+	vzeroupper
+
+	test	%r8d,%r8d
+	jz	.Lbase2_64_avx
+
+	test	\$31,$len
+	jz	.Leven_avx
+
+	push	%rbx
+	push	%rbp
+	push	%r12
+	push	%r13
+	push	%r14
+	push	%r15
+.Lblocks_avx_body:
+
+	mov	$len,%r15		# reassign $len
+
+	mov	0($ctx),$d1		# load hash value
+	mov	8($ctx),$d2
+	mov	16($ctx),$h2#d
+
+	mov	24($ctx),$r0		# load r
+	mov	32($ctx),$s1
+
+	################################# base 2^26 -&gt; base 2^64
+	mov	$d1#d,$h0#d
+	and	\$-1&lt;&lt;31,$d1
+	mov	$d2,$r1			# borrow $r1
+	mov	$d2#d,$h1#d
+	and	\$-1&lt;&lt;31,$d2
+
+	shr	\$6,$d1
+	shl	\$52,$r1
+	add	$d1,$h0
+	shr	\$12,$h1
+	shr	\$18,$d2
+	add	$r1,$h0
+	adc	$d2,$h1
+
+	mov	$h2,$d1
+	shl	\$40,$d1
+	shr	\$24,$h2
+	add	$d1,$h1
+	adc	\$0,$h2			# can be partially reduced...
+
+	mov	\$-4,$d2		# ... so reduce
+	mov	$h2,$d1
+	and	$h2,$d2
+	shr	\$2,$d1
+	and	\$3,$h2
+	add	$d2,$d1			# =*5
+	add	$d1,$h0
+	adc	\$0,$h1
+
+	mov	$s1,$r1
+	mov	$s1,%rax
+	shr	\$2,$s1
+	add	$r1,$s1			# s1 = r1 + (r1 &gt;&gt; 2)
+
+	add	0($inp),$h0		# accumulate input
+	adc	8($inp),$h1
+	lea	16($inp),$inp
+	adc	$padbit,$h2
+
+	call	__poly1305_block
+
+	test	$padbit,$padbit		# if $padbit is zero,
+	jz	.Lstore_base2_64_avx	# store hash in base 2^64 format
+
+	################################# base 2^64 -&gt; base 2^26
+	mov	$h0,%rax
+	mov	$h0,%rdx
+	shr	\$52,$h0
+	mov	$h1,$r0
+	mov	$h1,$r1
+	shr	\$26,%rdx
+	and	\$0x3ffffff,%rax	# h[0]
+	shl	\$12,$r0
+	and	\$0x3ffffff,%rdx	# h[1]
+	shr	\$14,$h1
+	or	$r0,$h0
+	shl	\$24,$h2
+	and	\$0x3ffffff,$h0		# h[2]
+	shr	\$40,$r1
+	and	\$0x3ffffff,$h1		# h[3]
+	or	$r1,$h2			# h[4]
+
+	sub	\$16,%r15
+	jz	.Lstore_base2_26_avx
+
+	vmovd	%rax#d,$H0
+	vmovd	%rdx#d,$H1
+	vmovd	$h0#d,$H2
+	vmovd	$h1#d,$H3
+	vmovd	$h2#d,$H4
+	jmp	.Lproceed_avx
+
+.align	32
+.Lstore_base2_64_avx:
+	mov	$h0,0($ctx)
+	mov	$h1,8($ctx)
+	mov	$h2,16($ctx)		# note that is_base2_26 is zeroed
+	jmp	.Ldone_avx
+
+.align	16
+.Lstore_base2_26_avx:
+	mov	%rax#d,0($ctx)		# store hash value base 2^26
+	mov	%rdx#d,4($ctx)
+	mov	$h0#d,8($ctx)
+	mov	$h1#d,12($ctx)
+	mov	$h2#d,16($ctx)
+.align	16
+.Ldone_avx:
+	mov	0(%rsp),%r15
+	mov	8(%rsp),%r14
+	mov	16(%rsp),%r13
+	mov	24(%rsp),%r12
+	mov	32(%rsp),%rbp
+	mov	40(%rsp),%rbx
+	lea	48(%rsp),%rsp
+.Lno_data_avx:
+.Lblocks_avx_epilogue:
+	ret
+
+.align	32
+.Lbase2_64_avx:
+	push	%rbx
+	push	%rbp
+	push	%r12
+	push	%r13
+	push	%r14
+	push	%r15
+.Lbase2_64_avx_body:
+
+	mov	$len,%r15		# reassign $len
+
+	mov	24($ctx),$r0		# load r
+	mov	32($ctx),$s1
+
+	mov	0($ctx),$h0		# load hash value
+	mov	8($ctx),$h1
+	mov	16($ctx),$h2#d
+
+	mov	$s1,$r1
+	mov	$s1,%rax
+	shr	\$2,$s1
+	add	$r1,$s1			# s1 = r1 + (r1 &gt;&gt; 2)
+
+	test	\$31,$len
+	jz	.Linit_avx
+
+	add	0($inp),$h0		# accumulate input
+	adc	8($inp),$h1
+	lea	16($inp),$inp
+	adc	$padbit,$h2
+	sub	\$16,%r15
+
+	call	__poly1305_block
+
+.Linit_avx:
+	################################# base 2^64 -&gt; base 2^26
+	mov	$h0,%rax
+	mov	$h0,%rdx
+	shr	\$52,$h0
+	mov	$h1,$d1
+	mov	$h1,$d2
+	shr	\$26,%rdx
+	and	\$0x3ffffff,%rax	# h[0]
+	shl	\$12,$d1
+	and	\$0x3ffffff,%rdx	# h[1]
+	shr	\$14,$h1
+	or	$d1,$h0
+	shl	\$24,$h2
+	and	\$0x3ffffff,$h0		# h[2]
+	shr	\$40,$d2
+	and	\$0x3ffffff,$h1		# h[3]
+	or	$d2,$h2			# h[4]
+
+	vmovd	%rax#d,$H0
+	vmovd	%rdx#d,$H1
+	vmovd	$h0#d,$H2
+	vmovd	$h1#d,$H3
+	vmovd	$h2#d,$H4
+	movl	\$1,20($ctx)		# set is_base2_26
+
+	call	__poly1305_init_avx
+
+.Lproceed_avx:
+	mov	%r15,$len
+
+	mov	0(%rsp),%r15
+	mov	8(%rsp),%r14
+	mov	16(%rsp),%r13
+	mov	24(%rsp),%r12
+	mov	32(%rsp),%rbp
+	mov	40(%rsp),%rbx
+	lea	48(%rsp),%rax
+	lea	48(%rsp),%rsp
+.Lbase2_64_avx_epilogue:
+	jmp	.Ldo_avx
+
+.align	32
+.Leven_avx:
+	vmovd		4*0($ctx),$H0		# load hash value
+	vmovd		4*1($ctx),$H1
+	vmovd		4*2($ctx),$H2
+	vmovd		4*3($ctx),$H3
+	vmovd		4*4($ctx),$H4
+
+.Ldo_avx:
+___
+$code.=&lt;&lt;___	if (!$win64);
+	lea		-0x58(%rsp),%r11
+	sub		\$0x178,%rsp
+___
+$code.=&lt;&lt;___	if ($win64);
+	lea		-0xf8(%rsp),%r11
+	sub		\$0x218,%rsp
+	vmovdqa		%xmm6,0x50(%r11)
+	vmovdqa		%xmm7,0x60(%r11)
+	vmovdqa		%xmm8,0x70(%r11)
+	vmovdqa		%xmm9,0x80(%r11)
+	vmovdqa		%xmm10,0x90(%r11)
+	vmovdqa		%xmm11,0xa0(%r11)
+	vmovdqa		%xmm12,0xb0(%r11)
+	vmovdqa		%xmm13,0xc0(%r11)
+	vmovdqa		%xmm14,0xd0(%r11)
+	vmovdqa		%xmm15,0xe0(%r11)
+.Ldo_avx_body:
+___
+$code.=&lt;&lt;___;
+	sub		\$64,$len
+	lea		-32($inp),%rax
+	cmovc		%rax,$inp
+
+	vmovdqu		`16*3`($ctx),$D4	# preload r0^2
+	lea		`16*3+64`($ctx),$ctx	# size optimization
+	lea		.Lconst(%rip),%rcx
+
+	################################################################
+	# load input
+	vmovdqu		16*2($inp),$T0
+	vmovdqu		16*3($inp),$T1
+	vmovdqa		64(%rcx),$MASK		# .Lmask26
+
+	vpsrldq		\$6,$T0,$T2		# splat input
+	vpsrldq		\$6,$T1,$T3
+	vpunpckhqdq	$T1,$T0,$T4		# 4
+	vpunpcklqdq	$T1,$T0,$T0		# 0:1
+	vpunpcklqdq	$T3,$T2,$T3		# 2:3
+
+	vpsrlq		\$40,$T4,$T4		# 4
+	vpsrlq		\$26,$T0,$T1
+	vpand		$MASK,$T0,$T0		# 0
+	vpsrlq		\$4,$T3,$T2
+	vpand		$MASK,$T1,$T1		# 1
+	vpsrlq		\$30,$T3,$T3
+	vpand		$MASK,$T2,$T2		# 2
+	vpand		$MASK,$T3,$T3		# 3
+	vpor		32(%rcx),$T4,$T4	# padbit, yes, always
+
+	jbe		.Lskip_loop_avx
+
+	# expand and copy pre-calculated table to stack
+	vmovdqu		`16*1-64`($ctx),$D1
+	vmovdqu		`16*2-64`($ctx),$D2
+	vpshufd		\$0xEE,$D4,$D3		# 34xx -&gt; 3434
+	vpshufd		\$0x44,$D4,$D0		# xx12 -&gt; 1212
+	vmovdqa		$D3,-0x90(%r11)
+	vmovdqa		$D0,0x00(%rsp)
+	vpshufd		\$0xEE,$D1,$D4
+	vmovdqu		`16*3-64`($ctx),$D0
+	vpshufd		\$0x44,$D1,$D1
+	vmovdqa		$D4,-0x80(%r11)
+	vmovdqa		$D1,0x10(%rsp)
+	vpshufd		\$0xEE,$D2,$D3
+	vmovdqu		`16*4-64`($ctx),$D1
+	vpshufd		\$0x44,$D2,$D2
+	vmovdqa		$D3,-0x70(%r11)
+	vmovdqa		$D2,0x20(%rsp)
+	vpshufd		\$0xEE,$D0,$D4
+	vmovdqu		`16*5-64`($ctx),$D2
+	vpshufd		\$0x44,$D0,$D0
+	vmovdqa		$D4,-0x60(%r11)
+	vmovdqa		$D0,0x30(%rsp)
+	vpshufd		\$0xEE,$D1,$D3
+	vmovdqu		`16*6-64`($ctx),$D0
+	vpshufd		\$0x44,$D1,$D1
+	vmovdqa		$D3,-0x50(%r11)
+	vmovdqa		$D1,0x40(%rsp)
+	vpshufd		\$0xEE,$D2,$D4
+	vmovdqu		`16*7-64`($ctx),$D1
+	vpshufd		\$0x44,$D2,$D2
+	vmovdqa		$D4,-0x40(%r11)
+	vmovdqa		$D2,0x50(%rsp)
+	vpshufd		\$0xEE,$D0,$D3
+	vmovdqu		`16*8-64`($ctx),$D2
+	vpshufd		\$0x44,$D0,$D0
+	vmovdqa		$D3,-0x30(%r11)
+	vmovdqa		$D0,0x60(%rsp)
+	vpshufd		\$0xEE,$D1,$D4
+	vpshufd		\$0x44,$D1,$D1
+	vmovdqa		$D4,-0x20(%r11)
+	vmovdqa		$D1,0x70(%rsp)
+	vpshufd		\$0xEE,$D2,$D3
+	 vmovdqa	0x00(%rsp),$D4		# preload r0^2
+	vpshufd		\$0x44,$D2,$D2
+	vmovdqa		$D3,-0x10(%r11)
+	vmovdqa		$D2,0x80(%rsp)
+
+	jmp		.Loop_avx
+
+.align	32
+.Loop_avx:
+	################################################################
+	# ((inp[0]*r^4+inp[2]*r^2+inp[4])*r^4+inp[6]*r^2
+	# ((inp[1]*r^4+inp[3]*r^2+inp[5])*r^3+inp[7]*r
+	#   \___________________/
+	# ((inp[0]*r^4+inp[2]*r^2+inp[4])*r^4+inp[6]*r^2+inp[8])*r^2
+	# ((inp[1]*r^4+inp[3]*r^2+inp[5])*r^4+inp[7]*r^2+inp[9])*r
+	#   \___________________/ \____________________/
+	#
+	# Note that we start with inp[2:3]*r^2. This is because it
+	# doesn't depend on reduction in previous iteration.
+	################################################################
+	# d4 = h4*r0 + h3*r1   + h2*r2   + h1*r3   + h0*r4
+	# d3 = h3*r0 + h2*r1   + h1*r2   + h0*r3   + h4*5*r4
+	# d2 = h2*r0 + h1*r1   + h0*r2   + h4*5*r3 + h3*5*r4
+	# d1 = h1*r0 + h0*r1   + h4*5*r2 + h3*5*r3 + h2*5*r4
+	# d0 = h0*r0 + h4*5*r1 + h3*5*r2 + h2*5*r3 + h1*5*r4
+	#
+	# though note that $Tx and $Hx are &quot;reversed&quot; in this section,
+	# and $D4 is preloaded with r0^2...
+
+	vpmuludq	$T0,$D4,$D0		# d0 = h0*r0
+	vpmuludq	$T1,$D4,$D1		# d1 = h1*r0
+	  vmovdqa	$H2,0x20(%r11)				# offload hash
+	vpmuludq	$T2,$D4,$D2		# d3 = h2*r0
+	 vmovdqa	0x10(%rsp),$H2		# r1^2
+	vpmuludq	$T3,$D4,$D3		# d3 = h3*r0
+	vpmuludq	$T4,$D4,$D4		# d4 = h4*r0
+
+	  vmovdqa	$H0,0x00(%r11)				#
+	vpmuludq	0x20(%rsp),$T4,$H0	# h4*s1
+	  vmovdqa	$H1,0x10(%r11)				#
+	vpmuludq	$T3,$H2,$H1		# h3*r1
+	vpaddq		$H0,$D0,$D0		# d0 += h4*s1
+	vpaddq		$H1,$D4,$D4		# d4 += h3*r1
+	  vmovdqa	$H3,0x30(%r11)				#
+	vpmuludq	$T2,$H2,$H0		# h2*r1
+	vpmuludq	$T1,$H2,$H1		# h1*r1
+	vpaddq		$H0,$D3,$D3		# d3 += h2*r1
+	 vmovdqa	0x30(%rsp),$H3		# r2^2
+	vpaddq		$H1,$D2,$D2		# d2 += h1*r1
+	  vmovdqa	$H4,0x40(%r11)				#
+	vpmuludq	$T0,$H2,$H2		# h0*r1
+	 vpmuludq	$T2,$H3,$H0		# h2*r2
+	vpaddq		$H2,$D1,$D1		# d1 += h0*r1
+
+	 vmovdqa	0x40(%rsp),$H4		# s2^2
+	vpaddq		$H0,$D4,$D4		# d4 += h2*r2
+	vpmuludq	$T1,$H3,$H1		# h1*r2
+	vpmuludq	$T0,$H3,$H3		# h0*r2
+	vpaddq		$H1,$D3,$D3		# d3 += h1*r2
+	 vmovdqa	0x50(%rsp),$H2		# r3^2
+	vpaddq		$H3,$D2,$D2		# d2 += h0*r2
+	vpmuludq	$T4,$H4,$H0		# h4*s2
+	vpmuludq	$T3,$H4,$H4		# h3*s2
+	vpaddq		$H0,$D1,$D1		# d1 += h4*s2
+	 vmovdqa	0x60(%rsp),$H3		# s3^2
+	vpaddq		$H4,$D0,$D0		# d0 += h3*s2
+
+	 vmovdqa	0x80(%rsp),$H4		# s4^2
+	vpmuludq	$T1,$H2,$H1		# h1*r3
+	vpmuludq	$T0,$H2,$H2		# h0*r3
+	vpaddq		$H1,$D4,$D4		# d4 += h1*r3
+	vpaddq		$H2,$D3,$D3		# d3 += h0*r3
+	vpmuludq	$T4,$H3,$H0		# h4*s3
+	vpmuludq	$T3,$H3,$H1		# h3*s3
+	vpaddq		$H0,$D2,$D2		# d2 += h4*s3
+	 vmovdqu	16*0($inp),$H0				# load input
+	vpaddq		$H1,$D1,$D1		# d1 += h3*s3
+	vpmuludq	$T2,$H3,$H3		# h2*s3
+	 vpmuludq	$T2,$H4,$T2		# h2*s4
+	vpaddq		$H3,$D0,$D0		# d0 += h2*s3
+
+	 vmovdqu	16*1($inp),$H1				#
+	vpaddq		$T2,$D1,$D1		# d1 += h2*s4
+	vpmuludq	$T3,$H4,$T3		# h3*s4
+	vpmuludq	$T4,$H4,$T4		# h4*s4
+	 vpsrldq	\$6,$H0,$H2				# splat input
+	vpaddq		$T3,$D2,$D2		# d2 += h3*s4
+	vpaddq		$T4,$D3,$D3		# d3 += h4*s4
+	 vpsrldq	\$6,$H1,$H3				#
+	vpmuludq	0x70(%rsp),$T0,$T4	# h0*r4
+	vpmuludq	$T1,$H4,$T0		# h1*s4
+	 vpunpckhqdq	$H1,$H0,$H4		# 4
+	vpaddq		$T4,$D4,$D4		# d4 += h0*r4
+	 vmovdqa	-0x90(%r11),$T4		# r0^4
+	vpaddq		$T0,$D0,$D0		# d0 += h1*s4
+
+	vpunpcklqdq	$H1,$H0,$H0		# 0:1
+	vpunpcklqdq	$H3,$H2,$H3		# 2:3
+
+	#vpsrlq		\$40,$H4,$H4		# 4
+	vpsrldq		\$`40/8`,$H4,$H4	# 4
+	vpsrlq		\$26,$H0,$H1
+	vpand		$MASK,$H0,$H0		# 0
+	vpsrlq		\$4,$H3,$H2
+	vpand		$MASK,$H1,$H1		# 1
+	vpand		0(%rcx),$H4,$H4		# .Lmask24
+	vpsrlq		\$30,$H3,$H3
+	vpand		$MASK,$H2,$H2		# 2
+	vpand		$MASK,$H3,$H3		# 3
+	vpor		32(%rcx),$H4,$H4	# padbit, yes, always
+
+	vpaddq		0x00(%r11),$H0,$H0	# add hash value
+	vpaddq		0x10(%r11),$H1,$H1
+	vpaddq		0x20(%r11),$H2,$H2
+	vpaddq		0x30(%r11),$H3,$H3
+	vpaddq		0x40(%r11),$H4,$H4
+
+	lea		16*2($inp),%rax
+	lea		16*4($inp),$inp
+	sub		\$64,$len
+	cmovc		%rax,$inp
+
+	################################################################
+	# Now we accumulate (inp[0:1]+hash)*r^4
+	################################################################
+	# d4 = h4*r0 + h3*r1   + h2*r2   + h1*r3   + h0*r4
+	# d3 = h3*r0 + h2*r1   + h1*r2   + h0*r3   + h4*5*r4
+	# d2 = h2*r0 + h1*r1   + h0*r2   + h4*5*r3 + h3*5*r4
+	# d1 = h1*r0 + h0*r1   + h4*5*r2 + h3*5*r3 + h2*5*r4
+	# d0 = h0*r0 + h4*5*r1 + h3*5*r2 + h2*5*r3 + h1*5*r4
+
+	vpmuludq	$H0,$T4,$T0		# h0*r0
+	vpmuludq	$H1,$T4,$T1		# h1*r0
+	vpaddq		$T0,$D0,$D0
+	vpaddq		$T1,$D1,$D1
+	 vmovdqa	-0x80(%r11),$T2		# r1^4
+	vpmuludq	$H2,$T4,$T0		# h2*r0
+	vpmuludq	$H3,$T4,$T1		# h3*r0
+	vpaddq		$T0,$D2,$D2
+	vpaddq		$T1,$D3,$D3
+	vpmuludq	$H4,$T4,$T4		# h4*r0
+	 vpmuludq	-0x70(%r11),$H4,$T0	# h4*s1
+	vpaddq		$T4,$D4,$D4
+
+	vpaddq		$T0,$D0,$D0		# d0 += h4*s1
+	vpmuludq	$H2,$T2,$T1		# h2*r1
+	vpmuludq	$H3,$T2,$T0		# h3*r1
+	vpaddq		$T1,$D3,$D3		# d3 += h2*r1
+	 vmovdqa	-0x60(%r11),$T3		# r2^4
+	vpaddq		$T0,$D4,$D4		# d4 += h3*r1
+	vpmuludq	$H1,$T2,$T1		# h1*r1
+	vpmuludq	$H0,$T2,$T2		# h0*r1
+	vpaddq		$T1,$D2,$D2		# d2 += h1*r1
+	vpaddq		$T2,$D1,$D1		# d1 += h0*r1
+
+	 vmovdqa	-0x50(%r11),$T4		# s2^4
+	vpmuludq	$H2,$T3,$T0		# h2*r2
+	vpmuludq	$H1,$T3,$T1		# h1*r2
+	vpaddq		$T0,$D4,$D4		# d4 += h2*r2
+	vpaddq		$T1,$D3,$D3		# d3 += h1*r2
+	 vmovdqa	-0x40(%r11),$T2		# r3^4
+	vpmuludq	$H0,$T3,$T3		# h0*r2
+	vpmuludq	$H4,$T4,$T0		# h4*s2
+	vpaddq		$T3,$D2,$D2		# d2 += h0*r2
+	vpaddq		$T0,$D1,$D1		# d1 += h4*s2
+	 vmovdqa	-0x30(%r11),$T3		# s3^4
+	vpmuludq	$H3,$T4,$T4		# h3*s2
+	 vpmuludq	$H1,$T2,$T1		# h1*r3
+	vpaddq		$T4,$D0,$D0		# d0 += h3*s2
+
+	 vmovdqa	-0x10(%r11),$T4		# s4^4
+	vpaddq		$T1,$D4,$D4		# d4 += h1*r3
+	vpmuludq	$H0,$T2,$T2		# h0*r3
+	vpmuludq	$H4,$T3,$T0		# h4*s3
+	vpaddq		$T2,$D3,$D3		# d3 += h0*r3
+	vpaddq		$T0,$D2,$D2		# d2 += h4*s3
+	 vmovdqu	16*2($inp),$T0				# load input
+	vpmuludq	$H3,$T3,$T2		# h3*s3
+	vpmuludq	$H2,$T3,$T3		# h2*s3
+	vpaddq		$T2,$D1,$D1		# d1 += h3*s3
+	 vmovdqu	16*3($inp),$T1				#
+	vpaddq		$T3,$D0,$D0		# d0 += h2*s3
+
+	vpmuludq	$H2,$T4,$H2		# h2*s4
+	vpmuludq	$H3,$T4,$H3		# h3*s4
+	 vpsrldq	\$6,$T0,$T2				# splat input
+	vpaddq		$H2,$D1,$D1		# d1 += h2*s4
+	vpmuludq	$H4,$T4,$H4		# h4*s4
+	 vpsrldq	\$6,$T1,$T3				#
+	vpaddq		$H3,$D2,$H2		# h2 = d2 + h3*s4
+	vpaddq		$H4,$D3,$H3		# h3 = d3 + h4*s4
+	vpmuludq	-0x20(%r11),$H0,$H4	# h0*r4
+	vpmuludq	$H1,$T4,$H0
+	 vpunpckhqdq	$T1,$T0,$T4		# 4
+	vpaddq		$H4,$D4,$H4		# h4 = d4 + h0*r4
+	vpaddq		$H0,$D0,$H0		# h0 = d0 + h1*s4
+
+	vpunpcklqdq	$T1,$T0,$T0		# 0:1
+	vpunpcklqdq	$T3,$T2,$T3		# 2:3
+
+	#vpsrlq		\$40,$T4,$T4		# 4
+	vpsrldq		\$`40/8`,$T4,$T4	# 4
+	vpsrlq		\$26,$T0,$T1
+	 vmovdqa	0x00(%rsp),$D4		# preload r0^2
+	vpand		$MASK,$T0,$T0		# 0
+	vpsrlq		\$4,$T3,$T2
+	vpand		$MASK,$T1,$T1		# 1
+	vpand		0(%rcx),$T4,$T4		# .Lmask24
+	vpsrlq		\$30,$T3,$T3
+	vpand		$MASK,$T2,$T2		# 2
+	vpand		$MASK,$T3,$T3		# 3
+	vpor		32(%rcx),$T4,$T4	# padbit, yes, always
+
+	################################################################
+	# lazy reduction as discussed in &quot;NEON crypto&quot; by D.J. Bernstein
+	# and P. Schwabe
+
+	vpsrlq		\$26,$H3,$D3
+	vpand		$MASK,$H3,$H3
+	vpaddq		$D3,$H4,$H4		# h3 -&gt; h4
+
+	vpsrlq		\$26,$H0,$D0
+	vpand		$MASK,$H0,$H0
+	vpaddq		$D0,$D1,$H1		# h0 -&gt; h1
+
+	vpsrlq		\$26,$H4,$D0
+	vpand		$MASK,$H4,$H4
+
+	vpsrlq		\$26,$H1,$D1
+	vpand		$MASK,$H1,$H1
+	vpaddq		$D1,$H2,$H2		# h1 -&gt; h2
+
+	vpaddq		$D0,$H0,$H0
+	vpsllq		\$2,$D0,$D0
+	vpaddq		$D0,$H0,$H0		# h4 -&gt; h0
+
+	vpsrlq		\$26,$H2,$D2
+	vpand		$MASK,$H2,$H2
+	vpaddq		$D2,$H3,$H3		# h2 -&gt; h3
+
+	vpsrlq		\$26,$H0,$D0
+	vpand		$MASK,$H0,$H0
+	vpaddq		$D0,$H1,$H1		# h0 -&gt; h1
+
+	vpsrlq		\$26,$H3,$D3
+	vpand		$MASK,$H3,$H3
+	vpaddq		$D3,$H4,$H4		# h3 -&gt; h4
+
+	ja		.Loop_avx
+
+.Lskip_loop_avx:
+	################################################################
+	# multiply (inp[0:1]+hash) or inp[2:3] by r^2:r^1
+
+	vpshufd		\$0x10,$D4,$D4		# r0^n, xx12 -&gt; x1x2
+	add		\$32,$len
+	jnz		.Long_tail_avx
+
+	vpaddq		$H2,$T2,$T2
+	vpaddq		$H0,$T0,$T0
+	vpaddq		$H1,$T1,$T1
+	vpaddq		$H3,$T3,$T3
+	vpaddq		$H4,$T4,$T4
+
+.Long_tail_avx:
+	vmovdqa		$H2,0x20(%r11)
+	vmovdqa		$H0,0x00(%r11)
+	vmovdqa		$H1,0x10(%r11)
+	vmovdqa		$H3,0x30(%r11)
+	vmovdqa		$H4,0x40(%r11)
+
+	# d4 = h4*r0 + h3*r1   + h2*r2   + h1*r3   + h0*r4
+	# d3 = h3*r0 + h2*r1   + h1*r2   + h0*r3   + h4*5*r4
+	# d2 = h2*r0 + h1*r1   + h0*r2   + h4*5*r3 + h3*5*r4
+	# d1 = h1*r0 + h0*r1   + h4*5*r2 + h3*5*r3 + h2*5*r4
+	# d0 = h0*r0 + h4*5*r1 + h3*5*r2 + h2*5*r3 + h1*5*r4
+
+	vpmuludq	$T2,$D4,$D2		# d2 = h2*r0
+	vpmuludq	$T0,$D4,$D0		# d0 = h0*r0
+	 vpshufd	\$0x10,`16*1-64`($ctx),$H2		# r1^n
+	vpmuludq	$T1,$D4,$D1		# d1 = h1*r0
+	vpmuludq	$T3,$D4,$D3		# d3 = h3*r0
+	vpmuludq	$T4,$D4,$D4		# d4 = h4*r0
+
+	vpmuludq	$T3,$H2,$H0		# h3*r1
+	vpaddq		$H0,$D4,$D4		# d4 += h3*r1
+	 vpshufd	\$0x10,`16*2-64`($ctx),$H3		# s1^n
+	vpmuludq	$T2,$H2,$H1		# h2*r1
+	vpaddq		$H1,$D3,$D3		# d3 += h2*r1
+	 vpshufd	\$0x10,`16*3-64`($ctx),$H4		# r2^n
+	vpmuludq	$T1,$H2,$H0		# h1*r1
+	vpaddq		$H0,$D2,$D2		# d2 += h1*r1
+	vpmuludq	$T0,$H2,$H2		# h0*r1
+	vpaddq		$H2,$D1,$D1		# d1 += h0*r1
+	vpmuludq	$T4,$H3,$H3		# h4*s1
+	vpaddq		$H3,$D0,$D0		# d0 += h4*s1
+
+	 vpshufd	\$0x10,`16*4-64`($ctx),$H2		# s2^n
+	vpmuludq	$T2,$H4,$H1		# h2*r2
+	vpaddq		$H1,$D4,$D4		# d4 += h2*r2
+	vpmuludq	$T1,$H4,$H0		# h1*r2
+	vpaddq		$H0,$D3,$D3		# d3 += h1*r2
+	 vpshufd	\$0x10,`16*5-64`($ctx),$H3		# r3^n
+	vpmuludq	$T0,$H4,$H4		# h0*r2
+	vpaddq		$H4,$D2,$D2		# d2 += h0*r2
+	vpmuludq	$T4,$H2,$H1		# h4*s2
+	vpaddq		$H1,$D1,$D1		# d1 += h4*s2
+	 vpshufd	\$0x10,`16*6-64`($ctx),$H4		# s3^n
+	vpmuludq	$T3,$H2,$H2		# h3*s2
+	vpaddq		$H2,$D0,$D0		# d0 += h3*s2
+
+	vpmuludq	$T1,$H3,$H0		# h1*r3
+	vpaddq		$H0,$D4,$D4		# d4 += h1*r3
+	vpmuludq	$T0,$H3,$H3		# h0*r3
+	vpaddq		$H3,$D3,$D3		# d3 += h0*r3
+	 vpshufd	\$0x10,`16*7-64`($ctx),$H2		# r4^n
+	vpmuludq	$T4,$H4,$H1		# h4*s3
+	vpaddq		$H1,$D2,$D2		# d2 += h4*s3
+	 vpshufd	\$0x10,`16*8-64`($ctx),$H3		# s4^n
+	vpmuludq	$T3,$H4,$H0		# h3*s3
+	vpaddq		$H0,$D1,$D1		# d1 += h3*s3
+	vpmuludq	$T2,$H4,$H4		# h2*s3
+	vpaddq		$H4,$D0,$D0		# d0 += h2*s3
+
+	vpmuludq	$T0,$H2,$H2		# h0*r4
+	vpaddq		$H2,$D4,$D4		# h4 = d4 + h0*r4
+	vpmuludq	$T4,$H3,$H1		# h4*s4
+	vpaddq		$H1,$D3,$D3		# h3 = d3 + h4*s4
+	vpmuludq	$T3,$H3,$H0		# h3*s4
+	vpaddq		$H0,$D2,$D2		# h2 = d2 + h3*s4
+	vpmuludq	$T2,$H3,$H1		# h2*s4
+	vpaddq		$H1,$D1,$D1		# h1 = d1 + h2*s4
+	vpmuludq	$T1,$H3,$H3		# h1*s4
+	vpaddq		$H3,$D0,$D0		# h0 = d0 + h1*s4
+
+	jz		.Lshort_tail_avx
+
+	vmovdqu		16*0($inp),$H0		# load input
+	vmovdqu		16*1($inp),$H1
+
+	vpsrldq		\$6,$H0,$H2		# splat input
+	vpsrldq		\$6,$H1,$H3
+	vpunpckhqdq	$H1,$H0,$H4		# 4
+	vpunpcklqdq	$H1,$H0,$H0		# 0:1
+	vpunpcklqdq	$H3,$H2,$H3		# 2:3
+
+	vpsrlq		\$40,$H4,$H4		# 4
+	vpsrlq		\$26,$H0,$H1
+	vpand		$MASK,$H0,$H0		# 0
+	vpsrlq		\$4,$H3,$H2
+	vpand		$MASK,$H1,$H1		# 1
+	vpsrlq		\$30,$H3,$H3
+	vpand		$MASK,$H2,$H2		# 2
+	vpand		$MASK,$H3,$H3		# 3
+	vpor		32(%rcx),$H4,$H4	# padbit, yes, always
+
+	vpshufd		\$0x32,`16*0-64`($ctx),$T4	# r0^n, 34xx -&gt; x3x4
+	vpaddq		0x00(%r11),$H0,$H0
+	vpaddq		0x10(%r11),$H1,$H1
+	vpaddq		0x20(%r11),$H2,$H2
+	vpaddq		0x30(%r11),$H3,$H3
+	vpaddq		0x40(%r11),$H4,$H4
+
+	################################################################
+	# multiply (inp[0:1]+hash) by r^4:r^3 and accumulate
+
+	vpmuludq	$H0,$T4,$T0		# h0*r0
+	vpaddq		$T0,$D0,$D0		# d0 += h0*r0
+	vpmuludq	$H1,$T4,$T1		# h1*r0
+	vpaddq		$T1,$D1,$D1		# d1 += h1*r0
+	vpmuludq	$H2,$T4,$T0		# h2*r0
+	vpaddq		$T0,$D2,$D2		# d2 += h2*r0
+	 vpshufd	\$0x32,`16*1-64`($ctx),$T2		# r1^n
+	vpmuludq	$H3,$T4,$T1		# h3*r0
+	vpaddq		$T1,$D3,$D3		# d3 += h3*r0
+	vpmuludq	$H4,$T4,$T4		# h4*r0
+	vpaddq		$T4,$D4,$D4		# d4 += h4*r0
+
+	vpmuludq	$H3,$T2,$T0		# h3*r1
+	vpaddq		$T0,$D4,$D4		# d4 += h3*r1
+	 vpshufd	\$0x32,`16*2-64`($ctx),$T3		# s1
+	vpmuludq	$H2,$T2,$T1		# h2*r1
+	vpaddq		$T1,$D3,$D3		# d3 += h2*r1
+	 vpshufd	\$0x32,`16*3-64`($ctx),$T4		# r2
+	vpmuludq	$H1,$T2,$T0		# h1*r1
+	vpaddq		$T0,$D2,$D2		# d2 += h1*r1
+	vpmuludq	$H0,$T2,$T2		# h0*r1
+	vpaddq		$T2,$D1,$D1		# d1 += h0*r1
+	vpmuludq	$H4,$T3,$T3		# h4*s1
+	vpaddq		$T3,$D0,$D0		# d0 += h4*s1
+
+	 vpshufd	\$0x32,`16*4-64`($ctx),$T2		# s2
+	vpmuludq	$H2,$T4,$T1		# h2*r2
+	vpaddq		$T1,$D4,$D4		# d4 += h2*r2
+	vpmuludq	$H1,$T4,$T0		# h1*r2
+	vpaddq		$T0,$D3,$D3		# d3 += h1*r2
+	 vpshufd	\$0x32,`16*5-64`($ctx),$T3		# r3
+	vpmuludq	$H0,$T4,$T4		# h0*r2
+	vpaddq		$T4,$D2,$D2		# d2 += h0*r2
+	vpmuludq	$H4,$T2,$T1		# h4*s2
+	vpaddq		$T1,$D1,$D1		# d1 += h4*s2
+	 vpshufd	\$0x32,`16*6-64`($ctx),$T4		# s3
+	vpmuludq	$H3,$T2,$T2		# h3*s2
+	vpaddq		$T2,$D0,$D0		# d0 += h3*s2
+
+	vpmuludq	$H1,$T3,$T0		# h1*r3
+	vpaddq		$T0,$D4,$D4		# d4 += h1*r3
+	vpmuludq	$H0,$T3,$T3		# h0*r3
+	vpaddq		$T3,$D3,$D3		# d3 += h0*r3
+	 vpshufd	\$0x32,`16*7-64`($ctx),$T2		# r4
+	vpmuludq	$H4,$T4,$T1		# h4*s3
+	vpaddq		$T1,$D2,$D2		# d2 += h4*s3
+	 vpshufd	\$0x32,`16*8-64`($ctx),$T3		# s4
+	vpmuludq	$H3,$T4,$T0		# h3*s3
+	vpaddq		$T0,$D1,$D1		# d1 += h3*s3
+	vpmuludq	$H2,$T4,$T4		# h2*s3
+	vpaddq		$T4,$D0,$D0		# d0 += h2*s3
+
+	vpmuludq	$H0,$T2,$T2		# h0*r4
+	vpaddq		$T2,$D4,$D4		# d4 += h0*r4
+	vpmuludq	$H4,$T3,$T1		# h4*s4
+	vpaddq		$T1,$D3,$D3		# d3 += h4*s4
+	vpmuludq	$H3,$T3,$T0		# h3*s4
+	vpaddq		$T0,$D2,$D2		# d2 += h3*s4
+	vpmuludq	$H2,$T3,$T1		# h2*s4
+	vpaddq		$T1,$D1,$D1		# d1 += h2*s4
+	vpmuludq	$H1,$T3,$T3		# h1*s4
+	vpaddq		$T3,$D0,$D0		# d0 += h1*s4
+
+.Lshort_tail_avx:
+	################################################################
+	# lazy reduction
+
+	vpsrlq		\$26,$D3,$H3
+	vpand		$MASK,$D3,$D3
+	vpaddq		$H3,$D4,$D4		# h3 -&gt; h4
+
+	vpsrlq		\$26,$D0,$H0
+	vpand		$MASK,$D0,$D0
+	vpaddq		$H0,$D1,$D1		# h0 -&gt; h1
+
+	vpsrlq		\$26,$D4,$H4
+	vpand		$MASK,$D4,$D4
+
+	vpsrlq		\$26,$D1,$H1
+	vpand		$MASK,$D1,$D1
+	vpaddq		$H1,$D2,$D2		# h1 -&gt; h2
+
+	vpaddq		$H4,$D0,$D0
+	vpsllq		\$2,$H4,$H4
+	vpaddq		$H4,$D0,$D0		# h4 -&gt; h0
+
+	vpsrlq		\$26,$D2,$H2
+	vpand		$MASK,$D2,$D2
+	vpaddq		$H2,$D3,$D3		# h2 -&gt; h3
+
+	vpsrlq		\$26,$D0,$H0
+	vpand		$MASK,$D0,$D0
+	vpaddq		$H0,$D1,$D1		# h0 -&gt; h1
+
+	vpsrlq		\$26,$D3,$H3
+	vpand		$MASK,$D3,$D3
+	vpaddq		$H3,$D4,$D4		# h3 -&gt; h4
+
+	################################################################
+	# horizontal addition
+
+	vpsrldq		\$8,$D2,$T2
+	vpsrldq		\$8,$D0,$T0
+	vpsrldq		\$8,$D1,$T1
+	vpsrldq		\$8,$D3,$T3
+	vpsrldq		\$8,$D4,$T4
+	vpaddq		$T2,$D2,$H2
+	vpaddq		$T0,$D0,$H0
+	vpaddq		$T1,$D1,$H1
+	vpaddq		$T3,$D3,$H3
+	vpaddq		$T4,$D4,$H4
+
+	vmovd		$H0,`4*0-48-64`($ctx)	# save partially reduced
+	vmovd		$H1,`4*1-48-64`($ctx)
+	vmovd		$H2,`4*2-48-64`($ctx)
+	vmovd		$H3,`4*3-48-64`($ctx)
+	vmovd		$H4,`4*4-48-64`($ctx)
+___
+$code.=&lt;&lt;___	if ($win64);
+	vmovdqa		0x50(%r11),%xmm6
+	vmovdqa		0x60(%r11),%xmm7
+	vmovdqa		0x70(%r11),%xmm8
+	vmovdqa		0x80(%r11),%xmm9
+	vmovdqa		0x90(%r11),%xmm10
+	vmovdqa		0xa0(%r11),%xmm11
+	vmovdqa		0xb0(%r11),%xmm12
+	vmovdqa		0xc0(%r11),%xmm13
+	vmovdqa		0xd0(%r11),%xmm14
+	vmovdqa		0xe0(%r11),%xmm15
+	lea		0xf8(%r11),%rsp
+.Ldo_avx_epilogue:
+___
+$code.=&lt;&lt;___	if (!$win64);
+	lea		0x58(%r11),%rsp
+___
+$code.=&lt;&lt;___;
+	vzeroupper
+	ret
+.size	poly1305_blocks_avx,.-poly1305_blocks_avx
+
+.type	poly1305_emit_avx,\@function,3
+.align	32
+poly1305_emit_avx:
+	cmpl	\$0,20($ctx)	# is_base2_26?
+	je	poly1305_emit
+
+	mov	0($ctx),%eax	# load hash value base 2^26
+	mov	4($ctx),%ecx
+	mov	8($ctx),%r8d
+	mov	12($ctx),%r11d
+	mov	16($ctx),%r10d
+
+	shl	\$26,%rcx	# base 2^26 -&gt; base 2^64
+	mov	%r8,%r9
+	shl	\$52,%r8
+	add	%rcx,%rax
+	shr	\$12,%r9
+	add	%rax,%r8	# h0
+	adc	\$0,%r9
+
+	shl	\$14,%r11
+	mov	%r10,%rax
+	shr	\$24,%r10
+	add	%r11,%r9
+	shl	\$40,%rax
+	add	%rax,%r9	# h1
+	adc	\$0,%r10	# h2
+
+	mov	%r10,%rax	# could be partially reduced, so reduce
+	mov	%r10,%rcx
+	and	\$3,%r10
+	shr	\$2,%rax
+	and	\$-4,%rcx
+	add	%rcx,%rax
+	add	%rax,%r8
+	adc	\$0,%r9
+
+	mov	%r8,%rax
+	add	\$5,%r8		# compare to modulus
+	mov	%r9,%rcx
+	adc	\$0,%r9
+	adc	\$0,%r10
+	shr	\$2,%r10	# did 130-bit value overfow?
+	cmovnz	%r8,%rax
+	cmovnz	%r9,%rcx
+
+	add	0($nonce),%rax	# accumulate nonce
+	adc	8($nonce),%rcx
+	mov	%rax,0($mac)	# write result
+	mov	%rcx,8($mac)
+
+	ret
+.size	poly1305_emit_avx,.-poly1305_emit_avx
+___
+
+if ($avx&gt;1) {
+my ($H0,$H1,$H2,$H3,$H4, $MASK, $T4,$T0,$T1,$T2,$T3, $D0,$D1,$D2,$D3,$D4) =
+    map(&quot;%ymm$_&quot;,(0..15));
+my $S4=$MASK;
+
+$code.=&lt;&lt;___;
+.type	poly1305_blocks_avx2,\@function,4
+.align	32
+poly1305_blocks_avx2:
+	mov	20($ctx),%r8d		# is_base2_26
+	cmp	\$128,$len
+	jae	.Lblocks_avx2
+	test	%r8d,%r8d
+	jz	poly1305_blocks
+
+.Lblocks_avx2:
+	and	\$-16,$len
+	jz	.Lno_data_avx2
+
+	vzeroupper
+
+	test	%r8d,%r8d
+	jz	.Lbase2_64_avx2
+
+	test	\$63,$len
+	jz	.Leven_avx2
+
+	push	%rbx
+	push	%rbp
+	push	%r12
+	push	%r13
+	push	%r14
+	push	%r15
+.Lblocks_avx2_body:
+
+	mov	$len,%r15		# reassign $len
+
+	mov	0($ctx),$d1		# load hash value
+	mov	8($ctx),$d2
+	mov	16($ctx),$h2#d
+
+	mov	24($ctx),$r0		# load r
+	mov	32($ctx),$s1
+
+	################################# base 2^26 -&gt; base 2^64
+	mov	$d1#d,$h0#d
+	and	\$-1&lt;&lt;31,$d1
+	mov	$d2,$r1			# borrow $r1
+	mov	$d2#d,$h1#d
+	and	\$-1&lt;&lt;31,$d2
+
+	shr	\$6,$d1
+	shl	\$52,$r1
+	add	$d1,$h0
+	shr	\$12,$h1
+	shr	\$18,$d2
+	add	$r1,$h0
+	adc	$d2,$h1
+
+	mov	$h2,$d1
+	shl	\$40,$d1
+	shr	\$24,$h2
+	add	$d1,$h1
+	adc	\$0,$h2			# can be partially reduced...
+
+	mov	\$-4,$d2		# ... so reduce
+	mov	$h2,$d1
+	and	$h2,$d2
+	shr	\$2,$d1
+	and	\$3,$h2
+	add	$d2,$d1			# =*5
+	add	$d1,$h0
+	adc	\$0,$h1
+
+	mov	$s1,$r1
+	mov	$s1,%rax
+	shr	\$2,$s1
+	add	$r1,$s1			# s1 = r1 + (r1 &gt;&gt; 2)
+
+.Lbase2_26_pre_avx2:
+	add	0($inp),$h0		# accumulate input
+	adc	8($inp),$h1
+	lea	16($inp),$inp
+	adc	$padbit,$h2
+	sub	\$16,%r15
+
+	call	__poly1305_block
+	mov	$r1,%rax
+
+	test	\$63,%r15
+	jnz	.Lbase2_26_pre_avx2
+
+	test	$padbit,$padbit		# if $padbit is zero,
+	jz	.Lstore_base2_64_avx2	# store hash in base 2^64 format
+
+	################################# base 2^64 -&gt; base 2^26
+	mov	$h0,%rax
+	mov	$h0,%rdx
+	shr	\$52,$h0
+	mov	$h1,$r0
+	mov	$h1,$r1
+	shr	\$26,%rdx
+	and	\$0x3ffffff,%rax	# h[0]
+	shl	\$12,$r0
+	and	\$0x3ffffff,%rdx	# h[1]
+	shr	\$14,$h1
+	or	$r0,$h0
+	shl	\$24,$h2
+	and	\$0x3ffffff,$h0		# h[2]
+	shr	\$40,$r1
+	and	\$0x3ffffff,$h1		# h[3]
+	or	$r1,$h2			# h[4]
+
+	test	%r15,%r15
+	jz	.Lstore_base2_26_avx2
+
+	vmovd	%rax#d,%x#$H0
+	vmovd	%rdx#d,%x#$H1
+	vmovd	$h0#d,%x#$H2
+	vmovd	$h1#d,%x#$H3
+	vmovd	$h2#d,%x#$H4
+	jmp	.Lproceed_avx2
+
+.align	32
+.Lstore_base2_64_avx2:
+	mov	$h0,0($ctx)
+	mov	$h1,8($ctx)
+	mov	$h2,16($ctx)		# note that is_base2_26 is zeroed
+	jmp	.Ldone_avx2
+
+.align	16
+.Lstore_base2_26_avx2:
+	mov	%rax#d,0($ctx)		# store hash value base 2^26
+	mov	%rdx#d,4($ctx)
+	mov	$h0#d,8($ctx)
+	mov	$h1#d,12($ctx)
+	mov	$h2#d,16($ctx)
+.align	16
+.Ldone_avx2:
+	mov	0(%rsp),%r15
+	mov	8(%rsp),%r14
+	mov	16(%rsp),%r13
+	mov	24(%rsp),%r12
+	mov	32(%rsp),%rbp
+	mov	40(%rsp),%rbx
+	lea	48(%rsp),%rsp
+.Lno_data_avx2:
+.Lblocks_avx2_epilogue:
+	ret
+
+.align	32
+.Lbase2_64_avx2:
+	push	%rbx
+	push	%rbp
+	push	%r12
+	push	%r13
+	push	%r14
+	push	%r15
+.Lbase2_64_avx2_body:
+
+	mov	$len,%r15		# reassign $len
+
+	mov	24($ctx),$r0		# load r
+	mov	32($ctx),$s1
+
+	mov	0($ctx),$h0		# load hash value
+	mov	8($ctx),$h1
+	mov	16($ctx),$h2#d
+
+	mov	$s1,$r1
+	mov	$s1,%rax
+	shr	\$2,$s1
+	add	$r1,$s1			# s1 = r1 + (r1 &gt;&gt; 2)
+
+	test	\$63,$len
+	jz	.Linit_avx2
+
+.Lbase2_64_pre_avx2:
+	add	0($inp),$h0		# accumulate input
+	adc	8($inp),$h1
+	lea	16($inp),$inp
+	adc	$padbit,$h2
+	sub	\$16,%r15
+
+	call	__poly1305_block
+	mov	$r1,%rax
+
+	test	\$63,%r15
+	jnz	.Lbase2_64_pre_avx2
+
+.Linit_avx2:
+	################################# base 2^64 -&gt; base 2^26
+	mov	$h0,%rax
+	mov	$h0,%rdx
+	shr	\$52,$h0
+	mov	$h1,$d1
+	mov	$h1,$d2
+	shr	\$26,%rdx
+	and	\$0x3ffffff,%rax	# h[0]
+	shl	\$12,$d1
+	and	\$0x3ffffff,%rdx	# h[1]
+	shr	\$14,$h1
+	or	$d1,$h0
+	shl	\$24,$h2
+	and	\$0x3ffffff,$h0		# h[2]
+	shr	\$40,$d2
+	and	\$0x3ffffff,$h1		# h[3]
+	or	$d2,$h2			# h[4]
+
+	vmovd	%rax#d,%x#$H0
+	vmovd	%rdx#d,%x#$H1
+	vmovd	$h0#d,%x#$H2
+	vmovd	$h1#d,%x#$H3
+	vmovd	$h2#d,%x#$H4
+	movl	\$1,20($ctx)		# set is_base2_26
+
+	call	__poly1305_init_avx
+
+.Lproceed_avx2:
+	mov	%r15,$len
+
+	mov	0(%rsp),%r15
+	mov	8(%rsp),%r14
+	mov	16(%rsp),%r13
+	mov	24(%rsp),%r12
+	mov	32(%rsp),%rbp
+	mov	40(%rsp),%rbx
+	lea	48(%rsp),%rax
+	lea	48(%rsp),%rsp
+.Lbase2_64_avx2_epilogue:
+	jmp	.Ldo_avx2
+
+.align	32
+.Leven_avx2:
+	vmovd		4*0($ctx),%x#$H0	# load hash value base 2^26
+	vmovd		4*1($ctx),%x#$H1
+	vmovd		4*2($ctx),%x#$H2
+	vmovd		4*3($ctx),%x#$H3
+	vmovd		4*4($ctx),%x#$H4
+
+.Ldo_avx2:
+___
+$code.=&lt;&lt;___	if (!$win64);
+	lea		-8(%rsp),%r11
+	sub		\$0x128,%rsp
+___
+$code.=&lt;&lt;___	if ($win64);
+	lea		-0xf8(%rsp),%r11
+	sub		\$0x1c8,%rsp
+	vmovdqa		%xmm6,0x50(%r11)
+	vmovdqa		%xmm7,0x60(%r11)
+	vmovdqa		%xmm8,0x70(%r11)
+	vmovdqa		%xmm9,0x80(%r11)
+	vmovdqa		%xmm10,0x90(%r11)
+	vmovdqa		%xmm11,0xa0(%r11)
+	vmovdqa		%xmm12,0xb0(%r11)
+	vmovdqa		%xmm13,0xc0(%r11)
+	vmovdqa		%xmm14,0xd0(%r11)
+	vmovdqa		%xmm15,0xe0(%r11)
+.Ldo_avx2_body:
+___
+$code.=&lt;&lt;___;
+	lea		48+64($ctx),$ctx	# size optimization
+	lea		.Lconst(%rip),%rcx
+
+	# expand and copy pre-calculated table to stack
+	vmovdqu		`16*0-64`($ctx),%x#$T2
+	and		\$-512,%rsp
+	vmovdqu		`16*1-64`($ctx),%x#$T3
+	vmovdqu		`16*2-64`($ctx),%x#$T4
+	vmovdqu		`16*3-64`($ctx),%x#$D0
+	vmovdqu		`16*4-64`($ctx),%x#$D1
+	vmovdqu		`16*5-64`($ctx),%x#$D2
+	vmovdqu		`16*6-64`($ctx),%x#$D3
+	vpermq		\$0x15,$T2,$T2		# 00003412 -&gt; 12343434
+	vmovdqu		`16*7-64`($ctx),%x#$D4
+	vpermq		\$0x15,$T3,$T3
+	vpshufd		\$0xc8,$T2,$T2		# 12343434 -&gt; 14243444
+	vmovdqu		`16*8-64`($ctx),%x#$MASK
+	vpermq		\$0x15,$T4,$T4
+	vpshufd		\$0xc8,$T3,$T3
+	vmovdqa		$T2,0x00(%rsp)
+	vpermq		\$0x15,$D0,$D0
+	vpshufd		\$0xc8,$T4,$T4
+	vmovdqa		$T3,0x20(%rsp)
+	vpermq		\$0x15,$D1,$D1
+	vpshufd		\$0xc8,$D0,$D0
+	vmovdqa		$T4,0x40(%rsp)
+	vpermq		\$0x15,$D2,$D2
+	vpshufd		\$0xc8,$D1,$D1
+	vmovdqa		$D0,0x60(%rsp)
+	vpermq		\$0x15,$D3,$D3
+	vpshufd		\$0xc8,$D2,$D2
+	vmovdqa		$D1,0x80(%rsp)
+	vpermq		\$0x15,$D4,$D4
+	vpshufd		\$0xc8,$D3,$D3
+	vmovdqa		$D2,0xa0(%rsp)
+	vpermq		\$0x15,$MASK,$MASK
+	vpshufd		\$0xc8,$D4,$D4
+	vmovdqa		$D3,0xc0(%rsp)
+	vpshufd		\$0xc8,$MASK,$MASK
+	vmovdqa		$D4,0xe0(%rsp)
+	vmovdqa		$MASK,0x100(%rsp)
+	vmovdqa		64(%rcx),$MASK		# .Lmask26
+
+	################################################################
+	# load input
+	vmovdqu		16*0($inp),%x#$T0
+	vmovdqu		16*1($inp),%x#$T1
+	vinserti128	\$1,16*2($inp),$T0,$T0
+	vinserti128	\$1,16*3($inp),$T1,$T1
+	lea		16*4($inp),$inp
+
+	vpsrldq		\$6,$T0,$T2		# splat input
+	vpsrldq		\$6,$T1,$T3
+	vpunpckhqdq	$T1,$T0,$T4		# 4
+	vpunpcklqdq	$T3,$T2,$T2		# 2:3
+	vpunpcklqdq	$T1,$T0,$T0		# 0:1
+
+	vpsrlq		\$30,$T2,$T3
+	vpsrlq		\$4,$T2,$T2
+	vpsrlq		\$26,$T0,$T1
+	vpsrlq		\$40,$T4,$T4		# 4
+	vpand		$MASK,$T2,$T2		# 2
+	vpand		$MASK,$T0,$T0		# 0
+	vpand		$MASK,$T1,$T1		# 1
+	vpand		$MASK,$T3,$T3		# 3
+	vpor		32(%rcx),$T4,$T4	# padbit, yes, always
+
+	lea		0x90(%rsp),%rax		# size optimization
+	vpaddq		$H2,$T2,$H2		# accumulate input
+	sub		\$64,$len
+	jz		.Ltail_avx2
+	jmp		.Loop_avx2
+
+.align	32
+.Loop_avx2:
+	################################################################
+	# ((inp[0]*r^4+r[4])*r^4+r[8])*r^4
+	# ((inp[1]*r^4+r[5])*r^4+r[9])*r^3
+	# ((inp[2]*r^4+r[6])*r^4+r[10])*r^2
+	# ((inp[3]*r^4+r[7])*r^4+r[11])*r^1
+	#   \________/\________/
+	################################################################
+	#vpaddq		$H2,$T2,$H2		# accumulate input
+	vpaddq		$H0,$T0,$H0
+	vmovdqa		`32*0`(%rsp),$T0	# r0^4
+	vpaddq		$H1,$T1,$H1
+	vmovdqa		`32*1`(%rsp),$T1	# r1^4
+	vpaddq		$H3,$T3,$H3
+	vmovdqa		`32*3`(%rsp),$T2	# r2^4
+	vpaddq		$H4,$T4,$H4
+	vmovdqa		`32*6-0x90`(%rax),$T3	# s3^4
+	vmovdqa		`32*8-0x90`(%rax),$S4	# s4^4
+
+	# d4 = h4*r0 + h3*r1   + h2*r2   + h1*r3   + h0*r4
+	# d3 = h3*r0 + h2*r1   + h1*r2   + h0*r3   + h4*5*r4
+	# d2 = h2*r0 + h1*r1   + h0*r2   + h4*5*r3 + h3*5*r4
+	# d1 = h1*r0 + h0*r1   + h4*5*r2 + h3*5*r3 + h2*5*r4
+	# d0 = h0*r0 + h4*5*r1 + h3*5*r2 + h2*5*r3 + h1*5*r4
+	#
+	# however, as h2 is &quot;chronologically&quot; first one available pull
+	# corresponding operations up, so it's
+	#
+	# d4 = h2*r2   + h4*r0 + h3*r1             + h1*r3   + h0*r4
+	# d3 = h2*r1   + h3*r0           + h1*r2   + h0*r3   + h4*5*r4
+	# d2 = h2*r0           + h1*r1   + h0*r2   + h4*5*r3 + h3*5*r4
+	# d1 = h2*5*r4 + h1*r0 + h0*r1   + h4*5*r2 + h3*5*r3
+	# d0 = h2*5*r3 + h0*r0 + h4*5*r1 + h3*5*r2           + h1*5*r4
+
+	vpmuludq	$H2,$T0,$D2		# d2 = h2*r0
+	vpmuludq	$H2,$T1,$D3		# d3 = h2*r1
+	vpmuludq	$H2,$T2,$D4		# d4 = h2*r2
+	vpmuludq	$H2,$T3,$D0		# d0 = h2*s3
+	vpmuludq	$H2,$S4,$D1		# d1 = h2*s4
+
+	vpmuludq	$H0,$T1,$T4		# h0*r1
+	vpmuludq	$H1,$T1,$H2		# h1*r1, borrow $H2 as temp
+	vpaddq		$T4,$D1,$D1		# d1 += h0*r1
+	vpaddq		$H2,$D2,$D2		# d2 += h1*r1
+	vpmuludq	$H3,$T1,$T4		# h3*r1
+	vpmuludq	`32*2`(%rsp),$H4,$H2	# h4*s1
+	vpaddq		$T4,$D4,$D4		# d4 += h3*r1
+	vpaddq		$H2,$D0,$D0		# d0 += h4*s1
+	 vmovdqa	`32*4-0x90`(%rax),$T1	# s2
+
+	vpmuludq	$H0,$T0,$T4		# h0*r0
+	vpmuludq	$H1,$T0,$H2		# h1*r0
+	vpaddq		$T4,$D0,$D0		# d0 += h0*r0
+	vpaddq		$H2,$D1,$D1		# d1 += h1*r0
+	vpmuludq	$H3,$T0,$T4		# h3*r0
+	vpmuludq	$H4,$T0,$H2		# h4*r0
+	 vmovdqu	16*0($inp),%x#$T0	# load input
+	vpaddq		$T4,$D3,$D3		# d3 += h3*r0
+	vpaddq		$H2,$D4,$D4		# d4 += h4*r0
+	 vinserti128	\$1,16*2($inp),$T0,$T0
+
+	vpmuludq	$H3,$T1,$T4		# h3*s2
+	vpmuludq	$H4,$T1,$H2		# h4*s2
+	 vmovdqu	16*1($inp),%x#$T1
+	vpaddq		$T4,$D0,$D0		# d0 += h3*s2
+	vpaddq		$H2,$D1,$D1		# d1 += h4*s2
+	 vmovdqa	`32*5-0x90`(%rax),$H2	# r3
+	vpmuludq	$H1,$T2,$T4		# h1*r2
+	vpmuludq	$H0,$T2,$T2		# h0*r2
+	vpaddq		$T4,$D3,$D3		# d3 += h1*r2
+	vpaddq		$T2,$D2,$D2		# d2 += h0*r2
+	 vinserti128	\$1,16*3($inp),$T1,$T1
+	 lea		16*4($inp),$inp
+
+	vpmuludq	$H1,$H2,$T4		# h1*r3
+	vpmuludq	$H0,$H2,$H2		# h0*r3
+	 vpsrldq	\$6,$T0,$T2		# splat input
+	vpaddq		$T4,$D4,$D4		# d4 += h1*r3
+	vpaddq		$H2,$D3,$D3		# d3 += h0*r3
+	vpmuludq	$H3,$T3,$T4		# h3*s3
+	vpmuludq	$H4,$T3,$H2		# h4*s3
+	 vpsrldq	\$6,$T1,$T3
+	vpaddq		$T4,$D1,$D1		# d1 += h3*s3
+	vpaddq		$H2,$D2,$D2		# d2 += h4*s3
+	 vpunpckhqdq	$T1,$T0,$T4		# 4
+
+	vpmuludq	$H3,$S4,$H3		# h3*s4
+	vpmuludq	$H4,$S4,$H4		# h4*s4
+	 vpunpcklqdq	$T1,$T0,$T0		# 0:1
+	vpaddq		$H3,$D2,$H2		# h2 = d2 + h3*r4
+	vpaddq		$H4,$D3,$H3		# h3 = d3 + h4*r4
+	 vpunpcklqdq	$T3,$T2,$T3		# 2:3
+	vpmuludq	`32*7-0x90`(%rax),$H0,$H4	# h0*r4
+	vpmuludq	$H1,$S4,$H0		# h1*s4
+	vmovdqa		64(%rcx),$MASK		# .Lmask26
+	vpaddq		$H4,$D4,$H4		# h4 = d4 + h0*r4
+	vpaddq		$H0,$D0,$H0		# h0 = d0 + h1*s4
+
+	################################################################
+	# lazy reduction (interleaved with tail of input splat)
+
+	vpsrlq		\$26,$H3,$D3
+	vpand		$MASK,$H3,$H3
+	vpaddq		$D3,$H4,$H4		# h3 -&gt; h4
+
+	vpsrlq		\$26,$H0,$D0
+	vpand		$MASK,$H0,$H0
+	vpaddq		$D0,$D1,$H1		# h0 -&gt; h1
+
+	vpsrlq		\$26,$H4,$D4
+	vpand		$MASK,$H4,$H4
+
+	 vpsrlq		\$4,$T3,$T2
+
+	vpsrlq		\$26,$H1,$D1
+	vpand		$MASK,$H1,$H1
+	vpaddq		$D1,$H2,$H2		# h1 -&gt; h2
+
+	vpaddq		$D4,$H0,$H0
+	vpsllq		\$2,$D4,$D4
+	vpaddq		$D4,$H0,$H0		# h4 -&gt; h0
+
+	 vpand		$MASK,$T2,$T2		# 2
+	 vpsrlq		\$26,$T0,$T1
+
+	vpsrlq		\$26,$H2,$D2
+	vpand		$MASK,$H2,$H2
+	vpaddq		$D2,$H3,$H3		# h2 -&gt; h3
+
+	 vpaddq		$T2,$H2,$H2		# modulo-scheduled
+	 vpsrlq		\$30,$T3,$T3
+
+	vpsrlq		\$26,$H0,$D0
+	vpand		$MASK,$H0,$H0
+	vpaddq		$D0,$H1,$H1		# h0 -&gt; h1
+
+	 vpsrlq		\$40,$T4,$T4		# 4
+
+	vpsrlq		\$26,$H3,$D3
+	vpand		$MASK,$H3,$H3
+	vpaddq		$D3,$H4,$H4		# h3 -&gt; h4
+
+	 vpand		$MASK,$T0,$T0		# 0
+	 vpand		$MASK,$T1,$T1		# 1
+	 vpand		$MASK,$T3,$T3		# 3
+	 vpor		32(%rcx),$T4,$T4	# padbit, yes, always
+
+	sub		\$64,$len
+	jnz		.Loop_avx2
+
+	.byte		0x66,0x90
+.Ltail_avx2:
+	################################################################
+	# while above multiplications were by r^4 in all lanes, in last
+	# iteration we multiply least significant lane by r^4 and most
+	# significant one by r, so copy of above except that references
+	# to the precomputed table are displaced by 4...
+
+	#vpaddq		$H2,$T2,$H2		# accumulate input
+	vpaddq		$H0,$T0,$H0
+	vmovdqu		`32*0+4`(%rsp),$T0	# r0^4
+	vpaddq		$H1,$T1,$H1
+	vmovdqu		`32*1+4`(%rsp),$T1	# r1^4
+	vpaddq		$H3,$T3,$H3
+	vmovdqu		`32*3+4`(%rsp),$T2	# r2^4
+	vpaddq		$H4,$T4,$H4
+	vmovdqu		`32*6+4-0x90`(%rax),$T3	# s3^4
+	vmovdqu		`32*8+4-0x90`(%rax),$S4	# s4^4
+
+	vpmuludq	$H2,$T0,$D2		# d2 = h2*r0
+	vpmuludq	$H2,$T1,$D3		# d3 = h2*r1
+	vpmuludq	$H2,$T2,$D4		# d4 = h2*r2
+	vpmuludq	$H2,$T3,$D0		# d0 = h2*s3
+	vpmuludq	$H2,$S4,$D1		# d1 = h2*s4
+
+	vpmuludq	$H0,$T1,$T4		# h0*r1
+	vpmuludq	$H1,$T1,$H2		# h1*r1
+	vpaddq		$T4,$D1,$D1		# d1 += h0*r1
+	vpaddq		$H2,$D2,$D2		# d2 += h1*r1
+	vpmuludq	$H3,$T1,$T4		# h3*r1
+	vpmuludq	`32*2+4`(%rsp),$H4,$H2	# h4*s1
+	vpaddq		$T4,$D4,$D4		# d4 += h3*r1
+	vpaddq		$H2,$D0,$D0		# d0 += h4*s1
+
+	vpmuludq	$H0,$T0,$T4		# h0*r0
+	vpmuludq	$H1,$T0,$H2		# h1*r0
+	vpaddq		$T4,$D0,$D0		# d0 += h0*r0
+	 vmovdqu	`32*4+4-0x90`(%rax),$T1	# s2
+	vpaddq		$H2,$D1,$D1		# d1 += h1*r0
+	vpmuludq	$H3,$T0,$T4		# h3*r0
+	vpmuludq	$H4,$T0,$H2		# h4*r0
+	vpaddq		$T4,$D3,$D3		# d3 += h3*r0
+	vpaddq		$H2,$D4,$D4		# d4 += h4*r0
+
+	vpmuludq	$H3,$T1,$T4		# h3*s2
+	vpmuludq	$H4,$T1,$H2		# h4*s2
+	vpaddq		$T4,$D0,$D0		# d0 += h3*s2
+	vpaddq		$H2,$D1,$D1		# d1 += h4*s2
+	 vmovdqu	`32*5+4-0x90`(%rax),$H2	# r3
+	vpmuludq	$H1,$T2,$T4		# h1*r2
+	vpmuludq	$H0,$T2,$T2		# h0*r2
+	vpaddq		$T4,$D3,$D3		# d3 += h1*r2
+	vpaddq		$T2,$D2,$D2		# d2 += h0*r2
+
+	vpmuludq	$H1,$H2,$T4		# h1*r3
+	vpmuludq	$H0,$H2,$H2		# h0*r3
+	vpaddq		$T4,$D4,$D4		# d4 += h1*r3
+	vpaddq		$H2,$D3,$D3		# d3 += h0*r3
+	vpmuludq	$H3,$T3,$T4		# h3*s3
+	vpmuludq	$H4,$T3,$H2		# h4*s3
+	vpaddq		$T4,$D1,$D1		# d1 += h3*s3
+	vpaddq		$H2,$D2,$D2		# d2 += h4*s3
+
+	vpmuludq	$H3,$S4,$H3		# h3*s4
+	vpmuludq	$H4,$S4,$H4		# h4*s4
+	vpaddq		$H3,$D2,$H2		# h2 = d2 + h3*r4
+	vpaddq		$H4,$D3,$H3		# h3 = d3 + h4*r4
+	vpmuludq	`32*7+4-0x90`(%rax),$H0,$H4		# h0*r4
+	vpmuludq	$H1,$S4,$H0		# h1*s4
+	vmovdqa		64(%rcx),$MASK		# .Lmask26
+	vpaddq		$H4,$D4,$H4		# h4 = d4 + h0*r4
+	vpaddq		$H0,$D0,$H0		# h0 = d0 + h1*s4
+
+	################################################################
+	# lazy reduction
+
+	vpsrlq		\$26,$H3,$D3
+	vpand		$MASK,$H3,$H3
+	vpaddq		$D3,$H4,$H4		# h3 -&gt; h4
+
+	vpsrlq		\$26,$H0,$D0
+	vpand		$MASK,$H0,$H0
+	vpaddq		$D0,$D1,$H1		# h0 -&gt; h1
+
+	vpsrlq		\$26,$H4,$D4
+	vpand		$MASK,$H4,$H4
+
+	vpsrlq		\$26,$H1,$D1
+	vpand		$MASK,$H1,$H1
+	vpaddq		$D1,$H2,$H2		# h1 -&gt; h2
+
+	vpaddq		$D4,$H0,$H0
+	vpsllq		\$2,$D4,$D4
+	vpaddq		$D4,$H0,$H0		# h4 -&gt; h0
+
+	vpsrlq		\$26,$H2,$D2
+	vpand		$MASK,$H2,$H2
+	vpaddq		$D2,$H3,$H3		# h2 -&gt; h3
+
+	vpsrlq		\$26,$H0,$D0
+	vpand		$MASK,$H0,$H0
+	vpaddq		$D0,$H1,$H1		# h0 -&gt; h1
+
+	vpsrlq		\$26,$H3,$D3
+	vpand		$MASK,$H3,$H3
+	vpaddq		$D3,$H4,$H4		# h3 -&gt; h4
+
+	################################################################
+	# horizontal addition
+
+	vpsrldq		\$8,$H2,$T2
+	vpsrldq		\$8,$H0,$T0
+	vpsrldq		\$8,$H1,$T1
+	vpsrldq		\$8,$H3,$T3
+	vpsrldq		\$8,$H4,$T4
+	vpaddq		$T2,$H2,$H2
+	vpaddq		$T0,$H0,$H0
+	vpaddq		$T1,$H1,$H1
+	vpaddq		$T3,$H3,$H3
+	vpaddq		$T4,$H4,$H4
+
+	vpermq		\$0x2,$H2,$T2
+	vpermq		\$0x2,$H0,$T0
+	vpermq		\$0x2,$H1,$T1
+	vpermq		\$0x2,$H3,$T3
+	vpermq		\$0x2,$H4,$T4
+	vpaddq		$T2,$H2,$H2
+	vpaddq		$T0,$H0,$H0
+	vpaddq		$T1,$H1,$H1
+	vpaddq		$T3,$H3,$H3
+	vpaddq		$T4,$H4,$H4
+
+	vmovd		%x#$H0,`4*0-48-64`($ctx)# save partially reduced
+	vmovd		%x#$H1,`4*1-48-64`($ctx)
+	vmovd		%x#$H2,`4*2-48-64`($ctx)
+	vmovd		%x#$H3,`4*3-48-64`($ctx)
+	vmovd		%x#$H4,`4*4-48-64`($ctx)
+___
+$code.=&lt;&lt;___	if ($win64);
+	vmovdqa		0x50(%r11),%xmm6
+	vmovdqa		0x60(%r11),%xmm7
+	vmovdqa		0x70(%r11),%xmm8
+	vmovdqa		0x80(%r11),%xmm9
+	vmovdqa		0x90(%r11),%xmm10
+	vmovdqa		0xa0(%r11),%xmm11
+	vmovdqa		0xb0(%r11),%xmm12
+	vmovdqa		0xc0(%r11),%xmm13
+	vmovdqa		0xd0(%r11),%xmm14
+	vmovdqa		0xe0(%r11),%xmm15
+	lea		0xf8(%r11),%rsp
+.Ldo_avx2_epilogue:
+___
+$code.=&lt;&lt;___	if (!$win64);
+	lea		8(%r11),%rsp
+___
+$code.=&lt;&lt;___;
+	vzeroupper
+	ret
+.size	poly1305_blocks_avx2,.-poly1305_blocks_avx2
+___
+}
+$code.=&lt;&lt;___;
+.align	64
+.Lconst:
+.Lmask24:
+.long	0x0ffffff,0,0x0ffffff,0,0x0ffffff,0,0x0ffffff,0
+.L129:
+.long	1&lt;&lt;24,0,1&lt;&lt;24,0,1&lt;&lt;24,0,1&lt;&lt;24,0
+.Lmask26:
+.long	0x3ffffff,0,0x3ffffff,0,0x3ffffff,0,0x3ffffff,0
+.Lfive:
+.long	5,0,5,0,5,0,5,0
+___
+}
+
+$code.=&lt;&lt;___;
+.asciz	&quot;Poly1305 for x86_64, CRYPTOGAMS by &lt;appro\@openssl.org&gt;&quot;
+.align	16
+___
+
+# EXCEPTION_DISPOSITION handler (EXCEPTION_RECORD *rec,ULONG64 frame,
+#		CONTEXT *context,DISPATCHER_CONTEXT *disp)
+if ($win64) {
+$rec=&quot;%rcx&quot;;
+$frame=&quot;%rdx&quot;;
+$context=&quot;%r8&quot;;
+$disp=&quot;%r9&quot;;
+
+$code.=&lt;&lt;___;
+.extern	__imp_RtlVirtualUnwind
+.type	se_handler,\@abi-omnipotent
+.align	16
+se_handler:
+	push	%rsi
+	push	%rdi
+	push	%rbx
+	push	%rbp
+	push	%r12
+	push	%r13
+	push	%r14
+	push	%r15
+	pushfq
+	sub	\$64,%rsp
+
+	mov	120($context),%rax	# pull context-&gt;Rax
+	mov	248($context),%rbx	# pull context-&gt;Rip
+
+	mov	8($disp),%rsi		# disp-&gt;ImageBase
+	mov	56($disp),%r11		# disp-&gt;HandlerData
+
+	mov	0(%r11),%r10d		# HandlerData[0]
+	lea	(%rsi,%r10),%r10	# prologue label
+	cmp	%r10,%rbx		# context-&gt;Rip&lt;.Lprologue
+	jb	.Lcommon_seh_tail
+
+	mov	152($context),%rax	# pull context-&gt;Rsp
+
+	mov	4(%r11),%r10d		# HandlerData[1]
+	lea	(%rsi,%r10),%r10	# epilogue label
+	cmp	%r10,%rbx		# context-&gt;Rip&gt;=.Lepilogue
+	jae	.Lcommon_seh_tail
+
+	lea	48(%rax),%rax
+
+	mov	-8(%rax),%rbx
+	mov	-16(%rax),%rbp
+	mov	-24(%rax),%r12
+	mov	-32(%rax),%r13
+	mov	-40(%rax),%r14
+	mov	-48(%rax),%r15
+	mov	%rbx,144($context)	# restore context-&gt;Rbx
+	mov	%rbp,160($context)	# restore context-&gt;Rbp
+	mov	%r12,216($context)	# restore context-&gt;R12
+	mov	%r13,224($context)	# restore context-&gt;R13
+	mov	%r14,232($context)	# restore context-&gt;R14
+	mov	%r15,240($context)	# restore context-&gt;R14
+
+	jmp	.Lcommon_seh_tail
+.size	se_handler,.-se_handler
+
+.type	avx_handler,\@abi-omnipotent
+.align	16
+avx_handler:
+	push	%rsi
+	push	%rdi
+	push	%rbx
+	push	%rbp
+	push	%r12
+	push	%r13
+	push	%r14
+	push	%r15
+	pushfq
+	sub	\$64,%rsp
+
+	mov	120($context),%rax	# pull context-&gt;Rax
+	mov	248($context),%rbx	# pull context-&gt;Rip
+
+	mov	8($disp),%rsi		# disp-&gt;ImageBase
+	mov	56($disp),%r11		# disp-&gt;HandlerData
+
+	mov	0(%r11),%r10d		# HandlerData[0]
+	lea	(%rsi,%r10),%r10	# prologue label
+	cmp	%r10,%rbx		# context-&gt;Rip&lt;prologue label
+	jb	.Lcommon_seh_tail
+
+	mov	152($context),%rax	# pull context-&gt;Rsp
+
+	mov	4(%r11),%r10d		# HandlerData[1]
+	lea	(%rsi,%r10),%r10	# epilogue label
+	cmp	%r10,%rbx		# context-&gt;Rip&gt;=epilogue label
+	jae	.Lcommon_seh_tail
+
+	mov	208($context),%rax	# pull context-&gt;R11
+
+	lea	0x50(%rax),%rsi
+	lea	0xf8(%rax),%rax
+	lea	512($context),%rdi	# &amp;context.Xmm6
+	mov	\$20,%ecx
+	.long	0xa548f3fc		# cld; rep movsq
+
+.Lcommon_seh_tail:
+	mov	8(%rax),%rdi
+	mov	16(%rax),%rsi
+	mov	%rax,152($context)	# restore context-&gt;Rsp
+	mov	%rsi,168($context)	# restore context-&gt;Rsi
+	mov	%rdi,176($context)	# restore context-&gt;Rdi
+
+	mov	40($disp),%rdi		# disp-&gt;ContextRecord
+	mov	$context,%rsi		# context
+	mov	\$154,%ecx		# sizeof(CONTEXT)
+	.long	0xa548f3fc		# cld; rep movsq
+
+	mov	$disp,%rsi
+	xor	%rcx,%rcx		# arg1, UNW_FLAG_NHANDLER
+	mov	8(%rsi),%rdx		# arg2, disp-&gt;ImageBase
+	mov	0(%rsi),%r8		# arg3, disp-&gt;ControlPc
+	mov	16(%rsi),%r9		# arg4, disp-&gt;FunctionEntry
+	mov	40(%rsi),%r10		# disp-&gt;ContextRecord
+	lea	56(%rsi),%r11		# &amp;disp-&gt;HandlerData
+	lea	24(%rsi),%r12		# &amp;disp-&gt;EstablisherFrame
+	mov	%r10,32(%rsp)		# arg5
+	mov	%r11,40(%rsp)		# arg6
+	mov	%r12,48(%rsp)		# arg7
+	mov	%rcx,56(%rsp)		# arg8, (NULL)
+	call	*__imp_RtlVirtualUnwind(%rip)
+
+	mov	\$1,%eax		# ExceptionContinueSearch
+	add	\$64,%rsp
+	popfq
+	pop	%r15
+	pop	%r14
+	pop	%r13
+	pop	%r12
+	pop	%rbp
+	pop	%rbx
+	pop	%rdi
+	pop	%rsi
+	ret
+.size	avx_handler,.-avx_handler
+
+.section	.pdata
+.align	4
+	.rva	.LSEH_begin_poly1305_init
+	.rva	.LSEH_end_poly1305_init
+	.rva	.LSEH_info_poly1305_init
+
+	.rva	.LSEH_begin_poly1305_blocks
+	.rva	.LSEH_end_poly1305_blocks
+	.rva	.LSEH_info_poly1305_blocks
+
+	.rva	.LSEH_begin_poly1305_emit
+	.rva	.LSEH_end_poly1305_emit
+	.rva	.LSEH_info_poly1305_emit
+___
+$code.=&lt;&lt;___ if ($avx);
+	.rva	.LSEH_begin_poly1305_blocks_avx
+	.rva	.Lbase2_64_avx
+	.rva	.LSEH_info_poly1305_blocks_avx_1
+
+	.rva	.Lbase2_64_avx
+	.rva	.Leven_avx
+	.rva	.LSEH_info_poly1305_blocks_avx_2
+
+	.rva	.Leven_avx
+	.rva	.LSEH_end_poly1305_blocks_avx
+	.rva	.LSEH_info_poly1305_blocks_avx_3
+
+	.rva	.LSEH_begin_poly1305_emit_avx
+	.rva	.LSEH_end_poly1305_emit_avx
+	.rva	.LSEH_info_poly1305_emit_avx
+___
+$code.=&lt;&lt;___ if ($avx&gt;1);
+	.rva	.LSEH_begin_poly1305_blocks_avx2
+	.rva	.Lbase2_64_avx2
+	.rva	.LSEH_info_poly1305_blocks_avx2_1
+
+	.rva	.Lbase2_64_avx2
+	.rva	.Leven_avx2
+	.rva	.LSEH_info_poly1305_blocks_avx2_2
+
+	.rva	.Leven_avx2
+	.rva	.LSEH_end_poly1305_blocks_avx2
+	.rva	.LSEH_info_poly1305_blocks_avx2_3
+___
+$code.=&lt;&lt;___;
+.section	.xdata
+.align	8
+.LSEH_info_poly1305_init:
+	.byte	9,0,0,0
+	.rva	se_handler
+	.rva	.LSEH_begin_poly1305_init,.LSEH_begin_poly1305_init
+
+.LSEH_info_poly1305_blocks:
+	.byte	9,0,0,0
+	.rva	se_handler
+	.rva	.Lblocks_body,.Lblocks_epilogue
+
+.LSEH_info_poly1305_emit:
+	.byte	9,0,0,0
+	.rva	se_handler
+	.rva	.LSEH_begin_poly1305_emit,.LSEH_begin_poly1305_emit
+___
+$code.=&lt;&lt;___ if ($avx);
+.LSEH_info_poly1305_blocks_avx_1:
+	.byte	9,0,0,0
+	.rva	se_handler
+	.rva	.Lblocks_avx_body,.Lblocks_avx_epilogue		# HandlerData[]
+
+.LSEH_info_poly1305_blocks_avx_2:
+	.byte	9,0,0,0
+	.rva	se_handler
+	.rva	.Lbase2_64_avx_body,.Lbase2_64_avx_epilogue	# HandlerData[]
+
+.LSEH_info_poly1305_blocks_avx_3:
+	.byte	9,0,0,0
+	.rva	avx_handler
+	.rva	.Ldo_avx_body,.Ldo_avx_epilogue			# HandlerData[]
+
+.LSEH_info_poly1305_emit_avx:
+	.byte	9,0,0,0
+	.rva	se_handler
+	.rva	.LSEH_begin_poly1305_emit_avx,.LSEH_begin_poly1305_emit_avx
+___
+$code.=&lt;&lt;___ if ($avx&gt;1);
+.LSEH_info_poly1305_blocks_avx2_1:
+	.byte	9,0,0,0
+	.rva	se_handler
+	.rva	.Lblocks_avx2_body,.Lblocks_avx2_epilogue	# HandlerData[]
+
+.LSEH_info_poly1305_blocks_avx2_2:
+	.byte	9,0,0,0
+	.rva	se_handler
+	.rva	.Lbase2_64_avx2_body,.Lbase2_64_avx2_epilogue	# HandlerData[]
+
+.LSEH_info_poly1305_blocks_avx2_3:
+	.byte	9,0,0,0
+	.rva	avx_handler
+	.rva	.Ldo_avx2_body,.Ldo_avx2_epilogue		# HandlerData[]
+___
+}
+
+foreach (split('\n',$code)) {
+	s/\`([^\`]*)\`/eval($1)/ge;
+	s/%r([a-z]+)#d/%e$1/g;
+	s/%r([0-9]+)#d/%r$1d/g;
+	s/%x#%y/%x/g;
+
+	print $_,&quot;\n&quot;;
+}
+close STDOUT;
diff --git a/crypto/poly1305/poly1305.c b/crypto/poly1305/poly1305.c
index 9a44f27..7c9f302 100644
--- a/crypto/poly1305/poly1305.c
+++ b/crypto/poly1305/poly1305.c
@@ -454,6 +454,15 @@ void Poly1305_Init(POLY1305 *ctx, const unsigned char key[32])
 
 }
 
+#ifdef POLY1305_ASM
+/*
+ * This &quot;eclipses&quot; poly1305_blocks and poly1305_emit, but it's
+ * conscious choice imposed by -Wshadow compiler warnings.
+ */
+# define poly1305_blocks (*poly1305_blocks_p)
+# define poly1305_emit   (*poly1305_emit_p)
+#endif
+
 void Poly1305_Update(POLY1305 *ctx, const unsigned char *inp, size_t len)
 {
 #ifdef POLY1305_ASM
@@ -463,7 +472,7 @@ void Poly1305_Update(POLY1305 *ctx, const unsigned char *inp, size_t len)
      * property is fluently used in assembly modules to optimize
      * padbit handling on loop boundary.
      */
-    poly1305_blocks_f poly1305_blocks = ctx-&gt;func.blocks;
+    poly1305_blocks_f poly1305_blocks_p = ctx-&gt;func.blocks;
 #endif
     size_t rem, num;
 
@@ -499,8 +508,8 @@ void Poly1305_Update(POLY1305 *ctx, const unsigned char *inp, size_t len)
 void Poly1305_Final(POLY1305 *ctx, unsigned char mac[16])
 {
 #ifdef POLY1305_ASM
-    poly1305_blocks_f poly1305_blocks = ctx-&gt;func.blocks;
-    poly1305_emit_f poly1305_emit = ctx-&gt;func.emit;
+    poly1305_blocks_f poly1305_blocks_p = ctx-&gt;func.blocks;
+    poly1305_emit_f poly1305_emit_p = ctx-&gt;func.emit;
 #endif
     size_t num;
 
diff --git a/test/evptests.txt b/test/evptests.txt
index c276b6e..c325ac9 100644
--- a/test/evptests.txt
+++ b/test/evptests.txt
@@ -2903,9 +2903,57 @@ Ciphertext = f798a189f195e66982105ffb640bb7757f579da31602fc93ec01ac56f85ac3c134a
 Cipher = chacha20
 Key = 000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f
 IV = 00000000000000000001020304050607
+Plaintext = 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
+Ciphertext = f798a189f195e66982105ffb640bb7757f579da31602fc93ec01ac56f85ac3c134a4547b733b46413042c9440049176905d3be59ea1c53f15916155c2be8241a38008b9a26bc35941e2444177c8ade6689de95264986d95889fb60e84629c9bd9a5acb1cc118be563eb9b3a4a472f82e09a7e778492b562ef7130e88dfe031c79db9d4f7c7a899151b9a475032b63fc385245fe054e3dd5a97a5f576fe064025d3ce042c566ab2c507b138db853e3d6959660996546cc9c4a6eafdc777c040d70eaf46f76dad3979e5c5360c3317166a1c894c94a371876a94df7628fe4eaaf2ccb27d5aaae0ad7ad0f9d4b6ad3b54098746d4524d38407a6deb3ab78fab78c94213668bbbd394c5de93b853178addd6b97f9fa1ec3e56c00c9ddff0a44a204241175a4cab0f961ba53ede9bdf960b94f9829b1f3414726429b362c5b538e391520f489b7ed8d20ae3fd49e9e259e44397514d618c96c4846be3c680bdc11c71dcbbe29ccf80d62a0938fa549391e6ea57ecbe2606790ec15d2224ae307c144226b7c4e8c2f97d2a1d67852d29beba110edd445197012062a393a9c92803ad3b4f31d7bc6033ccf7932cfed3f019044d25905916777286f82f9a4cc1ffe430ffd1dcfc27deed327b9f9630d2fa969fb6f0603cd19dd9a9519e673bcfcd9014125291a44669ef7285e74ed3729b677f801c3cdf058c50963168b496043716c7307cd9e0cdd137fccb0f05b47cdbb95c5f54831622c3652a32b2531fe326bcd6e2bbf56a194fa196fbd1a54952110f51c73433865f7664b836685e3664b3d8444aF89A242805E18C975F1146324996FDE17007CF3E6E8F4E764022533EDBFE07D4733E48BB372D75B0EF48EC983EB78532161CC529E5ABB89837DFCCA6261DBB37
+
+Cipher = chacha20
+Key = 000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f
+IV = 00000000000000000001020304050607
 Plaintext = 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
 Ciphertext = f798a189f195e66982105ffb640bb7757f579da31602fc93ec01ac56f85ac3c134a4547b733b46413042c9440049176905d3be59ea1c53f15916155c2be8241a38008b9a26bc35941e2444177c8ade6689de95264986d95889fb60e84629c9bd9a5acb1cc118be563eb9b3a4a472f82e09a7e778492b562ef7130e88dfe031c79db9d4f7c7a899151b9a475032b63fc385245fe054e3dd5a97a5f576fe064025d3ce042c566ab2c507b138db853e3d6959660996546cc9c4a6eafdc777c040d70eaf46f76dad3979e5c5360c3317166a1c894c94a371876a94df7628fe4eaaf2ccb27d5aaae0ad7ad0f9d4b6ad3b54098746d4524d38407a6deb3ab78fab78c94213668bbbd394c5de93b853178addd6b97f9fa1ec3e56c00c9ddff0a44a204241175a4cab0f961ba53ede9bdf960b94f9829b1f3414726429b362c5b538e391520f489b7ed8d20ae3fd49e9e259e44397514d618c96c4846be3c680bdc11c71dcbbe29ccf80d62a0938fa549391e6ea57ecbe2606790ec15d2224ae307c144226b7c4e8c2f97d2a1d67852d29beba110edd445197012062a393a9c92803ad3b4f31d7bc6033ccf7932cfed3f019044d25905916777286f82f9a4cc1ffe430ffd1dcfc27deed327b9f9630d2fa969fb6f0603cd19dd9a9519e673bcfcd9014125291a44669ef7285e74ed3729b677f801c3cdf058c50963168b496043716c7307cd9e0cdd137fccb0f05b47cdbb95c5f54831622c3652a32b2531fe326bcd6e2bbf56a194fa196fbd1a54952110f51c73433865f7664b836685e3664b3d8444a
 
+Cipher = chacha20
+Key = 000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f
+IV = 00000000000000000001020304050607
+Plaintext = 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
+Ciphertext = f798a189f195e66982105ffb640bb7757f579da31602fc93ec01ac56f85ac3c134a4547b733b46413042c9440049176905d3be59ea1c53f15916155c2be8241a38008b9a26bc35941e2444177c8ade6689de95264986d95889fb60e84629c9bd9a5acb1cc118be563eb9b3a4a472f82e09a7e778492b562ef7130e88dfe031c79db9d4f7c7a899151b9a475032b63fc385245fe054e3dd5a97a5f576fe064025d3ce042c566ab2c507b138db853e3d6959660996546cc9c4a6eafdc777c040d70eaf46f76dad3979e5c5360c3317166a1c894c94a371876a94df7628fe4eaaf2ccb27d5aaae0ad7ad0f9d4b6ad3b54098746d4524d38407a6deb3ab78fab78c94213668bbbd394c5de93b853178addd6b97f9fa1ec3e56c00c9ddff0a44a204241175a4cab0f961ba53ede9bdf960b94f9829b1f3414726429b362c5b538e391520f489b7ed8d20ae3fd49e9e259e44397514d618c96c4846be3c680bdc11c71dcbbe29ccf80d62a0938fa549391e6ea57ecbe2606790ec15d2224ae307c144226b7c4e8c2f97d2a1d67852d29beba110edd445197012062a393a9c92803ad3b4f31d7bc6033ccf7932cfed3f019044d25905916777286f82f9a4cc1ffe430ffd1dcfc27deed327b9f9630d2fa969fb6f0603cd19dd9a9519e673bcfcd9014125291a44669ef7285e74ed3729b677f801c3cdf058c50963168b496043716c7307cd9e0cdd137fccb0f05b47cdbb95c5f54831622c3652a32b2531fe326bcd6e2bbf56a194fa196fbd1a54952110f51c73433865f7664b836685e3664b3d8444aF89A242805E18C975F1146324996FDE17007CF3E6E8F4E764022533EDBFE07D4733E48BB372D75B0EF48EC983EB78532161CC529E5ABB89837DFCCA6261DBB37C7C5E6A87478BF41EE85A518C0F4EFA9BDE828C5A71B8E46597B634AFD204D3C501334239C3414285ED72D3A9169EABBD4DC25D52BB7516D3BA712D75AD8C0AE
+
+Cipher = chacha20
+Key = 000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f
+IV = 00000000000000000001020304050607
+Plaintext = 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
+Ciphertext = f798a189f195e66982105ffb640bb7757f579da31602fc93ec01ac56f85ac3c134a4547b733b46413042c9440049176905d3be59ea1c53f15916155c2be8241a38008b9a26bc35941e2444177c8ade6689de95264986d95889fb60e84629c9bd9a5acb1cc118be563eb9b3a4a472f82e09a7e778492b562ef7130e88dfe031c79db9d4f7c7a899151b9a475032b63fc385245fe054e3dd5a97a5f576fe064025d3ce042c566ab2c507b138db853e3d6959660996546cc9c4a6eafdc777c040d70eaf46f76dad3979e5c5360c3317166a1c894c94a371876a94df7628fe4eaaf2ccb27d5aaae0ad7ad0f9d4b6ad3b54098746d4524d38407a6deb3ab78fab78c94213668bbbd394c5de93b853178addd6b97f9fa1ec3e56c00c9ddff0a44a204241175a4cab0f961ba53ede9bdf960b94f9829b1f3414726429b362c5b538e391520f489b7ed8d20ae3fd49e9e259e44397514d618c96c4846be3c680bdc11c71dcbbe29ccf80d62a0938fa549391e6ea57ecbe2606790ec15d2224ae307c144226b7c4e8c2f97d2a1d67852d29beba110edd445197012062a393a9c92803ad3b4f31d7bc6033ccf7932cfed3f019044d25905916777286f82f9a4cc1ffe430ffd1dcfc27deed327b9f9630d2fa969fb6f0603cd19dd9a9519e673bcfcd9014125291a44669ef7285e74ed3729b677f801c3cdf058c50963168b496043716c7307cd9e0cdd137fccb0f05b47cdbb95c5f54831622c3652a32b2531fe326bcd6e2bbf56a194fa196fbd1a54952110f51c73433865f7664b836685e3664b3d8444aF89A242805E18C975F1146324996FDE17007CF3E6E8F4E764022533EDBFE07D4733E48BB372D75B0EF48EC983EB78532161CC529E5ABB89837DFCCA6261DBB37C7C5E6A87478BF41EE85A518C0F4EFA9BDE828C5A71B8E46597B634AFD204D3C501334239C3414285ED72D3A9169EABBD4DC25D52BB7516D3BA712D75AD8C0AE5D493C19E38A77939E7A058D713E9CCCCA58045F436B434B1C80D365472406E392951987DB6905C80D431DA18451135BE7E82BCAB358CB3971E61405B2FF1798
+
+Cipher = chacha20
+Key = 000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f
+IV = 00000000000000000001020304050607
+Plaintext = 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
+Ciphertext = f798a189f195e66982105ffb640bb7757f579da31602fc93ec01ac56f85ac3c134a4547b733b46413042c9440049176905d3be59ea1c53f15916155c2be8241a38008b9a26bc35941e2444177c8ade6689de95264986d95889fb60e84629c9bd9a5acb1cc118be563eb9b3a4a472f82e09a7e778492b562ef7130e88dfe031c79db9d4f7c7a899151b9a475032b63fc385245fe054e3dd5a97a5f576fe064025d3ce042c566ab2c507b138db853e3d6959660996546cc9c4a6eafdc777c040d70eaf46f76dad3979e5c5360c3317166a1c894c94a371876a94df7628fe4eaaf2ccb27d5aaae0ad7ad0f9d4b6ad3b54098746d4524d38407a6deb3ab78fab78c94213668bbbd394c5de93b853178addd6b97f9fa1ec3e56c00c9ddff0a44a204241175a4cab0f961ba53ede9bdf960b94f9829b1f3414726429b362c5b538e391520f489b7ed8d20ae3fd49e9e259e44397514d618c96c4846be3c680bdc11c71dcbbe29ccf80d62a0938fa549391e6ea57ecbe2606790ec15d2224ae307c144226b7c4e8c2f97d2a1d67852d29beba110edd445197012062a393a9c92803ad3b4f31d7bc6033ccf7932cfed3f019044d25905916777286f82f9a4cc1ffe430ffd1dcfc27deed327b9f9630d2fa969fb6f0603cd19dd9a9519e673bcfcd9014125291a44669ef7285e74ed3729b677f801c3cdf058c50963168b496043716c7307cd9e0cdd137fccb0f05b47cdbb95c5f54831622c3652a32b2531fe326bcd6e2bbf56a194fa196fbd1a54952110f51c73433865f7664b836685e3664b3d8444aF89A242805E18C975F1146324996FDE17007CF3E6E8F4E764022533EDBFE07D4733E48BB372D75B0EF48EC983EB78532161CC529E5ABB89837DFCCA6261DBB37C7C5E6A87478BF41EE85A518C0F4EFA9BDE828C5A71B8E46597B634AFD204D3C501334239C3414285ED72D3A9169EABBD4DC25D52BB7516D3BA712D75AD8C0AE5D493C19E38A77939E7A058D713E9CCCCA58045F436B434B1C80D365472406E392951987DB6905C80D431DA18451135BE7E82BCAB358CB3971E61405B2FF17980D6E7E67E861E28201C1EE30B441040FD06878D65042C95582A4318207BFC700BE0CE32889AEC2FFE5085E8967910D879FA0E8C0FF85FDC510B9FF2FBF87CFCB
+
+Cipher = chacha20
+Key = 000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f
+IV = 00000000000000000001020304050607
+Plaintext = 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
+Ciphertext = f798a189f195e66982105ffb640bb7757f579da31602fc93ec01ac56f85ac3c134a4547b733b46413042c9440049176905d3be59ea1c53f15916155c2be8241a38008b9a26bc35941e2444177c8ade6689de95264986d95889fb60e84629c9bd9a5acb1cc118be563eb9b3a4a472f82e09a7e778492b562ef7130e88dfe031c79db9d4f7c7a899151b9a475032b63fc385245fe054e3dd5a97a5f576fe064025d3ce042c566ab2c507b138db853e3d6959660996546cc9c4a6eafdc777c040d70eaf46f76dad3979e5c5360c3317166a1c894c94a371876a94df7628fe4eaaf2ccb27d5aaae0ad7ad0f9d4b6ad3b54098746d4524d38407a6deb3ab78fab78c94213668bbbd394c5de93b853178addd6b97f9fa1ec3e56c00c9ddff0a44a204241175a4cab0f961ba53ede9bdf960b94f9829b1f3414726429b362c5b538e391520f489b7ed8d20ae3fd49e9e259e44397514d618c96c4846be3c680bdc11c71dcbbe29ccf80d62a0938fa549391e6ea57ecbe2606790ec15d2224ae307c144226b7c4e8c2f97d2a1d67852d29beba110edd445197012062a393a9c92803ad3b4f31d7bc6033ccf7932cfed3f019044d25905916777286f82f9a4cc1ffe430ffd1dcfc27deed327b9f9630d2fa969fb6f0603cd19dd9a9519e673bcfcd9014125291a44669ef7285e74ed3729b677f801c3cdf058c50963168b496043716c7307cd9e0cdd137fccb0f05b47cdbb95c5f54831622c3652a32b2531fe326bcd6e2bbf56a194fa196fbd1a54952110f51c73433865f7664b836685e3664b3d8444aF89A242805E18C975F1146324996FDE17007CF3E6E8F4E764022533EDBFE07D4733E48BB372D75B0EF48EC983EB78532161CC529E5ABB89837DFCCA6261DBB37C7C5E6A87478BF41EE85A518C0F4EFA9BDE828C5A71B8E46597B634AFD204D3C501334239C3414285ED72D3A9169EABBD4DC25D52BB7516D3BA712D75AD8C0AE5D493C19E38A77939E7A058D713E9CCCCA58045F436B434B1C80D365472406E392951987DB6905C80D431DA18451135BE7E82BCAB358CB3971E61405B2FF17980D6E7E67E861E28201C1EE30B441040FD06878D65042C95582A4318207BFC700BE0CE32889AEC2FFE5085E8967910D879FA0E8C0FF85FDC510B9FF2FBF87CFCB29577D68099E04FFA05F752A73D377C70D3A8BC2DA80E6E780EC057182C33AD1DE387252258A1E18E6FAD910327CE7F42FD1E1E0515F9586E2F2EFCB9F472B1D
+
+Cipher = chacha20
+Key = 000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f
+IV = 00000000000000000001020304050607
+Plaintext = 000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
+Ciphertext = f798a189f195e66982105ffb640bb7757f579da31602fc93ec01ac56f85ac3c134a4547b733b46413042c9440049176905d3be59ea1c53f15916155c2be8241a38008b9a26bc35941e2444177c8ade6689de95264986d95889fb60e84629c9bd9a5acb1cc118be563eb9b3a4a472f82e09a7e778492b562ef7130e88dfe031c79db9d4f7c7a899151b9a475032b63fc385245fe054e3dd5a97a5f576fe064025d3ce042c566ab2c507b138db853e3d6959660996546cc9c4a6eafdc777c040d70eaf46f76dad3979e5c5360c3317166a1c894c94a371876a94df7628fe4eaaf2ccb27d5aaae0ad7ad0f9d4b6ad3b54098746d4524d38407a6deb3ab78fab78c94213668bbbd394c5de93b853178addd6b97f9fa1ec3e56c00c9ddff0a44a204241175a4cab0f961ba53ede9bdf960b94f9829b1f3414726429b362c5b538e391520f489b7ed8d20ae3fd49e9e259e44397514d618c96c4846be3c680bdc11c71dcbbe29ccf80d62a0938fa549391e6ea57ecbe2606790ec15d2224ae307c144226b7c4e8c2f97d2a1d67852d29beba110edd445197012062a393a9c92803ad3b4f31d7bc6033ccf7932cfed3f019044d25905916777286f82f9a4cc1ffe430ffd1dcfc27deed327b9f9630d2fa969fb6f0603cd19dd9a9519e673bcfcd9014125291a44669ef7285e74ed3729b677f801c3cdf058c50963168b496043716c7307cd9e0cdd137fccb0f05b47cdbb95c5f54831622c3652a32b2531fe326bcd6e2bbf56a194fa196fbd1a54952110f51c73433865f7664b836685e3664b3d8444aF89A242805E18C975F1146324996FDE17007CF3E6E8F4E764022533EDBFE07D4733E48BB372D75B0EF48EC983EB78532161CC529E5ABB89837DFCCA6261DBB37C7C5E6A87478BF41EE85A518C0F4EFA9BDE828C5A71B8E46597B634AFD204D3C501334239C3414285ED72D3A9169EABBD4DC25D52BB7516D3BA712D75AD8C0AE5D493C19E38A77939E7A058D713E9CCCCA58045F436B434B1C80D365472406E392951987DB6905C80D431DA18451135BE7E82BCAB358CB3971E61405B2FF17980D6E7E67E861E28201C1EE30B441040FD06878D65042C95582A4318207BFC700BE0CE32889AEC2FFE5085E8967910D879FA0E8C0FF85FDC510B9FF2FBF87CFCB29577D68099E04FFA05F752A73D377C70D3A8BC2DA80E6E780EC057182C33AD1DE387252258A1E18E6FAD910327CE7F42FD1E1E0515F9586E2F2EFCB9F472B1DBDBAC354A4162151E9D92C79FB08BB4DDC56F19448C0175A46E2E6C491FEC71419AA43A349BEA768A92C75DE68FD9591E68067F3197094D3FB87ED81785EA075
+
+Cipher = chacha20
+Key = 000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f
+IV = 00000000000000000001020304050607
+Plaintext = 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
+Ciphertext = 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
+
+Cipher = chacha20
+Key = 000102030405060708090a0b0c0d0e0f101112131415161718191a1b1c1d1e1f
+IV = 00000000000000000001020304050607
+Plaintext = 0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
+Ciphertext = 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
+
 # RFC7539
 Cipher = chacha20-poly1305
 Key = 808182838485868788898a8b8c8d8e8f909192939495969798999a9b9c9d9e9f
</PRE>


<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="003852.html">[openssl-commits] [openssl]  master update
</A></li>
	<LI>Next message: <A HREF="003860.html">[openssl-commits] [openssl]  master update
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3857">[ date ]</a>
              <a href="thread.html#3857">[ thread ]</a>
              <a href="subject.html#3857">[ subject ]</a>
              <a href="author.html#3857">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="../../../mailman/listinfo/openssl-commits.html">More information about the openssl-commits
mailing list</a><br>
</body></html>
